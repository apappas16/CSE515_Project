{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the data directory path: Dataset 6/\n",
      "Enter the window length: 3\n",
      "Enter the shift length: 3\n",
      "Enter the resolution: 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-6c5579faf6a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;31m# TASK 0B\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword_tuple\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munique_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m         \u001b[0mtfValue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalcTfValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m         \u001b[0midfValue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalcIdfValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[0mtf_idf_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfValue\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0midfValue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-6c5579faf6a6>\u001b[0m in \u001b[0;36mcalcTfValue\u001b[1;34m(wordTuple)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".wrd\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcomponent\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m             \u001b[0mgestFileWords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetWordsFromFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m             \u001b[0mnum_occurs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mgestFileWords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msensorIndex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[0mtotalWords\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgestFileWords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msensorIndex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-6c5579faf6a6>\u001b[0m in \u001b[0;36mgetWordsFromFile\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m             \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" - [\"\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# list of all words as they occur in the file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "import os\n",
    "from scipy.integrate import quad\n",
    "\n",
    "\n",
    "def calcAvgSensorValue(sensorValues):\n",
    "    avg = sum(sensorValues) / len(sensorValues)\n",
    "    avg = round(avg, 9)\n",
    "    return avg\n",
    "\n",
    "\n",
    "def calcStdDev(sensorValues, meanVal):\n",
    "    sd = []\n",
    "    for num in sensorValues:\n",
    "        result = num - meanVal\n",
    "        result = result * result\n",
    "        sd.append(result)\n",
    "    return calcAvgSensorValue(sd)\n",
    "\n",
    "\n",
    "def normalize(sensorValues):\n",
    "    normalized_sensor = []\n",
    "    for val in sensorValues:\n",
    "        val = float(val)\n",
    "        normalized = 2 * ((val - min(sensorValues)) / (max(sensorValues) - min(sensorValues))) - 1\n",
    "        normalized_sensor.append(normalized)\n",
    "    return normalized_sensor\n",
    "\n",
    "\n",
    "def integral(i):\n",
    "    return getGaussianVal(i, 0, 0.25)\n",
    "\n",
    "\n",
    "def getGaussianVal(i, avg, sd):\n",
    "    i = float(i - avg) / sd\n",
    "    gauss = math.exp(-i * i / 2.0) / math.sqrt(2.0 * math.pi) / sd\n",
    "    return gauss\n",
    "\n",
    "\n",
    "def determineBands():\n",
    "    numBands = r * 2\n",
    "    bandList = []\n",
    "    bandStart = -1\n",
    "    for i in range(1, numBands):\n",
    "        integral1, e = quad(integral, (i - r - 1) / r, (i - r) / r)\n",
    "        integral2, e = quad(integral, -1, 1)\n",
    "        length_i = 2 * (integral1 / integral2)\n",
    "        band = bandStart + length_i\n",
    "        bandList.append(band)\n",
    "        bandStart = band\n",
    "    bandList.append(1.0)\n",
    "    return bandList\n",
    "\n",
    "\n",
    "def quantize(values, bandList):\n",
    "    quantized = \"\"\n",
    "    for i in range(len(values)):\n",
    "        bound = -1\n",
    "        for band in bandList:\n",
    "            if band >= values[i] > bound:\n",
    "                quantized += str(bandList.index(band) + 1)\n",
    "                break\n",
    "            else:\n",
    "                bound = band\n",
    "    return quantized\n",
    "\n",
    "\n",
    "def getWords():\n",
    "    wordList = []\n",
    "    i = 0\n",
    "    while (i + w - 1) < len(quantizedSensor):\n",
    "        word = quantizedSensor[i:i + w]\n",
    "        wordList.append(word)\n",
    "        i += s\n",
    "    return wordList\n",
    "\n",
    "\n",
    "def addToUniqueDict(word_tuple):\n",
    "    inList = False\n",
    "    for word in unique_dict:\n",
    "        if word == word_tuple:\n",
    "            inList = True\n",
    "            break\n",
    "    if not inList:\n",
    "        unique_dict.append(word_tuple)\n",
    "\n",
    "\n",
    "def calcAvgQuanAmp():\n",
    "    avgQuanAmpList = []\n",
    "    normWord = []\n",
    "    i = 0\n",
    "    while (i + w - 1) < len(normSensorVals):\n",
    "        word = normSensorVals[i:i + w]\n",
    "        normWord.append(word)\n",
    "        i += s\n",
    "\n",
    "    for word in normWord:\n",
    "        avgAmp = sum(word) / len(word)\n",
    "        avgQuanAmpList.append(avgAmp)\n",
    "\n",
    "    return avgQuanAmpList\n",
    "\n",
    "\n",
    "def getWordsFromFile(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        words = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            row = line.split(\" - [\")  # list of all words as they occur in the file\n",
    "            if len(row) > 1:\n",
    "                words.append(row[1])\n",
    "        f.close()\n",
    "    return formatWordsFromFile(words)\n",
    "\n",
    "\n",
    "def formatWordsFromFile(tempList):\n",
    "    allWords = []\n",
    "    for sensorWords in tempList:\n",
    "        sensorWords = sensorWords.replace(\"]\", \"\")\n",
    "        sensorWords = sensorWords.replace(\"'\", \"\")\n",
    "        wrd = sensorWords.split(\", \")\n",
    "        allWords.append(wrd)\n",
    "    return allWords\n",
    "\n",
    "\n",
    "def getUniqueWordsInGesture(allWordsInGesture):\n",
    "    uniqueWords = []\n",
    "    for i in range(len(allWordsInGesture)):  # i is each sensor in the gesture\n",
    "        for word in allWordsInGesture[i]:\n",
    "            uniqueWord = (direct, i+1, word)\n",
    "            if uniqueWord not in uniqueWords:\n",
    "                uniqueWords.append(uniqueWord)\n",
    "    return uniqueWords\n",
    "\n",
    "\n",
    "def calcTfValue(wordTuple):\n",
    "    totalWords = 0\n",
    "    sensorIndex = wordTuple[1]-1\n",
    "    num_occurs = 0\n",
    "    component = wordTuple[0]\n",
    "\n",
    "    for file in os.listdir(directory + component):\n",
    "        if file.endswith(\".wrd\"):\n",
    "            file = directory + component + \"/\" + file\n",
    "            gestFileWords = getWordsFromFile(file)\n",
    "            num_occurs += gestFileWords[sensorIndex].count(wordTuple[2])\n",
    "            totalWords += len(gestFileWords[sensorIndex])\n",
    "    value = num_occurs / totalWords\n",
    "    return value\n",
    "\n",
    "\n",
    "def calcIdfValue(wordTuple):\n",
    "    numObjs = 60\n",
    "    numObjsWithWord = 0\n",
    "    sensorIndex = wordTuple[1]-1\n",
    "    component = wordTuple[0]\n",
    "\n",
    "    for file in os.listdir(directory + component):\n",
    "        if file.endswith(\".wrd\"):\n",
    "            file = directory + component + \"/\" + file\n",
    "            gestFileWords = getWordsFromFile(file)\n",
    "            if word_tuple[2] in gestFileWords[sensorIndex]:\n",
    "                numObjsWithWord += 1\n",
    "\n",
    "    value = math.log(numObjs / numObjsWithWord)\n",
    "    return value\n",
    "\n",
    "\n",
    "def writeValsToFile():\n",
    "    allWordsInGesture = getWordsFromFile(fullFileName)\n",
    "    uniqueWordsInGesture = getUniqueWordsInGesture(allWordsInGesture)\n",
    "    sensorId = 1\n",
    "    #unique_dict is what I want\n",
    "    for sensor in uniqueWordsInGesture:\n",
    "        word_tuple = (sensor[0], sensor[1], sensor[2])\n",
    "        for n, i in enumerate(unique_dict):\n",
    "            if i == word_tuple:\n",
    "                tfVal = unique_tf_dict[n]\n",
    "                tfidfVal = unique_tfidf_dict[n]\n",
    "                tfFile.write(str(i) + \"-\")\n",
    "                tfidfFile.write(str(i) + \"-\")\n",
    "                tfFile.write(str(tfVal))\n",
    "                tfidfFile.write(str(tfidfVal))\n",
    "                tfFile.write(\"\\n\")\n",
    "                tfidfFile.write(\"\\n\")\n",
    "        \n",
    "        #sensorId += 1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # GLOBAL VARIABLES:\n",
    "    unique_dict = []  # stores list of all unique word tuples found in entire DB in order\n",
    "    unique_tf_dict = []\n",
    "    unique_tfidf_dict = []\n",
    "    all_gesture_dict = []  # stores list of all words found in DB\n",
    "\n",
    "    # TASK 0\n",
    "    # TASK 0A\n",
    "    directory = input(\"Enter the data directory path: \")\n",
    "    w = input(\"Enter the window length: \")\n",
    "    s = input(\"Enter the shift length: \")\n",
    "    r = input(\"Enter the resolution: \")\n",
    "\n",
    "    w = int(w)\n",
    "    s = int(s)\n",
    "    r = int(r)\n",
    "\n",
    "    # for each data file create a .wrd file containing the following:\n",
    "    for direct in os.listdir(directory):\n",
    "        # for each csv file in X,Y,W,Z:\n",
    "        if not direct.startswith('.') and not direct.endswith(\".xlsx\"):\n",
    "            for filename in os.listdir(directory + direct):\n",
    "                if filename.endswith(\".csv\"):\n",
    "                    bands = determineBands()\n",
    "\n",
    "                    # generate .wrd file\n",
    "                    with open(str(directory) + str(direct) + \"/\" + str(filename) + \".wrd\", \"w\") as wrdFile:\n",
    "\n",
    "                        sensor_id = 1\n",
    "                        csvFile = open(str(directory) + str(direct) + \"/\" + filename, \"r\")\n",
    "                        reader = csv.reader(csvFile, delimiter=',')\n",
    "                        # for each sensor sj in file\n",
    "                        for sensor in reader:\n",
    "                            # output component ID, c in output file\n",
    "                            wrdFile.write(str(direct) + \", \")\n",
    "\n",
    "                            # write sensorID to wrd file\n",
    "                            wrdFile.write(str(sensor_id) + \", \")\n",
    "\n",
    "                            # compute and output average amplitude, avgij of the values\n",
    "                            sensorVals = list(sensor)\n",
    "                            sensorVals = [float(i) for i in sensorVals]\n",
    "                            sensorAvg = calcAvgSensorValue(sensorVals)\n",
    "                            wrdFile.write(str(sensorAvg) + \", \")\n",
    "\n",
    "                            # compute and output standard deviations stdij of the values\n",
    "                            stdDev = calcStdDev(sensorVals, sensorAvg)\n",
    "                            wrdFile.write(str(stdDev) + \", \")\n",
    "\n",
    "                            # normalize entries between -1 and 1\n",
    "                            normSensorVals = normalize(sensorVals)\n",
    "\n",
    "                            # quantizes entries into 2r levels as in phase 1\n",
    "                            quantizedSensor = quantize(normSensorVals, bands)\n",
    "\n",
    "                            # moves a w-length window on time series (by shifting it s units at a time), and at position h\n",
    "                            sensorWords = getWords()\n",
    "\n",
    "                            # computes and outputs in file average quantized amplitude avgQijh for window h of sensor sj\n",
    "                            avgQuanAmp = calcAvgQuanAmp()\n",
    "                            wrdFile.write(str(avgQuanAmp) + \", \" + \" - \")\n",
    "\n",
    "                            # outputs symbolic quantized window descriptor winQijh for the window h of sensor sj\n",
    "                            wrdFile.write(str(sensorWords) + \"\\n\")\n",
    "\n",
    "                            # add dictionary of each window to gestureDict list\n",
    "                            for window in sensorWords:\n",
    "                                wordDict = (direct, sensor_id, window)\n",
    "                                all_gesture_dict.append(wordDict)\n",
    "                                if wordDict not in unique_dict:\n",
    "                                    unique_dict.append(wordDict)\n",
    "\n",
    "                            sensor_id += 1\n",
    "        # The dictionary of the words consists of <componentName, sensorID, winQ>\n",
    "\n",
    "    # TASK 0B\n",
    "    for word_tuple in unique_dict:\n",
    "        tfValue = calcTfValue(word_tuple)\n",
    "        idfValue = calcIdfValue(word_tuple)\n",
    "        tf_idf_value = tfValue * idfValue\n",
    "        unique_tf_dict.append(tfValue)\n",
    "        unique_tfidf_dict.append(tf_idf_value)\n",
    "\n",
    "    for direct in os.listdir(directory):\n",
    "        if not direct.startswith('.') and not direct.endswith(\".xlsx\"):\n",
    "            for wrdFile in os.listdir(directory + direct):\n",
    "                if wrdFile.endswith(\".wrd\"):\n",
    "                    fullFileName = directory + direct + \"/\" + wrdFile\n",
    "                    tfFile = open(directory + direct + \"/tf_vectors_\" + wrdFile[:-8] + \".txt\", \"w\")\n",
    "                    tfidfFile = open(directory + direct + \"/tfidf_vectors_\" + wrdFile[:-8] + \".txt\", \"w\")\n",
    "                    writeValsToFile()\n",
    "                    tfFile.close()\n",
    "                    tfidfFile.close()\n",
    "    # End of TASK0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the following inputs as the same values you used for task 0: \n",
      "Enter the data directory path (ex: Data/: Dataset 5/\n",
      "Enter the window length (ex: 3): 3\n",
      "Enter the shift length (ex: 3): 3\n",
      "Enter the resolution (ex: 3): 3\n",
      "Enter the vector model (ex: tf): tf\n",
      "Enter k (ex: 4): 3\n",
      "Enter the analysis you would like to use (ex: PCA): PCA\n",
      "          0             1             2             3             4      \\\n",
      "0 -1.278862e-07  9.940335e-06  1.376658e-16 -8.266911e-18 -3.076799e-17   \n",
      "1 -3.144890e-08  1.829147e-06 -5.766857e-16  2.741317e-16 -5.359400e-17   \n",
      "2 -3.887020e-08 -1.881789e-07 -1.016091e-16  7.416768e-17  1.985570e-17   \n",
      "\n",
      "          5             6             7             8             9      \\\n",
      "0  2.901250e-18 -1.286100e-17 -1.254986e-17  4.785405e-19 -1.247794e-17   \n",
      "1  2.600323e-19  1.398759e-18  2.363352e-18 -6.837194e-19 -4.121165e-18   \n",
      "2  6.112208e-18 -8.150797e-18  4.329210e-18  1.363027e-18  7.740278e-19   \n",
      "\n",
      "       ...       44470  44471  44472  44473  44474  44475  44476  \\\n",
      "0      ...        -0.0   -0.0   -0.0   -0.0   -0.0   -0.0   -0.0   \n",
      "1      ...         0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "2      ...         0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "\n",
      "          44477         44478         44479  \n",
      "0 -6.027741e-06  9.483471e-05  2.089086e-06  \n",
      "1 -4.793616e-07 -2.144054e-06  4.992844e-08  \n",
      "2 -3.400306e-07 -1.582286e-07  4.054684e-08  \n",
      "\n",
      "[3 rows x 44480 columns]\n",
      "\n",
      "PCA:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexs\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:309: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\alexs\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-bffa2cec1e52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[0museOp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Enter the analysis you would like to use (ex: PCA): \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 396\u001b[1;33m \u001b[0mtask1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0museOp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-bffa2cec1e52>\u001b[0m in \u001b[0;36mtask1\u001b[1;34m(vectModel, useOp, k)\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[1;31m#print(topk)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m         \u001b[0mdictofComponents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreatedictofComponents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictofComponents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-bffa2cec1e52>\u001b[0m in \u001b[0;36mcreatedictofComponents\u001b[1;34m(topk, k)\u001b[0m\n\u001b[0;32m    307\u001b[0m                 \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Z'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msens\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m             \u001b[0moutputMat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtopk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    936\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m         \u001b[1;31m# do the setitem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 938\u001b[1;33m         \u001b[0mcacher_needs_updating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_is_chained_assignment_possible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    939\u001b[0m         \u001b[0msetitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    940\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcacher_needs_updating\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_check_is_chained_assignment_possible\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2625\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mref\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_mixed_type\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2626\u001b[0m                 self._check_setitem_copy(stacklevel=4, t='referant',\n\u001b[1;32m-> 2627\u001b[1;33m                                          force=True)\n\u001b[0m\u001b[0;32m   2628\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2629\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_copy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_check_setitem_copy\u001b[1;34m(self, stacklevel, t, force)\u001b[0m\n\u001b[0;32m   2672\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2673\u001b[0m                 \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2674\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_referents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2675\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_copy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2676\u001b[0m                     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Task 1\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "import os\n",
    "import re\n",
    "\n",
    "numbers = re.compile(r'(\\d+)')\n",
    "\n",
    "#unnecessary lines if you are using anaconda or another \n",
    "print(\"Please enter the following inputs as the same values you used for task 0: \")\n",
    "directory = input(\"Enter the data directory path (ex: Data/: \")\n",
    "w = input(\"Enter the window length (ex: 3): \")\n",
    "s = input(\"Enter the shift length (ex: 3): \")\n",
    "r = input(\"Enter the resolution (ex: 3): \")\n",
    "\n",
    "w = int(w)\n",
    "s = int(s)\n",
    "r = int(r)\n",
    "#end of non-anaconda lines\n",
    "\n",
    "def numericalSort(value):\n",
    "    parts = numbers.split(value)\n",
    "    parts[1::2] = map(int, parts[1::2])\n",
    "    return parts\n",
    "\n",
    "#returns the top-k topics \n",
    "def PCAsetup(wordMat, k):\n",
    "    #calculate PCA\n",
    "    pca = PCA(k)\n",
    "    pc = pca.fit_transform(wordMat)\n",
    "    UT = pca.components_\n",
    "    topK = pd.DataFrame(data = UT)\n",
    "    \n",
    "    original_df = pd.DataFrame(data = pc)\n",
    "    original_df.to_pickle(\"./PCA_\" + vectModel + \".pkl\")\n",
    "    \n",
    "    print(topK)\n",
    "    return topK\n",
    "\n",
    "def SVDsetup(wordMat, k):\n",
    "    #calculate SVD\n",
    "    svd = TruncatedSVD(k)\n",
    "    sv = svd.fit_transform(wordMat)\n",
    "    VT = svd.components_\n",
    "    topK = pd.DataFrame(data = VT)\n",
    "    \n",
    "    original_df = pd.DataFrame(data = sv)\n",
    "    original_df.to_pickle(\"./SVD_\" + vectModel + \".pkl\")\n",
    "    \n",
    "    print(topK)\n",
    "    return topK\n",
    "\n",
    "def NMFsetup(wordMat, k):\n",
    "    #calculate NMF\n",
    "    nmf = NMF(k)\n",
    "    nm = nmf.fit_transform(wordMat)\n",
    "    R = nmf.components_\n",
    "    topK = pd.DataFrame(data = R)\n",
    "    \n",
    "    original_df = pd.DataFrame(data = nm)\n",
    "    original_df.to_pickle(\"./NMF_\" + vectModel + \".pkl\")\n",
    "    \n",
    "    print(topK)\n",
    "    return topK\n",
    "\n",
    "def LDAsetup(wordMat, k):\n",
    "    #calculate LDA\n",
    "    lda = LDA(k)\n",
    "    ld = lda.fit_transform(wordMat)\n",
    "    V = lda.components_\n",
    "    topK = pd.DataFrame(data = V)\n",
    "    \n",
    "    original_df = pd.DataFrame(data = ld)\n",
    "    original_df.to_pickle(\"./LDA_\" + vectModel + \".pkl\")\n",
    "    \n",
    "    print(topK)\n",
    "    return topK\n",
    "\n",
    "\n",
    "def makeMat(vectModel):    \n",
    "    #read files\n",
    "    Wmat = []\n",
    "    Xmat = []\n",
    "    Ymat = []\n",
    "    Zmat = []\n",
    "    if vectModel == \"tf\":\n",
    "        for axisNum in range(1, 5):\n",
    "            if axisNum == 1:\n",
    "                axis = 'W'\n",
    "            elif axisNum == 2:\n",
    "                axis = 'X'\n",
    "            elif axisNum == 3:\n",
    "                axis = 'Y'\n",
    "            elif axisNum == 4:\n",
    "                axis = 'Z'\n",
    "                \n",
    "            for file in sorted(glob.glob(directory + axis + \"/tf_vectors_*.txt\"), key=numericalSort):\n",
    "                #if not file.endswith(\".txt\") or not file.startswith(\"tf_vectors_\"):\n",
    "                    #continue\n",
    "                #Xmat = []\n",
    "                #read tf file\n",
    "                f = open(file, \"r\")\n",
    "                tf_vectors = f.readlines()\n",
    "        \n",
    "                gestWords = []\n",
    "                tfVals = []\n",
    "        \n",
    "                #split the line into the word and tf value\n",
    "                for line in tf_vectors:\n",
    "                    noDash = line.split(\"-\")\n",
    "                    tf_val = noDash[-1]\n",
    "                    tf_val = tf_val.replace(\"\\n\", \"\")\n",
    "                    gestWords.append(noDash[0])\n",
    "                    tfVals.append(tf_val)\n",
    "           \n",
    "        \n",
    "                index = 0\n",
    "                startI = \"1\"\n",
    "                for y in range(1, w):\n",
    "                    startI = startI + \"1\"\n",
    "                startI = int(startI)\n",
    "        \n",
    "                #create dictionary with every word for every sensor and every directory\n",
    "                numWords = (startI * (2*r) - startI) * 20\n",
    "                wordMat = []\n",
    "    \n",
    "                for i in range(0, numWords + 20):\n",
    "                    wordMat.append(0)\n",
    "                \n",
    "                # put tf values into matrix where column = word\n",
    "                for x in gestWords:\n",
    "                    word = x.split(\", \")\n",
    "                    sensorNum = word[1].replace(\"'\", \"\")\n",
    "                    wordID = word[2].replace(\"'\", \"\")\n",
    "                    wordID = wordID.replace(\")\", \"\")\n",
    "                \n",
    "                    wordID = int(wordID)\n",
    "                    sensorNum = int(sensorNum)\n",
    "                \n",
    "                    #axisSplit = len(wordMat) / 4\n",
    "                    sensorSplit = len(wordMat) / 20\n",
    "                    \n",
    "                    wordIndex = int(wordID + ((sensorNum - 1) * sensorSplit)) #+ ((axisNum - 1) * axisSplit)))\n",
    "                    wordIndex = wordIndex - startI\n",
    "                    \n",
    "                    #temp\n",
    "                    if \"e\" in tfVals[index]:\n",
    "                        #continue\n",
    "                        tfVals[index].replace(\"e\", \"\")\n",
    "                        tfVals[index].replace(\"-\", \"\")\n",
    "                        tfVals[index] = str(float(tfVals[index]) * 0.00001)\n",
    "                        \n",
    "                    #temp\n",
    "            \n",
    "                    wordMat[wordIndex] = float(tfVals[index])\n",
    "                    index = index + 1\n",
    "                \n",
    "\n",
    "                if axisNum == 1:\n",
    "                    axis = 'W'\n",
    "                    Wmat.append(wordMat)\n",
    "                elif axisNum == 2:\n",
    "                    axis = 'X'\n",
    "                    Xmat.append(wordMat)\n",
    "                elif axisNum == 3:\n",
    "                    axis = 'Y'\n",
    "                    Ymat.append(wordMat)\n",
    "                elif axisNum == 4:\n",
    "                    axis = 'Z'\n",
    "                    Zmat.append(wordMat)\n",
    "                f.close()\n",
    "                \n",
    "        finalMat = np.append(Wmat, Xmat, axis = 1)\n",
    "        finalMat = np.append(finalMat, Ymat, axis = 1)\n",
    "        finalMat = np.append(finalMat, Zmat, axis = 1)\n",
    "        \n",
    "        #print(finalMat)\n",
    "        \n",
    "        return finalMat\n",
    "            #print(Xmat)\n",
    "    \n",
    "    elif vectModel == \"tfidf\":\n",
    "        for axisNum in range(1, 5):\n",
    "            if axisNum == 1:\n",
    "                axis = 'W'\n",
    "            elif axisNum == 2:\n",
    "                axis = 'X'\n",
    "            elif axisNum == 3:\n",
    "                axis = 'Y'\n",
    "            elif axisNum == 4:\n",
    "                axis = 'Z'\n",
    "                \n",
    "            for file in sorted(glob.glob(directory + axis + \"/tfidf_vectors_*.txt\"), key=numericalSort):\n",
    "                #if not file.endswith(\".txt\") or not file.startswith(\"tfidf_vectors_\"):\n",
    "                    #continue\n",
    "                #read tf file\n",
    "                f = open(file, \"r\")\n",
    "                tf_vectors = f.readlines()\n",
    "        \n",
    "                gestWords = []\n",
    "                tfVals = []\n",
    "        \n",
    "                #split the line into the word and tf value\n",
    "                for line in tf_vectors:\n",
    "                    noDash = line.split(\"-\")\n",
    "                    tf_val = noDash[-1]\n",
    "                    tf_val = tf_val.replace(\"\\n\", \"\")\n",
    "                    gestWords.append(noDash[0])\n",
    "                    tfVals.append(tf_val)\n",
    "           \n",
    "        \n",
    "                index = 0\n",
    "                startI = \"1\"\n",
    "                for y in range(1, w):\n",
    "                    startI = startI + \"1\"\n",
    "                startI = int(startI)\n",
    "        \n",
    "                #create dictionary with every word for every sensor and every directory\n",
    "                numWords = (startI * (2*r) - startI) * 20\n",
    "                wordMat = []\n",
    "    \n",
    "                for i in range(0, numWords + 20):\n",
    "                    wordMat.append(0)\n",
    "                \n",
    "                \n",
    "                # put tf values into matrix where column = word\n",
    "                for x in gestWords:\n",
    "                    word = x.split(\", \")\n",
    "                    sensorNum = word[1].replace(\"'\", \"\")\n",
    "                    wordID = word[2].replace(\"'\", \"\")\n",
    "                    wordID = wordID.replace(\")\", \"\")\n",
    "                \n",
    "                    wordID = int(wordID)\n",
    "                    sensorNum = int(sensorNum)\n",
    "                \n",
    "                    #axisSplit = len(wordMat) / 4\n",
    "                    sensorSplit = len(wordMat) / 20\n",
    "                    \n",
    "                    wordIndex = int(wordID + ((sensorNum - 1) * sensorSplit)) #+ ((axisNum - 1) * axisSplit)))\n",
    "                    wordIndex = wordIndex - startI\n",
    "            \n",
    "                    #temp\n",
    "                    if \"e\" in tfVals[index]:\n",
    "                        tfVals[index].replace(\"e\", \"\")\n",
    "                        tfVals[index].replace(\"-\", \"\")\n",
    "                        tfVals[index] = str(float(tfVals[index]) * 0.00001)\n",
    "                    #temp\n",
    "                    wordMat[wordIndex] = float(tfVals[index])\n",
    "                    index = index + 1\n",
    "                \n",
    "\n",
    "                if axisNum == 1:\n",
    "                    axis = 'W'\n",
    "                    Wmat.append(wordMat)\n",
    "                elif axisNum == 2:\n",
    "                    axis = 'X'\n",
    "                    Xmat.append(wordMat)\n",
    "                elif axisNum == 3:\n",
    "                    axis = 'Y'\n",
    "                    Ymat.append(wordMat)\n",
    "                elif axisNum == 4:\n",
    "                    axis = 'Z'\n",
    "                    Zmat.append(wordMat)\n",
    "                f.close()\n",
    "           \n",
    "        finalMat = np.append(Wmat, Xmat, axis = 1)\n",
    "        finalMat = np.append(finalMat, Ymat, axis = 1)\n",
    "        finalMat = np.append(finalMat, Zmat, axis = 1)\n",
    "        \n",
    "        #print(finalMat)\n",
    "        \n",
    "        return finalMat\n",
    "            #print(Xmat)\n",
    "    \n",
    "def createdictofComponents(topk, k):\n",
    "    #creates a matrix that contains tuples of the word and score\n",
    "    outputMat = []\n",
    "    outputMat = topk\n",
    "    startI = \"1\"\n",
    "    for y in range(1, w):\n",
    "        startI = startI + \"1\"\n",
    "    startI = int(startI)\n",
    "    numWords = (startI * (2*r) - startI)\n",
    "    \n",
    "    for j in range(0, len(topk.columns)):\n",
    "        axisSplit = len(topk) / 4\n",
    "        sensorSplit = axisSplit / 20\n",
    "        \n",
    "        ax = int(j/axisSplit)\n",
    "        sens = int((j - (ax * axisSplit))/sensorSplit)\n",
    "        word = int((j - ((sens * sensorSplit) + (ax * axisSplit))) - startI)\n",
    "        \n",
    "        for row in range(0, k):\n",
    "            if ax == 0:\n",
    "                axis = 'W'\n",
    "            elif ax == 1:\n",
    "                axis = 'X'\n",
    "            elif ax == 2:\n",
    "                axis = 'Y'\n",
    "            elif ax == 3:\n",
    "                axis = 'Z'\n",
    "            label = axis + str(sens) + str(word)\n",
    "            outputMat[j][row] = (topk[j][row], label)\n",
    "            \n",
    "        \n",
    "                \n",
    "            \"\"\"for sensor in range(0, 20):\n",
    "            \n",
    "                for word in range(0, numWords + 1):\n",
    "                    label = axis + str(sensor - 1) + str(word + startI)\n",
    "                    \n",
    "                    #axisSplit = len(topk) / 4\n",
    "                    sensorSplit = axisSplit / 20\n",
    "                    \n",
    "                    wordIndex = int((word + startI) + (sensor * sensorSplit) + (ax * axisSplit))\n",
    "                    wordIndex = wordIndex - startI\n",
    "                    outputMat[wordIndex][row] = (topk[wordIndex][row], label)\"\"\"\n",
    "                \n",
    "                \n",
    "    #for i in range(0, k):\n",
    "     #   word = \"w\" + str(i)\n",
    "      #  for j in range(0, len(topk)):\n",
    "       #     outputMat[i][j] = (topk[i][j], word)\n",
    "            \n",
    "    print(outputMat)\n",
    "    #sorts the words acording to their scores\n",
    "    outputMat = np.transpose(outputMat)\n",
    "    outputMat = outputMat.apply(lambda x: x.sort_values(ascending = False).values)\n",
    "    outputMat = np.transpose(outputMat)\n",
    "    finalMat = outputMat\n",
    "    #for i in range(0, k):\n",
    "     #   for j in range(0, len(outputMat)):\n",
    "      #      finalMat[i][j] = (outputMat[i][j][1], outputMat[i][j][0])\n",
    "            \n",
    "    file = open(\"./userOutput.txt\", \"w\")\n",
    "    file.write(str(finalMat))\n",
    "    file.close()\n",
    "    return finalMat\n",
    "    \n",
    "    \n",
    "def task1(vectModel, useOp, k):\n",
    "    \n",
    "    if useOp == \"PCA\":\n",
    "        #PCA\n",
    "        wordMat = makeMat(vectModel)\n",
    "        \n",
    "        topk = PCAsetup(wordMat, k)\n",
    "        print(\"\\nPCA:\\n\")\n",
    "        #print(topk)\n",
    "        \n",
    "        dictofComponents = createdictofComponents(topk, k)\n",
    "        print(dictofComponents)\n",
    "        \n",
    "        \n",
    "    elif useOp == \"SVD\":\n",
    "        wordMat = makeMat(vectModel)\n",
    "        topk = SVDsetup(wordMat, k)\n",
    "        print(\"\\nSVD:\\n\")\n",
    "        #print(topk)\n",
    "        \n",
    "        dictofComponents = createdictofComponents(topk, k)\n",
    "        print(dictofComponents)\n",
    "        \n",
    "        \n",
    "    elif useOp == \"NMF\":\n",
    "        wordMat = makeMat(vectModel)\n",
    "        topk = NMFsetup(wordMat, k)\n",
    "        print(\"\\nNMF:\\n\")\n",
    "        #print(topk)\n",
    "        \n",
    "        dictofComponents = createdictofComponents(topk, k)\n",
    "        print(dictofComponents)\n",
    "\n",
    "        \n",
    "    elif useOp == \"LDA\":\n",
    "        wordMat = makeMat(vectModel)\n",
    "        topk = LDAsetup(wordMat, k)\n",
    "        print(\"\\nLDA:\\n\")\n",
    "        #print(topk)\n",
    "        \n",
    "        dictofComponents = createdictofComponents(topk, k)\n",
    "        print(dictofComponents)\n",
    "        \n",
    "\n",
    "\n",
    "vectModel = input(\"Enter the vector model (ex: tf): \")\n",
    "k = input(\"Enter k (ex: 4): \")\n",
    "useOp = input(\"Enter the analysis you would like to use (ex: PCA): \")\n",
    "k = int(k)\n",
    "task1(vectModel, useOp, k)\n",
    "\n",
    "\n",
    "#sample output: \n",
    "#Enter the vector model: tf\n",
    "#Enter k: 10\n",
    "#Enter the analysis you would like to use: PCA\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter gesture file(e.g Data/1.csv) :\n",
      "* WARNING : Please make .pkl in TASK 1 before you use principal component\n",
      "Data/59.csv\n",
      "Enter vector model (tf, tfidf):\n",
      "tfidf\n",
      "Enter user options (1 ~ 7)\n",
      "* HINT : 1 = Dot similarity, 2 = PCA, 3 = SVD, 4 = NMF, 5 = LDA, 6 = Edit Distance, 7 = DTW\n",
      "7\n",
      "Most similar (gesture, score) \n",
      "(59, 0.0)\n",
      "(3, 517.226686729692)\n",
      "(0, 546.2733304634784)\n",
      "(32, 548.2429271479214)\n",
      "(42, 588.7590218944844)\n",
      "(14, 595.3573641879669)\n",
      "(2, 599.1992888542443)\n",
      "(45, 606.6645926870148)\n",
      "(6, 630.4273810272091)\n",
      "(36, 636.7623912495804)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity                          \n",
    "from math import log\n",
    "import re\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy import spatial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log2\n",
    "\n",
    "\n",
    "def dot_similarity(gesture1, gesture2):                  \n",
    "    x = np.array(gesture1)\n",
    "    y = np.array(gesture2)\n",
    "    \"\"\"if len(x) > len(y):\n",
    "        y = np.pad(y, (0, len(x) - len(y)))\n",
    "    else:\n",
    "        x = np.pad(x, (0, len(y) - len(x)))\"\"\"\n",
    "    return np.dot(x, y)\n",
    "\n",
    "def pears_similarity(vec1, vec2):\n",
    "    pearson_coef, p_value = stats.pearsonr(vec1, vec2)\n",
    "    return pearson_coef\n",
    "    \n",
    "def cos_similarity(vec1, vec2):                            \n",
    "    x = np.array(vec1)\n",
    "    y = np.array(vec2)                                                    \n",
    "    similarity = 1- spatial.distance.cosine(x, y)                             \n",
    "                                                                            \n",
    "    return similarity                             \n",
    "\n",
    "def KL_div_similarity(p, q):\n",
    "    return sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n",
    "\n",
    "def edit_distance(sensor1, sensor2, m, n):\n",
    "    # Create a table to store results of subproblems \n",
    "    dp = [[0 for x in range(n + 1)] for x in range(m + 1)] \n",
    "  \n",
    "    # Fill d[][] in bottom up manner \n",
    "    for i in range(m + 1): \n",
    "        for j in range(n + 1): \n",
    "  \n",
    "            # If first string is empty, only option is to \n",
    "            # insert all characters of second string \n",
    "            if i == 0: \n",
    "                dp[i][j] = j    # Min. operations = j \n",
    "  \n",
    "            # If second string is empty, only option is to \n",
    "            # remove all characters of second string \n",
    "            elif j == 0: \n",
    "                dp[i][j] = i    # Min. operations = i \n",
    "  \n",
    "            # If last characters are same, ignore last char \n",
    "            # and recur for remaining string \n",
    "            elif sensor1[i-1] == sensor2[j-1]: \n",
    "                dp[i][j] = dp[i-1][j-1] \n",
    "  \n",
    "            # If last character are different, consider all \n",
    "            # possibilities and find minimum \n",
    "            else: \n",
    "                dp[i][j] = 1 + min(dp[i][j-1],        # Insert \n",
    "                                   dp[i-1][j],        # Remove \n",
    "                                   dp[i-1][j-1])    # Replace \n",
    "  \n",
    "    return dp[m][n] \n",
    "    \n",
    "def dynamic_time_warping(sensor1, sensor2, m, n):\n",
    "    dp = [[0 for x in range(n+1)] for x in range(m+1)] \n",
    "    \n",
    "    for i in range(1, m+1): \n",
    "        for j in range(1, n+1): \n",
    "            dp[i][j] = abs(sensor1[i-1] - sensor2[j-1])\n",
    "             \n",
    "            if i == 1 and j == 1:\n",
    "                continue\n",
    "            elif i == 1 and j != 1:\n",
    "                dp[i][j] += dp[i][j-1] \n",
    "            elif i != 1 and j == 1:\n",
    "                dp[i][j] += dp[i-1][j]\n",
    "            else:\n",
    "                dp[i][j] += min(dp[i][j-1],        # Insert \n",
    "                                dp[i-1][j],        # Remove \n",
    "                                dp[i-1][j-1])    # Replace \n",
    "            \n",
    "    return dp[m][n] \n",
    "\n",
    "def tfidf_loader(directory):\n",
    "    gestures = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.startswith(\"tfidf_\") or not filename.endswith(\".txt\") :\n",
    "            continue\n",
    "            \n",
    "        path = directory+\"/\"+filename\n",
    "        with open(path, \"r\") as w:\n",
    "            gesture = w.readlines()\n",
    "\n",
    "        matrix=[]\n",
    "\n",
    "        for sensor in gesture :\n",
    "            tfidf = float(sensor.split(\"-\")[1].strip())\n",
    "            matrix.append(tfidf)\n",
    "                \n",
    "        gestures.append(matrix)\n",
    "        \n",
    "    return gestures\n",
    "    \n",
    "    \n",
    "def tf_loader(directory):\n",
    "    gestures = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.startswith(\"tf_\") or not filename.endswith(\".txt\") :\n",
    "            continue\n",
    "            \n",
    "        path = directory+\"/\"+filename\n",
    "        with open(path, \"r\") as w:\n",
    "            gesture = w.readlines()\n",
    "\n",
    "        matrix=[]\n",
    "\n",
    "        for sensor in gesture :\n",
    "            tf = float(sensor.split(\"-\")[1].strip())   \n",
    "            matrix.append(tf)\n",
    "        gestures.append(matrix)\n",
    "        \n",
    "    return gestures\n",
    "    \n",
    "def symbol_loader(directory):\n",
    "    gestures = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".wrd\") :\n",
    "            continue\n",
    "        path = directory+\"/\"+filename\n",
    "        with open(path, \"r\") as w:\n",
    "            gesture = w.readlines()\n",
    "\n",
    "        matrix=[]\n",
    "\n",
    "        for sensor in gesture :\n",
    "            sym_quant_window = [re.findall(r'\\d+',word)[0] for word in sensor.split(\" - \")[1].split(\",\")] \n",
    "            matrix.append(sym_quant_window)\n",
    "                \n",
    "        gestures.append(matrix)\n",
    "        \n",
    "    return gestures\n",
    "\n",
    "def amplitude_loader(directory):\n",
    "    gestures = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".wrd\") :\n",
    "            continue\n",
    "        path = directory+\"/\"+filename\n",
    "        with open(path, \"r\") as w:\n",
    "            gesture = w.readlines()\n",
    "\n",
    "        matrix=[]\n",
    "\n",
    "        for sensor in gesture :\n",
    "            avg_quant_ampitude = [float(word) for word in sensor.split(\" - \")[0].split(\"[\")[1].replace(\"],\",\"\").split(\",\")]\n",
    "            matrix.append(avg_quant_ampitude)\n",
    "                \n",
    "        gestures.append(matrix)\n",
    "        \n",
    "    return gestures\n",
    "\n",
    "###### GET Input from users######\n",
    "print(\"Enter gesture file(e.g Data/1.csv) :\")\n",
    "print(\"* WARNING : Please make .pkl in TASK 1 before you use principal component\")\n",
    "gesture_path = input()\n",
    "\n",
    "directory = gesture_path.split(\"/\")[0]\n",
    "#axis = gesture_path.split(\"/\")[1]\n",
    "filename = gesture_path.split(\"/\")[1]\n",
    "key_idx = int(re.findall(r'\\d+', filename)[0])\n",
    "\n",
    "print(\"Enter vector model (tf, tfidf):\")\n",
    "vector_model = input()\n",
    "\n",
    "print(\"Enter user options (1 ~ 7)\")\n",
    "print(\"* HINT : 1 = Dot similarity, 2 = PCA, 3 = SVD, 4 = NMF, 5 = LDA, 6 = Edit Distance, 7 = DTW\")\n",
    "user_option = int(input())  \n",
    "\n",
    "#Retrieve only top 10 gestures with high similarity\n",
    "top_K = 10\n",
    "\n",
    "#Calculate similarity(cost) based on User options\n",
    "if user_option == 1 :\n",
    "    directory = directory + \"/\"\n",
    "    gestures = makeMat(vector_model)\n",
    "    directory = directory.replace(\"/\", \"\")\n",
    "    \n",
    "    cost=[]\n",
    "    key_gesture = gestures[key_idx]\n",
    "    for gesture in gestures :\n",
    "        similarity = dot_similarity(key_gesture, gesture)\n",
    "        cost.append(similarity)\n",
    "    cost_top_K = sorted(cost, reverse=True)[0:top_K]                                        \n",
    "    \n",
    "elif user_option == 2 :\n",
    "    PC_path = [\"PCA\", vector_model]\n",
    "    PC_path = \"_\".join(PC_path)\n",
    "    pca = pd.read_pickle(PC_path + \".pkl\")\n",
    "    num = len(pca[0])\n",
    "\n",
    "    pca = pca.T\n",
    "    key_vec = pca[key_idx]\n",
    "    cost=[]\n",
    "    for idx in range(num) :\n",
    "        similarity = pears_similarity(key_vec, pca[idx])\n",
    "        cost.append(similarity)\n",
    "    cost_top_K = sorted(cost, reverse=True)[0:top_K]                                        \n",
    "\n",
    "elif user_option == 3 : \n",
    "    PC_path = [\"SVD\", vector_model]\n",
    "    PC_path = \"_\".join(PC_path)\n",
    "    svd = pd.read_pickle(PC_path + \".pkl\")                                      \n",
    "    num = len(svd[0])                                                           \n",
    "                                                                                \n",
    "    svd = svd.T                                                                 \n",
    "    key_vec = svd[key_idx]      \n",
    "    \n",
    "    cost=[]                                                                     \n",
    "    for idx in range(num) :                                                     \n",
    "        similarity = cos_similarity(key_vec, svd[idx])                            \n",
    "        cost.append(similarity)                 \n",
    "    cost_top_K = sorted(cost, reverse=True)[0:top_K]                                        \n",
    "    \n",
    "elif user_option == 4 :                                                         \n",
    "    PC_path = [\"NMF\", vector_model]\n",
    "    PC_path = \"_\".join(PC_path)\n",
    "    nmf = pd.read_pickle(PC_path + \".pkl\")\n",
    "    num = len(nmf[0])                                                           \n",
    "                                                                                \n",
    "    nmf = nmf.T                                                                 \n",
    "    key_vec = nmf[key_idx]    \n",
    "    \n",
    "    cost=[]                                                                     \n",
    "    for idx in range(num) :                                                     \n",
    "        similarity = cos_similarity(key_vec, nmf[idx])                            \n",
    "        cost.append(similarity)           \n",
    "    cost_top_K = sorted(cost, reverse=True)[0:top_K]                                        \n",
    "                  \n",
    "elif user_option == 5 :                                                         \n",
    "    PC_path = [\"LDA\", vector_model]\n",
    "    PC_path = \"_\".join(PC_path)\n",
    "    lda = pd.read_pickle(PC_path + \".pkl\")\n",
    "    num = len(lda[0])                                                           \n",
    "                                                                                \n",
    "    lda = lda.T                                                                 \n",
    "    key_vec = lda[key_idx]    \n",
    "    \n",
    "    cost=[]                                                                     \n",
    "    for idx in range(num) :                                                     \n",
    "        similarity = KL_div_similarity(key_vec, lda[idx])                            \n",
    "        cost.append(similarity)                                  \n",
    "    cost_top_K = sorted(cost, reverse=True)[0:top_K]                                        \n",
    "\n",
    "elif user_option == 6 :\n",
    "    pathW = directory + \"/W\"\n",
    "    pathX = directory + \"/X\"\n",
    "    pathY = directory + \"/Y\"\n",
    "    pathZ = directory + \"/Z\"\n",
    "    gesturesW = symbol_loader(pathW)\n",
    "    gesturesX = symbol_loader(pathX)\n",
    "    gesturesY = symbol_loader(pathY)\n",
    "    gesturesZ = symbol_loader(pathZ)\n",
    "    \n",
    "    gestures = np.append(gesturesW, gesturesX, axis = 1)\n",
    "    gestures = np.append(gestures, gesturesY, axis = 1)\n",
    "    gestures = np.append(gestures, gesturesZ, axis = 1)\n",
    "        \n",
    "    key_gesture = gestures[key_idx]\n",
    "    \n",
    "    cost=[0]*len(gestures)\n",
    "    for i, gesture in enumerate(gestures) :\n",
    "        for j, sensor in enumerate(gesture) :\n",
    "            n = len(sensor)\n",
    "            m = len(key_gesture[j])\n",
    "            cost[i] += edit_distance(key_gesture[j], sensor, m, n)\n",
    "    cost_top_K = sorted(cost)[0:top_K]                                        \n",
    "\n",
    "elif user_option == 7 :\n",
    "    pathW = directory + \"/W\"\n",
    "    pathX = directory + \"/X\"\n",
    "    pathY = directory + \"/Y\"\n",
    "    pathZ = directory + \"/Z\"\n",
    "    gesturesW = amplitude_loader(pathW)\n",
    "    gesturesX = amplitude_loader(pathX)\n",
    "    gesturesY = amplitude_loader(pathY)\n",
    "    gesturesZ = amplitude_loader(pathZ)\n",
    "    \n",
    "    gestures = np.append(gesturesW, gesturesX, axis = 1)\n",
    "    gestures = np.append(gestures, gesturesY, axis = 1)\n",
    "    gestures = np.append(gestures, gesturesZ, axis = 1)\n",
    "    \n",
    "    key_gesture = gestures[key_idx]\n",
    "    \n",
    "    cost=[0]*len(gestures)\n",
    "    for i, gesture in enumerate(gestures) :                                     \n",
    "        for j, sensor in enumerate(gesture) :                                   \n",
    "            n = len(sensor)                                                     \n",
    "            m = len(key_gesture[j])                                             \n",
    "            cost[i] += dynamic_time_warping(key_gesture[j], sensor, m, n)    \n",
    "    cost_top_K = sorted(cost)[0:top_K]                                        \n",
    "else:\n",
    "    print(\"ERROR : No such user option in this program\")\n",
    "    \n",
    "                                                           \n",
    "print(\"Most similar (gesture, score) \")              \n",
    "for k in cost_top_K :                                                       \n",
    "    print((cost.index(k), k))       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the following inputs as the same values you used for task 0: \n",
      "Enter the window length (ex: 3): 3\n",
      "Enter the shift length (ex: 3): 3\n",
      "Enter the resolution (ex: 3): 3\n",
      "Enter directory (e.g. Data/) :\n",
      "Dataset 5/\n",
      "Enter value p :\n",
      "3\n",
      "Enter user options (1 ~ 7)\n",
      "* HINT : 1 = Dot similarity, 2 = PCA, 3 = SVD, 4 = NMF, 5 = LDA, 6 = Edit Distance, 7 = DTW\n",
      "2\n",
      "Enter vector model (tf, tfidf):\n",
      "tf\n",
      "Number of gesture :  1023\n",
      "  (0, 0)\t1.0\n",
      "  (0, 1)\t-0.8721963066757121\n",
      "  (0, 2)\t-0.8923259439210433\n",
      "  (0, 3)\t-0.8676868631121248\n",
      "  (0, 4)\t-0.8940454524227719\n",
      "  (0, 5)\t-0.9307711518318526\n",
      "  (0, 6)\t-0.00844530874796971\n",
      "  (0, 7)\t-0.8675286397820352\n",
      "  (0, 8)\t-0.9011659499493458\n",
      "  (0, 9)\t-0.8909616212470274\n",
      "  (0, 10)\t-0.8879130965872309\n",
      "  (0, 11)\t0.99448020672601\n",
      "  (0, 12)\t-0.8531902287350643\n",
      "  (0, 13)\t-0.8563270050504989\n",
      "  (0, 14)\t-0.9009189454004138\n",
      "  (0, 15)\t-0.919362808372269\n",
      "  (0, 16)\t-0.9072724254161018\n",
      "  (0, 17)\t0.03457545022440288\n",
      "  (0, 18)\t-0.9051137867622205\n",
      "  (0, 19)\t-0.9238377687558991\n",
      "  (0, 20)\t-0.6276334315669285\n",
      "  (0, 21)\t-0.8352310117279691\n",
      "  (0, 22)\t0.985146155065034\n",
      "  (0, 23)\t-0.9374358017471797\n",
      "  (0, 24)\t-0.9441506351882056\n",
      "  :\t:\n",
      "  (1022, 998)\t0.9999999651805163\n",
      "  (1022, 999)\t0.9999999411013324\n",
      "  (1022, 1000)\t0.9999988536566642\n",
      "  (1022, 1001)\t0.9999974433147899\n",
      "  (1022, 1002)\t0.9999997740595461\n",
      "  (1022, 1003)\t0.999999906402011\n",
      "  (1022, 1004)\t0.9999999934948595\n",
      "  (1022, 1005)\t0.9999998902372298\n",
      "  (1022, 1006)\t0.9999999604888046\n",
      "  (1022, 1007)\t0.9999971732522277\n",
      "  (1022, 1008)\t0.999999901502011\n",
      "  (1022, 1009)\t0.9999988405714824\n",
      "  (1022, 1010)\t0.9999998932621287\n",
      "  (1022, 1011)\t0.9999983823541009\n",
      "  (1022, 1012)\t0.9999974433147899\n",
      "  (1022, 1013)\t0.9999997005094182\n",
      "  (1022, 1014)\t0.9999999628111699\n",
      "  (1022, 1015)\t0.9999999416381797\n",
      "  (1022, 1016)\t0.9999998013259295\n",
      "  (1022, 1017)\t0.9999984526648421\n",
      "  (1022, 1018)\t0.999999420332145\n",
      "  (1022, 1019)\t0.9999978789795505\n",
      "  (1022, 1020)\t0.9999999917084555\n",
      "  (1022, 1021)\t0.9999997047898562\n",
      "  (1022, 1022)\t1.0\n",
      "------TOP P SVD after  PCA -----\n",
      "* ordered in gesture, score\n",
      "           0             1             2             3             4     \\\n",
      "0  2.994012e+01 -3.045764e+01 -3.072495e+01 -3.039327e+01 -3.074611e+01   \n",
      "1 -1.964485e+00 -1.526321e+00 -1.236659e+00 -1.587868e+00 -1.210660e+00   \n",
      "2 -8.057219e-16 -6.242976e-16 -8.075318e-16  2.661485e-15  1.115428e-15   \n",
      "\n",
      "           5             6             7             8             9     \\\n",
      "0 -3.111415e+01 -9.133087e+00 -3.039099e+01 -3.083051e+01 -3.070797e+01   \n",
      "1 -5.929411e-01 -6.606290e+00 -1.590007e+00 -1.100624e+00 -1.257135e+00   \n",
      "2 -1.448082e-15  1.444201e-16  1.519902e-15  6.233168e-17 -5.558147e-16   \n",
      "\n",
      "       ...               1013          1014          1015          1016  \\\n",
      "0      ...       3.118022e+01  3.117934e+01  3.117825e+01  3.117773e+01   \n",
      "1      ...       3.875121e-01  3.909692e-01  3.952066e-01  3.971978e-01   \n",
      "2      ...      -1.846236e-15  1.737317e-15 -1.409648e-15 -1.847153e-15   \n",
      "\n",
      "           1017          1018          1019          1020          1021  \\\n",
      "0  3.117569e+01  3.117693e+01  3.117514e+01  3.117863e+01  3.118022e+01   \n",
      "1  4.049828e-01  4.002763e-01  4.070548e-01  3.937384e-01  3.875504e-01   \n",
      "2 -2.342090e-15  2.461838e-15 -2.238438e-15 -1.364246e-15 -9.074700e-16   \n",
      "\n",
      "           1022  \n",
      "0  3.117886e+01  \n",
      "1  3.928502e-01  \n",
      "2 -5.867329e-16  \n",
      "\n",
      "[3 rows x 1023 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to NMF (input X)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-6156d13bc0e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[0mnmf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNMF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m \u001b[0mpc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnmf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\nmf.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, W, H)\u001b[0m\n\u001b[0;32m   1248\u001b[0m             \u001b[0ml1_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml1_ratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'both'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1249\u001b[0m             \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1250\u001b[1;33m             shuffle=self.shuffle)\n\u001b[0m\u001b[0;32m   1251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1252\u001b[0m         self.reconstruction_err_ = _beta_divergence(X, W, H, self.beta_loss,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\nmf.py\u001b[0m in \u001b[0;36mnon_negative_factorization\u001b[1;34m(X, W, H, n_components, init, update_H, solver, beta_loss, tol, max_iter, alpha, l1_ratio, regularization, random_state, verbose, shuffle)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m     \u001b[0mcheck_non_negative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"NMF (input X)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m     \u001b[0mbeta_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_string_param\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularization\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_non_negative\u001b[1;34m(X, whom)\u001b[0m\n\u001b[0;32m    966\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 968\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Negative values in data passed to %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mwhom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: Negative values in data passed to NMF (input X)"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity                          \n",
    "from math import log\n",
    "import re\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy import spatial\n",
    "import pandas as pd\n",
    "import numpy as np                  \n",
    "from math import log2  \n",
    "import glob\n",
    "\n",
    "numbers = re.compile(r'(\\d+)')\n",
    "\n",
    "print(\"Please enter the following inputs as the same values you used for task 0: \")\n",
    "#directory = input(\"Enter the data directory path (ex: Data/: \")\n",
    "w = input(\"Enter the window length (ex: 3): \")\n",
    "s = input(\"Enter the shift length (ex: 3): \")\n",
    "r = input(\"Enter the resolution (ex: 3): \")\n",
    "\n",
    "w = int(w)\n",
    "s = int(s)\n",
    "r = int(r)\n",
    "\n",
    "def numericalSort(value):\n",
    "    parts = numbers.split(value)\n",
    "    parts[1::2] = map(int, parts[1::2])\n",
    "    return parts\n",
    "                               \n",
    "def makeMat(vectModel):    \n",
    "    #read files\n",
    "    Wmat = []\n",
    "    Xmat = []\n",
    "    Ymat = []\n",
    "    Zmat = []\n",
    "    if vectModel == \"tf\":\n",
    "        for axisNum in range(1, 5):\n",
    "            if axisNum == 1:\n",
    "                axis = 'W'\n",
    "            elif axisNum == 2:\n",
    "                axis = 'X'\n",
    "            elif axisNum == 3:\n",
    "                axis = 'Y'\n",
    "            elif axisNum == 4:\n",
    "                axis = 'Z'\n",
    "                \n",
    "            for file in glob.glob(directory + axis + \"/tf_vectors_*.txt\"):\n",
    "                #Xmat = []\n",
    "                #read tf file\n",
    "                f = open(file, \"r\")\n",
    "                tf_vectors = f.readlines()\n",
    "        \n",
    "                gestWords = []\n",
    "                tfVals = []\n",
    "        \n",
    "                #split the line into the word and tf value\n",
    "                for line in tf_vectors:\n",
    "                    noDash = line.split(\"-\")\n",
    "                    tf_val = noDash[1]\n",
    "                    tf_val = tf_val.replace(\"\\n\", \"\")\n",
    "                    gestWords.append(noDash[0])\n",
    "                    tfVals.append(tf_val)\n",
    "           \n",
    "        \n",
    "                index = 0\n",
    "                startI = \"1\"\n",
    "                for y in range(1, w):\n",
    "                    startI = startI + \"1\"\n",
    "                startI = int(startI)\n",
    "        \n",
    "                #create dictionary with every word for every sensor and every directory\n",
    "                numWords = (startI * (2*r) - startI) * 20\n",
    "                wordMat = []\n",
    "    \n",
    "                for i in range(0, numWords + 20):\n",
    "                    wordMat.append(0)\n",
    "                \n",
    "                # put tf values into matrix where column = word\n",
    "                for x in gestWords:\n",
    "                    word = x.split(\", \")\n",
    "                    sensorNum = word[1].replace(\"'\", \"\")\n",
    "                    wordID = word[2].replace(\"'\", \"\")\n",
    "                    wordID = wordID.replace(\")\", \"\")\n",
    "                \n",
    "                    wordID = int(wordID)\n",
    "                    sensorNum = int(sensorNum)\n",
    "                \n",
    "                    #axisSplit = len(wordMat) / 4\n",
    "                    sensorSplit = len(wordMat) / 20\n",
    "                    \n",
    "                    wordIndex = int(wordID + ((sensorNum - 1) * sensorSplit)) #+ ((axisNum - 1) * axisSplit)))\n",
    "                    wordIndex = wordIndex - startI\n",
    "            \n",
    "                    wordMat[wordIndex] = float(tfVals[index])\n",
    "                    index = index + 1\n",
    "                \n",
    "\n",
    "                if axisNum == 1:\n",
    "                    axis = 'W'\n",
    "                    Wmat.append(wordMat)\n",
    "                elif axisNum == 2:\n",
    "                    axis = 'X'\n",
    "                    Xmat.append(wordMat)\n",
    "                elif axisNum == 3:\n",
    "                    axis = 'Y'\n",
    "                    Ymat.append(wordMat)\n",
    "                elif axisNum == 4:\n",
    "                    axis = 'Z'\n",
    "                    Zmat.append(wordMat)\n",
    "                f.close()\n",
    "                \n",
    "        finalMat = np.append(Wmat, Xmat, axis = 1)\n",
    "        finalMat = np.append(finalMat, Ymat, axis = 1)\n",
    "        finalMat = np.append(finalMat, Zmat, axis = 1)\n",
    "        \n",
    "        #print(finalMat)\n",
    "        \n",
    "        return finalMat\n",
    "            #print(Xmat)\n",
    "    \n",
    "    elif vectModel == \"tfidf\":\n",
    "        for axisNum in range(1, 5):\n",
    "            if axisNum == 1:\n",
    "                axis = 'W'\n",
    "            elif axisNum == 2:\n",
    "                axis = 'X'\n",
    "            elif axisNum == 3:\n",
    "                axis = 'Y'\n",
    "            elif axisNum == 4:\n",
    "                axis = 'Z'\n",
    "                \n",
    "            for file in glob.glob(directory + axis + \"/tfidf_vectors_*.txt\"):\n",
    "                #read tf file\n",
    "                f = open(file, \"r\")\n",
    "                tf_vectors = f.readlines()\n",
    "        \n",
    "                gestWords = []\n",
    "                tfVals = []\n",
    "        \n",
    "                #split the line into the word and tf value\n",
    "                for line in tf_vectors:\n",
    "                    noDash = line.split(\"-\")\n",
    "                    tf_val = noDash[1]\n",
    "                    tf_val = tf_val.replace(\"\\n\", \"\")\n",
    "                    gestWords.append(noDash[0])\n",
    "                    tfVals.append(tf_val)\n",
    "           \n",
    "        \n",
    "                index = 0\n",
    "                startI = \"1\"\n",
    "                for y in range(1, w):\n",
    "                    startI = startI + \"1\"\n",
    "                startI = int(startI)\n",
    "        \n",
    "                #create dictionary with every word for every sensor and every directory\n",
    "                numWords = (startI * (2*r) - startI) * 20\n",
    "                wordMat = []\n",
    "    \n",
    "                for i in range(0, numWords + 20):\n",
    "                    wordMat.append(0)\n",
    "                \n",
    "                # put tf values into matrix where column = word\n",
    "                for x in gestWords:\n",
    "                    word = x.split(\", \")\n",
    "                    sensorNum = word[1].replace(\"'\", \"\")\n",
    "                    wordID = word[2].replace(\"'\", \"\")\n",
    "                    wordID = wordID.replace(\")\", \"\")\n",
    "                \n",
    "                    wordID = int(wordID)\n",
    "                    sensorNum = int(sensorNum)\n",
    "                \n",
    "                    #axisSplit = len(wordMat) / 4\n",
    "                    sensorSplit = len(wordMat) / 20\n",
    "                    \n",
    "                    wordIndex = int(wordID + ((sensorNum - 1) * sensorSplit)) #+ ((axisNum - 1) * axisSplit)))\n",
    "                    wordIndex = wordIndex - startI\n",
    "            \n",
    "                    wordMat[wordIndex] = float(tfVals[index])\n",
    "                    index = index + 1\n",
    "                \n",
    "\n",
    "                if axisNum == 1:\n",
    "                    axis = 'W'\n",
    "                    Wmat.append(wordMat)\n",
    "                elif axisNum == 2:\n",
    "                    axis = 'X'\n",
    "                    Xmat.append(wordMat)\n",
    "                elif axisNum == 3:\n",
    "                    axis = 'Y'\n",
    "                    Ymat.append(wordMat)\n",
    "                elif axisNum == 4:\n",
    "                    axis = 'Z'\n",
    "                    Zmat.append(wordMat)\n",
    "                f.close()\n",
    "           \n",
    "        finalMat = np.append(Wmat, Xmat, axis = 1)\n",
    "        finalMat = np.append(finalMat, Ymat, axis = 1)\n",
    "        finalMat = np.append(finalMat, Zmat, axis = 1)\n",
    "        \n",
    "        #print(finalMat)\n",
    "        \n",
    "        return finalMat\n",
    "            #print(Xmat)                                                       \n",
    "                                                                                \n",
    "def KL_div_similarity(p, q):                                                    \n",
    "    return sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n",
    "\n",
    "def dot_similarity(gesture1, gesture2):                  \n",
    "    x = np.array(gesture1)\n",
    "    y = np.array(gesture2)\n",
    "    \"\"\"if len(x) > len(y):\n",
    "        y = np.pad(y, (0, len(x) - len(y)))\n",
    "    else:\n",
    "        x = np.pad(x, (0, len(y) - len(x)))\"\"\"\n",
    "    return np.dot(x, y)\n",
    "\n",
    "def pears_similarity(vec1, vec2):\n",
    "    pearson_coef, p_value = stats.pearsonr(vec1, vec2)\n",
    "    return pearson_coef\n",
    "    \n",
    "def cos_similarity(vec1, vec2):                            \n",
    "    x = np.array(vec1)\n",
    "    y = np.array(vec2)                                                    \n",
    "    similarity = 1- spatial.distance.cosine(x, y)                             \n",
    "                                                                            \n",
    "    return similarity                             \n",
    "\n",
    "def edit_distance(sensor1, sensor2, m, n):\n",
    "    # Create a table to store results of subproblems \n",
    "    dp = [[0 for x in range(n + 1)] for x in range(m + 1)] \n",
    "  \n",
    "    # Fill d[][] in bottom up manner \n",
    "    for i in range(m + 1): \n",
    "        for j in range(n + 1): \n",
    "  \n",
    "            # If first string is empty, only option is to \n",
    "            # insert all characters of second string \n",
    "            if i == 0: \n",
    "                dp[i][j] = j    # Min. operations = j \n",
    "  \n",
    "            # If second string is empty, only option is to \n",
    "            # remove all characters of second string \n",
    "            elif j == 0: \n",
    "                dp[i][j] = i    # Min. operations = i \n",
    "  \n",
    "            # If last characters are same, ignore last char \n",
    "            # and recur for remaining string \n",
    "            elif sensor1[i-1] == sensor2[j-1]: \n",
    "                dp[i][j] = dp[i-1][j-1] \n",
    "  \n",
    "            # If last character are different, consider all \n",
    "            # possibilities and find minimum \n",
    "            else: \n",
    "                dp[i][j] = 1 + min(dp[i][j-1],        # Insert \n",
    "                                   dp[i-1][j],        # Remove \n",
    "                                   dp[i-1][j-1])    # Replace \n",
    "  \n",
    "    return dp[m][n] \n",
    "    \n",
    "def dynamic_time_warping(sensor1, sensor2, m, n):\n",
    "    dp = [[0 for x in range(n+1)] for x in range(m+1)] \n",
    "    \n",
    "    for i in range(1, m+1): \n",
    "        for j in range(1, n+1): \n",
    "            dp[i][j] = abs(sensor1[i-1] - sensor2[j-1])\n",
    "             \n",
    "            if i == 1 and j == 1:\n",
    "                continue\n",
    "            elif i == 1 and j != 1:\n",
    "                dp[i][j] += dp[i][j-1] \n",
    "            elif i != 1 and j == 1:\n",
    "                dp[i][j] += dp[i-1][j]\n",
    "            else:\n",
    "                dp[i][j] += min(dp[i][j-1],        # Insert \n",
    "                                dp[i-1][j],        # Remove \n",
    "                                dp[i-1][j-1])    # Replace \n",
    "            \n",
    "    return dp[m][n] \n",
    "\n",
    "def tfidf_loader(directory):\n",
    "    gestures = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.startswith(\"tfidf_\") or not filename.endswith(\".txt\") :\n",
    "            continue\n",
    "            \n",
    "        path = directory+\"/\"+filename\n",
    "        with open(path, \"r\") as w:\n",
    "            gesture = w.readlines()\n",
    "\n",
    "        matrix=[]\n",
    "\n",
    "        for sensor in gesture :\n",
    "            tfidf = float(sensor.split(\"-\")[1].strip())\n",
    "            matrix.append(tfidf)\n",
    "                \n",
    "        gestures.append(matrix)\n",
    "        \n",
    "    return gestures\n",
    "    \n",
    "    \n",
    "def tf_loader(directory):\n",
    "    gestures = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.startswith(\"tf_\") or not filename.endswith(\".txt\") :\n",
    "            continue\n",
    "            \n",
    "        path = directory+\"/\"+filename\n",
    "        with open(path, \"r\") as w:\n",
    "            gesture = w.readlines()\n",
    "\n",
    "        matrix=[]\n",
    "\n",
    "        for sensor in gesture :\n",
    "            tf = float(sensor.split(\"-\")[1].strip())   \n",
    "            matrix.append(tf)\n",
    "        gestures.append(matrix)\n",
    "        \n",
    "    return gestures\n",
    "    \n",
    "def symbol_loader(directory):\n",
    "    gestures = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".wrd\") :\n",
    "            continue\n",
    "        path = directory+\"/\"+filename\n",
    "        with open(path, \"r\") as w:\n",
    "            gesture = w.readlines()\n",
    "\n",
    "        matrix=[]\n",
    "\n",
    "        for sensor in gesture :\n",
    "            sym_quant_window = [re.findall(r'\\d+',word)[0] for word in sensor.split(\" - \")[1].split(\",\")] \n",
    "            matrix.append(sym_quant_window)\n",
    "                \n",
    "        gestures.append(matrix)\n",
    "        \n",
    "    return gestures\n",
    "\n",
    "def amplitude_loader(directory):\n",
    "    gestures = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".wrd\") :\n",
    "            continue\n",
    "        path = directory+\"/\"+filename\n",
    "        with open(path, \"r\") as w:\n",
    "            gesture = w.readlines()\n",
    "\n",
    "        matrix=[]\n",
    "\n",
    "        for sensor in gesture :\n",
    "            avg_quant_ampitude = [float(word) for word in sensor.split(\" - \")[0].split(\"[\")[1].replace(\"],\",\"\").split(\",\")]\n",
    "            matrix.append(avg_quant_ampitude)\n",
    "                \n",
    "        gestures.append(matrix)\n",
    "        \n",
    "    return gestures\n",
    "\n",
    "print(\"Enter directory (e.g. Data/) :\")\n",
    "directory = input()\n",
    "#axis = directory.split(\"/\")[1]\n",
    "\n",
    "print(\"Enter value p :\")\n",
    "p = int(input())\n",
    "print(\"Enter user options (1 ~ 7)\")\n",
    "print(\"* HINT : 1 = Dot similarity, 2 = PCA, 3 = SVD, 4 = NMF, 5 = LDA, 6 = Edit Distance, 7 = DTW\")\n",
    "user_option = int(input())  \n",
    "\n",
    "print(\"Enter vector model (tf, tfidf):\")\n",
    "vector_model = input()\n",
    "\n",
    "num_gestures = 0\n",
    "for filename in os.listdir(directory + \"X\"):                                          \n",
    "    if not filename.endswith(\".csv\") :                                          \n",
    "        continue\n",
    "    num_gestures+=1\n",
    "print(\"Number of gesture : \", num_gestures)\n",
    "gest_gest_sim = [0.0]*num_gestures\n",
    "gest_index = 1\n",
    "for filename in sorted(glob.glob(directory + \"/X\" + \"/*.csv\"), key=numericalSort):                                          \n",
    "    #if not filename.endswith(\".csv\") :                                          \n",
    "     #   continue                    \n",
    "    #get indext of file                                            \n",
    "    key_idx = gest_index\n",
    "    \n",
    "\n",
    "    if user_option == 1:\n",
    "        outname=\"DP\"\n",
    "        \n",
    "        gestures = makeMat(vector_model)\n",
    "        \n",
    "        #gestures = tf_loader(directory)\n",
    "        key_gesture = gestures[key_idx-1]\n",
    "        cost=[]\n",
    "        for gesture in gestures :\n",
    "            similarity = dot_similarity(key_gesture, gesture)\n",
    "            cost.append(similarity)\n",
    "        gest_gest_sim[key_idx-1] = cost\n",
    "    \n",
    "    elif user_option == 2:                                                        \n",
    "        outname=\"PCA\"\n",
    "        PC_path = [\"PCA\", vector_model]\n",
    "        PC_path = \"_\".join(PC_path)\n",
    "\n",
    "        pca = pd.read_pickle(PC_path + \".pkl\")\n",
    "        num = len(pca[0])\n",
    "        pca = pca.T\n",
    "        key_vec = pca[key_idx-1]\n",
    "        cost=[]\n",
    "        for idx in range(num) :\n",
    "            similarity = pears_similarity(key_vec, pca[idx])\n",
    "            cost.append(similarity)\n",
    "        gest_gest_sim[key_idx-1] = cost                                         \n",
    "    elif user_option == 3:\n",
    "        outname=\"SVD\"\n",
    "        PC_path = [\"SVD\", vector_model]\n",
    "        PC_path = \"_\".join(PC_path)\n",
    "        svd = pd.read_pickle(PC_path + \".pkl\")                                      \n",
    "        num = len(svd[0])                                                           \n",
    "                                                                                    \n",
    "        svd = svd.T                                                                 \n",
    "        key_vec = svd[key_idx-1]      \n",
    "        \n",
    "        cost=[]                                                                     \n",
    "        for idx in range(num) :                                                     \n",
    "            similarity = cos_similarity(key_vec, svd[idx])                            \n",
    "            cost.append(similarity)                 \n",
    "        gest_gest_sim[key_idx-1] = cost                                         \n",
    "    elif user_option == 4:\n",
    "        outname=\"NMF\"\n",
    "        PC_path = [\"NMF\", vector_model]\n",
    "        PC_path = \"_\".join(PC_path)\n",
    "        nmf = pd.read_pickle(PC_path + \".pkl\")\n",
    "        num = len(nmf[0])                                                           \n",
    "                                                                                    \n",
    "        nmf = nmf.T                                                                 \n",
    "        key_vec = nmf[key_idx-1]    \n",
    "        \n",
    "        cost=[]                                                                     \n",
    "        for idx in range(num) :                                                     \n",
    "            similarity = cos_similarity(key_vec, nmf[idx])                           \n",
    "            cost.append(similarity)\n",
    "        gest_gest_sim[key_idx-1] = cost                                         \n",
    "\n",
    "    elif user_option == 5: \n",
    "        outname=\"LDA\"\n",
    "        PC_path = [\"LDA\", vector_model]\n",
    "        PC_path = \"_\".join(PC_path)\n",
    "        lda = pd.read_pickle(PC_path + \".pkl\")\n",
    "        num = len(lda[0])                                                           \n",
    "                                                                                    \n",
    "        lda = lda.T                                                                 \n",
    "        key_vec = lda[key_idx-1]    \n",
    "        \n",
    "        cost=[]                                                                     \n",
    "        for idx in range(num) :                                                     \n",
    "            similarity = KL_div_similarity(key_vec, lda[idx])                            \n",
    "            cost.append(similarity)        \n",
    "        gest_gest_sim[key_idx-1] = cost                                         \n",
    "    elif user_option == 6 :\n",
    "        outname=\"ED\"\n",
    "        pathW = directory + \"/W\"\n",
    "        pathX = directory + \"/X\"\n",
    "        pathY = directory + \"/Y\"\n",
    "        pathZ = directory + \"/Z\"\n",
    "        gesturesW = symbol_loader(pathW)\n",
    "        gesturesX = symbol_loader(pathX)\n",
    "        gesturesY = symbol_loader(pathY)\n",
    "        gesturesZ = symbol_loader(pathZ)\n",
    "    \n",
    "        gestures = np.append(gesturesW, gesturesX, axis = 1)\n",
    "        gestures = np.append(gestures, gesturesY, axis = 1)\n",
    "        gestures = np.append(gestures, gesturesZ, axis = 1)\n",
    "        \n",
    "        #gestures = symbol_loader(path)\n",
    "        key_gesture = gestures[key_idx-1]\n",
    "        \n",
    "        cost=[0]*len(gestures)\n",
    "        for i, gesture in enumerate(gestures) :\n",
    "            for j, sensor in enumerate(gesture) :\n",
    "                n = len(sensor)\n",
    "                m = len(key_gesture[j])\n",
    "                cost[i] += edit_distance(key_gesture[j], sensor, m, n)\n",
    "        gest_gest_sim[key_idx-1] = cost                                         \n",
    "\n",
    "    elif user_option == 7 :\n",
    "        outname=\"DTW\"\n",
    "        \n",
    "        pathW = directory + \"/W\"\n",
    "        pathX = directory + \"/X\"\n",
    "        pathY = directory + \"/Y\"\n",
    "        pathZ = directory + \"/Z\"\n",
    "        gesturesW = amplitude_loader(pathW)\n",
    "        gesturesX = amplitude_loader(pathX)\n",
    "        gesturesY = amplitude_loader(pathY)\n",
    "        gesturesZ = amplitude_loader(pathZ)\n",
    "    \n",
    "        gestures = np.append(gesturesW, gesturesX, axis = 1)\n",
    "        gestures = np.append(gestures, gesturesY, axis = 1)\n",
    "        gestures = np.append(gestures, gesturesZ, axis = 1)\n",
    "        \n",
    "        #gestures = amplitude_loader(path)\n",
    "        key_gesture = gestures[key_idx-1]\n",
    "        \n",
    "        cost=[0]*len(gestures)\n",
    "        for i, gesture in enumerate(gestures) :                                     \n",
    "            for j, sensor in enumerate(gesture) :                                   \n",
    "                n = len(sensor)                                                     \n",
    "                m = len(key_gesture[j])                                             \n",
    "                cost[i] += dynamic_time_warping(key_gesture[j], sensor, m, n)    \n",
    "        gest_gest_sim[key_idx-1] = cost  \n",
    "        \n",
    "    gest_index = gest_index + 1\n",
    "     \n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD                                  \n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import random as sparse_random\n",
    "from sklearn.random_projection import sparse_random_matrix\n",
    "from sklearn.decomposition import NMF\n",
    "matrix = coo_matrix(gest_gest_sim,shape=(num_gestures,num_gestures)) \n",
    "print(matrix)\n",
    "np.savetxt(\"gest_sim.csv\", gest_gest_sim, delimiter=',')\n",
    "svd = TruncatedSVD(n_components=p)\n",
    "pc = svd.fit_transform(matrix)\n",
    "df = pd.DataFrame(data = pc) \n",
    "df = df.T\n",
    "print(\"------TOP P SVD after \", outname, \"-----\")\n",
    "print(\"* ordered in gesture, score\")\n",
    "print(df)\n",
    "df.to_pickle(\"./\"+\"SVD_\"+outname+\".pkl\")\n",
    "np.savetxt('component_SVD.csv', svd.components_, delimiter=',')\n",
    "\n",
    "\n",
    "nmf = NMF(n_components=p)\n",
    "pc = nmf.fit_transform(matrix)\n",
    "df = pd.DataFrame(data = pc) \n",
    "df = df.T\n",
    "print(\"------TOP P NMF after \", outname, \"-----\")\n",
    "print(\"* ordered in gesture, score\")\n",
    "print(df)\n",
    "df.to_pickle(\"./\"+\"NMF_\"+outname+\".pkl\")\n",
    "np.savetxt('component_NMF.csv', nmf.components_, delimiter=',')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
