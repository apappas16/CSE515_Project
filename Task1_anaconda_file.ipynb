{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the data directory path: Data/\n",
      "Enter the window length: 3\n",
      "Enter the shift length: 3\n",
      "Enter the resolution: 3\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "import os\n",
    "from scipy.integrate import quad\n",
    "\n",
    "\n",
    "def calcAvgSensorValue(sensorValues):\n",
    "    avg = sum(sensorValues) / len(sensorValues)\n",
    "    avg = round(avg, 9)\n",
    "    return avg\n",
    "\n",
    "\n",
    "def calcStdDev(sensorValues, meanVal):\n",
    "    sd = []\n",
    "    for num in sensorValues:\n",
    "        result = num - meanVal\n",
    "        result = result * result\n",
    "        sd.append(result)\n",
    "    return calcAvgSensorValue(sd)\n",
    "\n",
    "\n",
    "def normalize(sensorValues):\n",
    "    normalized_sensor = []\n",
    "    for val in sensorValues:\n",
    "        val = float(val)\n",
    "        normalized = 2 * ((val - min(sensorValues)) / (max(sensorValues) - min(sensorValues))) - 1\n",
    "        normalized_sensor.append(normalized)\n",
    "    return normalized_sensor\n",
    "\n",
    "\n",
    "def integral(i):\n",
    "    return getGaussianVal(i, 0, 0.25)\n",
    "\n",
    "\n",
    "def getGaussianVal(i, avg, sd):\n",
    "    i = float(i - avg) / sd\n",
    "    gauss = math.exp(-i * i / 2.0) / math.sqrt(2.0 * math.pi) / sd\n",
    "    return gauss\n",
    "\n",
    "\n",
    "def determineBands():\n",
    "    numBands = r * 2\n",
    "    bandList = []\n",
    "    bandStart = -1\n",
    "    for i in range(1, numBands):\n",
    "        integral1, e = quad(integral, (i - r - 1) / r, (i - r) / r)\n",
    "        integral2, e = quad(integral, -1, 1)\n",
    "        length_i = 2 * (integral1 / integral2)\n",
    "        band = bandStart + length_i\n",
    "        bandList.append(band)\n",
    "        bandStart = band\n",
    "    bandList.append(1.0)\n",
    "    return bandList\n",
    "\n",
    "\n",
    "def quantize(values, bandList):\n",
    "    quantized = \"\"\n",
    "    for i in range(len(values)):\n",
    "        bound = -1\n",
    "        for band in bandList:\n",
    "            if band >= values[i] > bound:\n",
    "                quantized += str(bandList.index(band) + 1)\n",
    "                break\n",
    "            else:\n",
    "                bound = band\n",
    "    return quantized\n",
    "\n",
    "\n",
    "def getWords():\n",
    "    wordList = []\n",
    "    i = 0\n",
    "    while (i + w - 1) < len(quantizedSensor):\n",
    "        word = quantizedSensor[i:i + w]\n",
    "        wordList.append(word)\n",
    "        i += s\n",
    "    return wordList\n",
    "\n",
    "\n",
    "def addToUniqueDict(word_tuple):\n",
    "    inList = False\n",
    "    for word in unique_dict:\n",
    "        if word == word_tuple:\n",
    "            inList = True\n",
    "            break\n",
    "    if not inList:\n",
    "        unique_dict.append(word_tuple)\n",
    "\n",
    "\n",
    "def calcAvgQuanAmp():\n",
    "    avgQuanAmpList = []\n",
    "    normWord = []\n",
    "    i = 0\n",
    "    while (i + w - 1) < len(normSensorVals):\n",
    "        word = normSensorVals[i:i + w]\n",
    "        normWord.append(word)\n",
    "        i += s\n",
    "\n",
    "    for word in normWord:\n",
    "        avgAmp = sum(word) / len(word)\n",
    "        avgQuanAmpList.append(avgAmp)\n",
    "\n",
    "    return avgQuanAmpList\n",
    "\n",
    "\n",
    "def getWordsFromFile(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        words = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            row = line.split(\" - [\")  # list of all words as they occur in the file\n",
    "            if len(row) > 1:\n",
    "                words.append(row[1])\n",
    "        f.close()\n",
    "    return formatWordsFromFile(words)\n",
    "\n",
    "\n",
    "def formatWordsFromFile(tempList):\n",
    "    allWords = []\n",
    "    for sensorWords in tempList:\n",
    "        sensorWords = sensorWords.replace(\"]\", \"\")\n",
    "        sensorWords = sensorWords.replace(\"'\", \"\")\n",
    "        wrd = sensorWords.split(\", \")\n",
    "        allWords.append(wrd)\n",
    "    return allWords\n",
    "\n",
    "\n",
    "def getUniqueWordsInGesture(allWordsInGesture):\n",
    "    uniqueWords = []\n",
    "    for i in range(len(allWordsInGesture)):  # i is each sensor in the gesture\n",
    "        for word in allWordsInGesture[i]:\n",
    "            uniqueWord = (direct, i+1, word)\n",
    "            if uniqueWord not in uniqueWords:\n",
    "                uniqueWords.append(uniqueWord)\n",
    "    return uniqueWords\n",
    "\n",
    "\n",
    "def calcTfValue(wordTuple):\n",
    "    totalWords = 0\n",
    "    sensorIndex = wordTuple[1]-1\n",
    "    num_occurs = 0\n",
    "    component = wordTuple[0]\n",
    "\n",
    "    for file in os.listdir(directory + component):\n",
    "        if file.endswith(\".wrd\"):\n",
    "            file = directory + component + \"/\" + file\n",
    "            gestFileWords = getWordsFromFile(file)\n",
    "            num_occurs += gestFileWords[sensorIndex].count(wordTuple[2])\n",
    "            totalWords += len(gestFileWords[sensorIndex])\n",
    "    value = num_occurs / totalWords\n",
    "    return value\n",
    "\n",
    "\n",
    "def calcIdfValue(wordTuple):\n",
    "    numObjs = 60\n",
    "    numObjsWithWord = 0\n",
    "    sensorIndex = wordTuple[1]-1\n",
    "    component = wordTuple[0]\n",
    "\n",
    "    for file in os.listdir(directory + component):\n",
    "        if file.endswith(\".wrd\"):\n",
    "            file = directory + component + \"/\" + file\n",
    "            gestFileWords = getWordsFromFile(file)\n",
    "            if word_tuple[2] in gestFileWords[sensorIndex]:\n",
    "                numObjsWithWord += 1\n",
    "\n",
    "    value = math.log(numObjs / numObjsWithWord)\n",
    "    return value\n",
    "\n",
    "\n",
    "def writeValsToFile():\n",
    "    allWordsInGesture = getWordsFromFile(fullFileName)\n",
    "    uniqueWordsInGesture = getUniqueWordsInGesture(allWordsInGesture)\n",
    "    sensorId = 1\n",
    "    #unique_dict is what I want\n",
    "    for sensor in uniqueWordsInGesture:\n",
    "        word_tuple = (sensor[0], sensor[1], sensor[2])\n",
    "        for n, i in enumerate(unique_dict):\n",
    "            if i == word_tuple:\n",
    "                tfVal = unique_tf_dict[n]\n",
    "                tfidfVal = unique_tfidf_dict[n]\n",
    "                tfFile.write(str(i) + \"-\")\n",
    "                tfidfFile.write(str(i) + \"-\")\n",
    "                tfFile.write(str(tfVal))\n",
    "                tfidfFile.write(str(tfidfVal))\n",
    "                tfFile.write(\"\\n\")\n",
    "                tfidfFile.write(\"\\n\")\n",
    "        \n",
    "        #sensorId += 1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # GLOBAL VARIABLES:\n",
    "    unique_dict = []  # stores list of all unique word tuples found in entire DB in order\n",
    "    unique_tf_dict = []\n",
    "    unique_tfidf_dict = []\n",
    "    all_gesture_dict = []  # stores list of all words found in DB\n",
    "\n",
    "    # TASK 0\n",
    "    # TASK 0A\n",
    "    directory = input(\"Enter the data directory path: \")\n",
    "    w = input(\"Enter the window length: \")\n",
    "    s = input(\"Enter the shift length: \")\n",
    "    r = input(\"Enter the resolution: \")\n",
    "\n",
    "    w = int(w)\n",
    "    s = int(s)\n",
    "    r = int(r)\n",
    "\n",
    "    # for each data file create a .wrd file containing the following:\n",
    "    for direct in os.listdir(directory):\n",
    "        # for each csv file in X,Y,W,Z:\n",
    "        for filename in os.listdir(directory + direct):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                bands = determineBands()\n",
    "\n",
    "                # generate .wrd file\n",
    "                with open(str(directory) + str(direct) + \"/\" + str(filename) + \".wrd\", \"w\") as wrdFile:\n",
    "\n",
    "                    sensor_id = 1\n",
    "                    csvFile = open(str(directory) + str(direct) + \"/\" + filename, \"r\")\n",
    "                    reader = csv.reader(csvFile, delimiter=',')\n",
    "                    # for each sensor sj in file\n",
    "                    for sensor in reader:\n",
    "                        # output component ID, c in output file\n",
    "                        wrdFile.write(str(direct) + \", \")\n",
    "\n",
    "                        # write sensorID to wrd file\n",
    "                        wrdFile.write(str(sensor_id) + \", \")\n",
    "\n",
    "                        # compute and output average amplitude, avgij of the values\n",
    "                        sensorVals = list(sensor)\n",
    "                        sensorVals = [float(i) for i in sensorVals]\n",
    "                        sensorAvg = calcAvgSensorValue(sensorVals)\n",
    "                        wrdFile.write(str(sensorAvg) + \", \")\n",
    "\n",
    "                        # compute and output standard deviations stdij of the values\n",
    "                        stdDev = calcStdDev(sensorVals, sensorAvg)\n",
    "                        wrdFile.write(str(stdDev) + \", \")\n",
    "\n",
    "                        # normalize entries between -1 and 1\n",
    "                        normSensorVals = normalize(sensorVals)\n",
    "\n",
    "                        # quantizes entries into 2r levels as in phase 1\n",
    "                        quantizedSensor = quantize(normSensorVals, bands)\n",
    "\n",
    "                        # moves a w-length window on time series (by shifting it s units at a time), and at position h\n",
    "                        sensorWords = getWords()\n",
    "\n",
    "                        # computes and outputs in file average quantized amplitude avgQijh for window h of sensor sj\n",
    "                        avgQuanAmp = calcAvgQuanAmp()\n",
    "                        wrdFile.write(str(avgQuanAmp) + \", \" + \" - \")\n",
    "\n",
    "                        # outputs symbolic quantized window descriptor winQijh for the window h of sensor sj\n",
    "                        wrdFile.write(str(sensorWords) + \"\\n\")\n",
    "\n",
    "                        # add dictionary of each window to gestureDict list\n",
    "                        for window in sensorWords:\n",
    "                            wordDict = (direct, sensor_id, window)\n",
    "                            all_gesture_dict.append(wordDict)\n",
    "                            if wordDict not in unique_dict:\n",
    "                                unique_dict.append(wordDict)\n",
    "\n",
    "                        sensor_id += 1\n",
    "        # The dictionary of the words consists of <componentName, sensorID, winQ>\n",
    "\n",
    "    # TASK 0B\n",
    "    for word_tuple in unique_dict:\n",
    "        tfValue = calcTfValue(word_tuple)\n",
    "        idfValue = calcIdfValue(word_tuple)\n",
    "        tf_idf_value = tfValue * idfValue\n",
    "        unique_tf_dict.append(tfValue)\n",
    "        unique_tfidf_dict.append(tf_idf_value)\n",
    "\n",
    "    for direct in os.listdir(directory):\n",
    "        for wrdFile in os.listdir(directory + direct):\n",
    "            if wrdFile.endswith(\".wrd\"):\n",
    "                fullFileName = directory + direct + \"/\" + wrdFile\n",
    "                tfFile = open(directory + direct + \"/tf_vectors_\" + wrdFile[:-8] + \".txt\", \"w\")\n",
    "                tfidfFile = open(directory + direct + \"/tfidf_vectors_\" + wrdFile[:-8] + \".txt\", \"w\")\n",
    "                writeValsToFile()\n",
    "                tfFile.close()\n",
    "                tfidfFile.close()\n",
    "    # End of TASK0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the following inputs as the same values you used for task 0: \n",
      "Enter the data directory path (ex: Data/: Data/\n",
      "Enter the window length (ex: 3): 3\n",
      "Enter the shift length (ex: 3): 3\n",
      "Enter the resolution (ex: 3): 3\n",
      "Enter the vector model (ex: tf): tfidf\n",
      "Enter k (ex: 4): 3\n",
      "Enter the analysis you would like to use (ex: PCA): SVD\n",
      "Data/W\\tfidf_vectors_1.txt\n",
      "Data/W\\tfidf_vectors_2.txt\n",
      "Data/W\\tfidf_vectors_3.txt\n",
      "Data/W\\tfidf_vectors_4.txt\n",
      "Data/W\\tfidf_vectors_5.txt\n",
      "Data/W\\tfidf_vectors_6.txt\n",
      "Data/W\\tfidf_vectors_7.txt\n",
      "Data/W\\tfidf_vectors_8.txt\n",
      "Data/W\\tfidf_vectors_9.txt\n",
      "Data/W\\tfidf_vectors_10.txt\n",
      "Data/W\\tfidf_vectors_11.txt\n",
      "Data/W\\tfidf_vectors_12.txt\n",
      "Data/W\\tfidf_vectors_13.txt\n",
      "Data/W\\tfidf_vectors_14.txt\n",
      "Data/W\\tfidf_vectors_15.txt\n",
      "Data/W\\tfidf_vectors_16.txt\n",
      "Data/W\\tfidf_vectors_17.txt\n",
      "Data/W\\tfidf_vectors_18.txt\n",
      "Data/W\\tfidf_vectors_19.txt\n",
      "Data/W\\tfidf_vectors_20.txt\n",
      "Data/W\\tfidf_vectors_21.txt\n",
      "Data/W\\tfidf_vectors_22.txt\n",
      "Data/W\\tfidf_vectors_23.txt\n",
      "Data/W\\tfidf_vectors_24.txt\n",
      "Data/W\\tfidf_vectors_25.txt\n",
      "Data/W\\tfidf_vectors_26.txt\n",
      "Data/W\\tfidf_vectors_27.txt\n",
      "Data/W\\tfidf_vectors_28.txt\n",
      "Data/W\\tfidf_vectors_29.txt\n",
      "Data/W\\tfidf_vectors_30.txt\n",
      "Data/W\\tfidf_vectors_31.txt\n",
      "Data/W\\tfidf_vectors_32.txt\n",
      "Data/W\\tfidf_vectors_33.txt\n",
      "Data/W\\tfidf_vectors_34.txt\n",
      "Data/W\\tfidf_vectors_35.txt\n",
      "Data/W\\tfidf_vectors_36.txt\n",
      "Data/W\\tfidf_vectors_37.txt\n",
      "Data/W\\tfidf_vectors_38.txt\n",
      "Data/W\\tfidf_vectors_39.txt\n",
      "Data/W\\tfidf_vectors_40.txt\n",
      "Data/W\\tfidf_vectors_41.txt\n",
      "Data/W\\tfidf_vectors_42.txt\n",
      "Data/W\\tfidf_vectors_43.txt\n",
      "Data/W\\tfidf_vectors_44.txt\n",
      "Data/W\\tfidf_vectors_45.txt\n",
      "Data/W\\tfidf_vectors_46.txt\n",
      "Data/W\\tfidf_vectors_47.txt\n",
      "Data/W\\tfidf_vectors_48.txt\n",
      "Data/W\\tfidf_vectors_49.txt\n",
      "Data/W\\tfidf_vectors_50.txt\n",
      "Data/W\\tfidf_vectors_51.txt\n",
      "Data/W\\tfidf_vectors_52.txt\n",
      "Data/W\\tfidf_vectors_53.txt\n",
      "Data/W\\tfidf_vectors_54.txt\n",
      "Data/W\\tfidf_vectors_55.txt\n",
      "Data/W\\tfidf_vectors_56.txt\n",
      "Data/W\\tfidf_vectors_57.txt\n",
      "Data/W\\tfidf_vectors_58.txt\n",
      "Data/W\\tfidf_vectors_59.txt\n",
      "Data/W\\tfidf_vectors_60.txt\n",
      "Data/X\\tfidf_vectors_1.txt\n",
      "Data/X\\tfidf_vectors_2.txt\n",
      "Data/X\\tfidf_vectors_3.txt\n",
      "Data/X\\tfidf_vectors_4.txt\n",
      "Data/X\\tfidf_vectors_5.txt\n",
      "Data/X\\tfidf_vectors_6.txt\n",
      "Data/X\\tfidf_vectors_7.txt\n",
      "Data/X\\tfidf_vectors_8.txt\n",
      "Data/X\\tfidf_vectors_9.txt\n",
      "Data/X\\tfidf_vectors_10.txt\n",
      "Data/X\\tfidf_vectors_11.txt\n",
      "Data/X\\tfidf_vectors_12.txt\n",
      "Data/X\\tfidf_vectors_13.txt\n",
      "Data/X\\tfidf_vectors_14.txt\n",
      "Data/X\\tfidf_vectors_15.txt\n",
      "Data/X\\tfidf_vectors_16.txt\n",
      "Data/X\\tfidf_vectors_17.txt\n",
      "Data/X\\tfidf_vectors_18.txt\n",
      "Data/X\\tfidf_vectors_19.txt\n",
      "Data/X\\tfidf_vectors_20.txt\n",
      "Data/X\\tfidf_vectors_21.txt\n",
      "Data/X\\tfidf_vectors_22.txt\n",
      "Data/X\\tfidf_vectors_23.txt\n",
      "Data/X\\tfidf_vectors_24.txt\n",
      "Data/X\\tfidf_vectors_25.txt\n",
      "Data/X\\tfidf_vectors_26.txt\n",
      "Data/X\\tfidf_vectors_27.txt\n",
      "Data/X\\tfidf_vectors_28.txt\n",
      "Data/X\\tfidf_vectors_29.txt\n",
      "Data/X\\tfidf_vectors_30.txt\n",
      "Data/X\\tfidf_vectors_31.txt\n",
      "Data/X\\tfidf_vectors_32.txt\n",
      "Data/X\\tfidf_vectors_33.txt\n",
      "Data/X\\tfidf_vectors_34.txt\n",
      "Data/X\\tfidf_vectors_35.txt\n",
      "Data/X\\tfidf_vectors_36.txt\n",
      "Data/X\\tfidf_vectors_37.txt\n",
      "Data/X\\tfidf_vectors_38.txt\n",
      "Data/X\\tfidf_vectors_39.txt\n",
      "Data/X\\tfidf_vectors_40.txt\n",
      "Data/X\\tfidf_vectors_41.txt\n",
      "Data/X\\tfidf_vectors_42.txt\n",
      "Data/X\\tfidf_vectors_43.txt\n",
      "Data/X\\tfidf_vectors_44.txt\n",
      "Data/X\\tfidf_vectors_45.txt\n",
      "Data/X\\tfidf_vectors_46.txt\n",
      "Data/X\\tfidf_vectors_47.txt\n",
      "Data/X\\tfidf_vectors_48.txt\n",
      "Data/X\\tfidf_vectors_49.txt\n",
      "Data/X\\tfidf_vectors_50.txt\n",
      "Data/X\\tfidf_vectors_51.txt\n",
      "Data/X\\tfidf_vectors_52.txt\n",
      "Data/X\\tfidf_vectors_53.txt\n",
      "Data/X\\tfidf_vectors_54.txt\n",
      "Data/X\\tfidf_vectors_55.txt\n",
      "Data/X\\tfidf_vectors_56.txt\n",
      "Data/X\\tfidf_vectors_57.txt\n",
      "Data/X\\tfidf_vectors_58.txt\n",
      "Data/X\\tfidf_vectors_59.txt\n",
      "Data/X\\tfidf_vectors_60.txt\n",
      "Data/Y\\tfidf_vectors_1.txt\n",
      "Data/Y\\tfidf_vectors_2.txt\n",
      "Data/Y\\tfidf_vectors_3.txt\n",
      "Data/Y\\tfidf_vectors_4.txt\n",
      "Data/Y\\tfidf_vectors_5.txt\n",
      "Data/Y\\tfidf_vectors_6.txt\n",
      "Data/Y\\tfidf_vectors_7.txt\n",
      "Data/Y\\tfidf_vectors_8.txt\n",
      "Data/Y\\tfidf_vectors_9.txt\n",
      "Data/Y\\tfidf_vectors_10.txt\n",
      "Data/Y\\tfidf_vectors_11.txt\n",
      "Data/Y\\tfidf_vectors_12.txt\n",
      "Data/Y\\tfidf_vectors_13.txt\n",
      "Data/Y\\tfidf_vectors_14.txt\n",
      "Data/Y\\tfidf_vectors_15.txt\n",
      "Data/Y\\tfidf_vectors_16.txt\n",
      "Data/Y\\tfidf_vectors_17.txt\n",
      "Data/Y\\tfidf_vectors_18.txt\n",
      "Data/Y\\tfidf_vectors_19.txt\n",
      "Data/Y\\tfidf_vectors_20.txt\n",
      "Data/Y\\tfidf_vectors_21.txt\n",
      "Data/Y\\tfidf_vectors_22.txt\n",
      "Data/Y\\tfidf_vectors_23.txt\n",
      "Data/Y\\tfidf_vectors_24.txt\n",
      "Data/Y\\tfidf_vectors_25.txt\n",
      "Data/Y\\tfidf_vectors_26.txt\n",
      "Data/Y\\tfidf_vectors_27.txt\n",
      "Data/Y\\tfidf_vectors_28.txt\n",
      "Data/Y\\tfidf_vectors_29.txt\n",
      "Data/Y\\tfidf_vectors_30.txt\n",
      "Data/Y\\tfidf_vectors_31.txt\n",
      "Data/Y\\tfidf_vectors_32.txt\n",
      "Data/Y\\tfidf_vectors_33.txt\n",
      "Data/Y\\tfidf_vectors_34.txt\n",
      "Data/Y\\tfidf_vectors_35.txt\n",
      "Data/Y\\tfidf_vectors_36.txt\n",
      "Data/Y\\tfidf_vectors_37.txt\n",
      "Data/Y\\tfidf_vectors_38.txt\n",
      "Data/Y\\tfidf_vectors_39.txt\n",
      "Data/Y\\tfidf_vectors_40.txt\n",
      "Data/Y\\tfidf_vectors_41.txt\n",
      "Data/Y\\tfidf_vectors_42.txt\n",
      "Data/Y\\tfidf_vectors_43.txt\n",
      "Data/Y\\tfidf_vectors_44.txt\n",
      "Data/Y\\tfidf_vectors_45.txt\n",
      "Data/Y\\tfidf_vectors_46.txt\n",
      "Data/Y\\tfidf_vectors_47.txt\n",
      "Data/Y\\tfidf_vectors_48.txt\n",
      "Data/Y\\tfidf_vectors_49.txt\n",
      "Data/Y\\tfidf_vectors_50.txt\n",
      "Data/Y\\tfidf_vectors_51.txt\n",
      "Data/Y\\tfidf_vectors_52.txt\n",
      "Data/Y\\tfidf_vectors_53.txt\n",
      "Data/Y\\tfidf_vectors_54.txt\n",
      "Data/Y\\tfidf_vectors_55.txt\n",
      "Data/Y\\tfidf_vectors_56.txt\n",
      "Data/Y\\tfidf_vectors_57.txt\n",
      "Data/Y\\tfidf_vectors_58.txt\n",
      "Data/Y\\tfidf_vectors_59.txt\n",
      "Data/Y\\tfidf_vectors_60.txt\n",
      "Data/Z\\tfidf_vectors_1.txt\n",
      "Data/Z\\tfidf_vectors_2.txt\n",
      "Data/Z\\tfidf_vectors_3.txt\n",
      "Data/Z\\tfidf_vectors_4.txt\n",
      "Data/Z\\tfidf_vectors_5.txt\n",
      "Data/Z\\tfidf_vectors_6.txt\n",
      "Data/Z\\tfidf_vectors_7.txt\n",
      "Data/Z\\tfidf_vectors_8.txt\n",
      "Data/Z\\tfidf_vectors_9.txt\n",
      "Data/Z\\tfidf_vectors_10.txt\n",
      "Data/Z\\tfidf_vectors_11.txt\n",
      "Data/Z\\tfidf_vectors_12.txt\n",
      "Data/Z\\tfidf_vectors_13.txt\n",
      "Data/Z\\tfidf_vectors_14.txt\n",
      "Data/Z\\tfidf_vectors_15.txt\n",
      "Data/Z\\tfidf_vectors_16.txt\n",
      "Data/Z\\tfidf_vectors_17.txt\n",
      "Data/Z\\tfidf_vectors_18.txt\n",
      "Data/Z\\tfidf_vectors_19.txt\n",
      "Data/Z\\tfidf_vectors_20.txt\n",
      "Data/Z\\tfidf_vectors_21.txt\n",
      "Data/Z\\tfidf_vectors_22.txt\n",
      "Data/Z\\tfidf_vectors_23.txt\n",
      "Data/Z\\tfidf_vectors_24.txt\n",
      "Data/Z\\tfidf_vectors_25.txt\n",
      "Data/Z\\tfidf_vectors_26.txt\n",
      "Data/Z\\tfidf_vectors_27.txt\n",
      "Data/Z\\tfidf_vectors_28.txt\n",
      "Data/Z\\tfidf_vectors_29.txt\n",
      "Data/Z\\tfidf_vectors_30.txt\n",
      "Data/Z\\tfidf_vectors_31.txt\n",
      "Data/Z\\tfidf_vectors_32.txt\n",
      "Data/Z\\tfidf_vectors_33.txt\n",
      "Data/Z\\tfidf_vectors_34.txt\n",
      "Data/Z\\tfidf_vectors_35.txt\n",
      "Data/Z\\tfidf_vectors_36.txt\n",
      "Data/Z\\tfidf_vectors_37.txt\n",
      "Data/Z\\tfidf_vectors_38.txt\n",
      "Data/Z\\tfidf_vectors_39.txt\n",
      "Data/Z\\tfidf_vectors_40.txt\n",
      "Data/Z\\tfidf_vectors_41.txt\n",
      "Data/Z\\tfidf_vectors_42.txt\n",
      "Data/Z\\tfidf_vectors_43.txt\n",
      "Data/Z\\tfidf_vectors_44.txt\n",
      "Data/Z\\tfidf_vectors_45.txt\n",
      "Data/Z\\tfidf_vectors_46.txt\n",
      "Data/Z\\tfidf_vectors_47.txt\n",
      "Data/Z\\tfidf_vectors_48.txt\n",
      "Data/Z\\tfidf_vectors_49.txt\n",
      "Data/Z\\tfidf_vectors_50.txt\n",
      "Data/Z\\tfidf_vectors_51.txt\n",
      "Data/Z\\tfidf_vectors_52.txt\n",
      "Data/Z\\tfidf_vectors_53.txt\n",
      "Data/Z\\tfidf_vectors_54.txt\n",
      "Data/Z\\tfidf_vectors_55.txt\n",
      "Data/Z\\tfidf_vectors_56.txt\n",
      "Data/Z\\tfidf_vectors_57.txt\n",
      "Data/Z\\tfidf_vectors_58.txt\n",
      "Data/Z\\tfidf_vectors_59.txt\n",
      "Data/Z\\tfidf_vectors_60.txt\n",
      "      0         1             2             3             4             5      \\\n",
      "0  0.001781  0.000835  6.909383e-23  1.674926e-22  3.593451e-24 -2.339705e-25   \n",
      "1  0.009230  0.005106  1.545802e-17  3.719671e-18  1.060457e-18  1.111772e-18   \n",
      "2  0.003384  0.004574  1.203192e-16  5.518081e-16  1.089915e-17 -3.421791e-18   \n",
      "\n",
      "          6             7             8             9        ...     44470  \\\n",
      "0  4.856041e-25 -1.267787e-25 -3.690551e-25 -4.394470e-26    ...       0.0   \n",
      "1 -7.999071e-19 -8.779437e-19 -2.143423e-20 -2.147692e-19    ...       0.0   \n",
      "2  2.761071e-18  3.088668e-18 -2.730377e-19  4.690623e-19    ...      -0.0   \n",
      "\n",
      "   44471  44472  44473  44474  44475  44476  44477     44478     44479  \n",
      "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  0.002138  0.008626  \n",
      "1    0.0    0.0    0.0    0.0    0.0    0.0    0.0  0.000461  0.016723  \n",
      "2   -0.0   -0.0   -0.0   -0.0   -0.0   -0.0   -0.0  0.001549 -0.008825  \n",
      "\n",
      "[3 rows x 44480 columns]\n",
      "\n",
      "SVD:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexs\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:294: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\alexs\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-21868f74c8a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[0museOp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Enter the analysis you would like to use (ex: PCA): \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m \u001b[0mtask1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0museOp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-21868f74c8a0>\u001b[0m in \u001b[0;36mtask1\u001b[1;34m(vectModel, useOp, k)\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[1;31m#print(topk)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[0mdictofComponents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreatedictofComponents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictofComponents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-21868f74c8a0>\u001b[0m in \u001b[0;36mcreatedictofComponents\u001b[1;34m(topk, k)\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Z'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msens\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[0moutputMat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtopk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    936\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m         \u001b[1;31m# do the setitem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 938\u001b[1;33m         \u001b[0mcacher_needs_updating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_is_chained_assignment_possible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    939\u001b[0m         \u001b[0msetitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    940\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcacher_needs_updating\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_check_is_chained_assignment_possible\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2625\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mref\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_mixed_type\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2626\u001b[0m                 self._check_setitem_copy(stacklevel=4, t='referant',\n\u001b[1;32m-> 2627\u001b[1;33m                                          force=True)\n\u001b[0m\u001b[0;32m   2628\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2629\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_copy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_check_setitem_copy\u001b[1;34m(self, stacklevel, t, force)\u001b[0m\n\u001b[0;32m   2672\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2673\u001b[0m                 \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2674\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_referents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2675\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_copy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2676\u001b[0m                     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Task 1\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "import os\n",
    "import re\n",
    "\n",
    "numbers = re.compile(r'(\\d+)')\n",
    "\n",
    "#unnecessary lines if you are using anaconda or another \n",
    "print(\"Please enter the following inputs as the same values you used for task 0: \")\n",
    "directory = input(\"Enter the data directory path (ex: Data/: \")\n",
    "w = input(\"Enter the window length (ex: 3): \")\n",
    "s = input(\"Enter the shift length (ex: 3): \")\n",
    "r = input(\"Enter the resolution (ex: 3): \")\n",
    "\n",
    "w = int(w)\n",
    "s = int(s)\n",
    "r = int(r)\n",
    "#end of non-anaconda lines\n",
    "\n",
    "def numericalSort(value):\n",
    "    parts = numbers.split(value)\n",
    "    parts[1::2] = map(int, parts[1::2])\n",
    "    return parts\n",
    "\n",
    "#returns the top-k topics \n",
    "def PCAsetup(wordMat, k):\n",
    "    #calculate PCA\n",
    "    pca = PCA(k)\n",
    "    pc = pca.fit_transform(wordMat)\n",
    "    UT = pca.components_\n",
    "    topK = pd.DataFrame(data = UT)\n",
    "    \n",
    "    original_df = pd.DataFrame(data = pc)\n",
    "    original_df.to_pickle(\"./PCA_\" + vectModel + \".pkl\")\n",
    "    \n",
    "    print(topK)\n",
    "    return topK\n",
    "\n",
    "def SVDsetup(wordMat, k):\n",
    "    #calculate SVD\n",
    "    svd = TruncatedSVD(k)\n",
    "    sv = svd.fit_transform(wordMat)\n",
    "    VT = svd.components_\n",
    "    topK = pd.DataFrame(data = VT)\n",
    "    \n",
    "    original_df = pd.DataFrame(data = sv)\n",
    "    original_df.to_pickle(\"./SVD_\" + vectModel + \".pkl\")\n",
    "    \n",
    "    print(topK)\n",
    "    return topK\n",
    "\n",
    "def NMFsetup(wordMat, k):\n",
    "    #calculate NMF\n",
    "    nmf = NMF(k)\n",
    "    nm = nmf.fit_transform(wordMat)\n",
    "    R = nmf.components_\n",
    "    topK = pd.DataFrame(data = R)\n",
    "    \n",
    "    original_df = pd.DataFrame(data = nm)\n",
    "    original_df.to_pickle(\"./NMF_\" + vectModel + \".pkl\")\n",
    "    \n",
    "    print(topK)\n",
    "    return topK\n",
    "\n",
    "def LDAsetup(wordMat, k):\n",
    "    #calculate LDA\n",
    "    lda = LDA(k)\n",
    "    ld = lda.fit_transform(wordMat)\n",
    "    V = lda.components_\n",
    "    topK = pd.DataFrame(data = V)\n",
    "    \n",
    "    original_df = pd.DataFrame(data = ld)\n",
    "    original_df.to_pickle(\"./LDA_\" + vectModel + \".pkl\")\n",
    "    \n",
    "    print(topK)\n",
    "    return topK\n",
    "\n",
    "\n",
    "def makeMat(vectModel):    \n",
    "    #read files\n",
    "    Wmat = []\n",
    "    Xmat = []\n",
    "    Ymat = []\n",
    "    Zmat = []\n",
    "    if vectModel == \"tf\":\n",
    "        for axisNum in range(1, 5):\n",
    "            if axisNum == 1:\n",
    "                axis = 'W'\n",
    "            elif axisNum == 2:\n",
    "                axis = 'X'\n",
    "            elif axisNum == 3:\n",
    "                axis = 'Y'\n",
    "            elif axisNum == 4:\n",
    "                axis = 'Z'\n",
    "                \n",
    "            for file in sorted(glob.glob(directory + axis + \"/tf_vectors_*.txt\"), key=numericalSort):\n",
    "                #if not file.endswith(\".txt\") or not file.startswith(\"tf_vectors_\"):\n",
    "                    #continue\n",
    "                #Xmat = []\n",
    "                #read tf file\n",
    "                f = open(file, \"r\")\n",
    "                tf_vectors = f.readlines()\n",
    "        \n",
    "                gestWords = []\n",
    "                tfVals = []\n",
    "        \n",
    "                #split the line into the word and tf value\n",
    "                for line in tf_vectors:\n",
    "                    noDash = line.split(\"-\")\n",
    "                    tf_val = noDash[1]\n",
    "                    tf_val = tf_val.replace(\"\\n\", \"\")\n",
    "                    gestWords.append(noDash[0])\n",
    "                    tfVals.append(tf_val)\n",
    "           \n",
    "        \n",
    "                index = 0\n",
    "                startI = \"1\"\n",
    "                for y in range(1, w):\n",
    "                    startI = startI + \"1\"\n",
    "                startI = int(startI)\n",
    "        \n",
    "                #create dictionary with every word for every sensor and every directory\n",
    "                numWords = (startI * (2*r) - startI) * 20\n",
    "                wordMat = []\n",
    "    \n",
    "                for i in range(0, numWords + 20):\n",
    "                    wordMat.append(0)\n",
    "                \n",
    "                # put tf values into matrix where column = word\n",
    "                for x in gestWords:\n",
    "                    word = x.split(\", \")\n",
    "                    sensorNum = word[1].replace(\"'\", \"\")\n",
    "                    wordID = word[2].replace(\"'\", \"\")\n",
    "                    wordID = wordID.replace(\")\", \"\")\n",
    "                \n",
    "                    wordID = int(wordID)\n",
    "                    sensorNum = int(sensorNum)\n",
    "                \n",
    "                    #axisSplit = len(wordMat) / 4\n",
    "                    sensorSplit = len(wordMat) / 20\n",
    "                    \n",
    "                    wordIndex = int(wordID + ((sensorNum - 1) * sensorSplit)) #+ ((axisNum - 1) * axisSplit)))\n",
    "                    wordIndex = wordIndex - startI\n",
    "            \n",
    "                    wordMat[wordIndex] = float(tfVals[index])\n",
    "                    index = index + 1\n",
    "                \n",
    "\n",
    "                if axisNum == 1:\n",
    "                    axis = 'W'\n",
    "                    Wmat.append(wordMat)\n",
    "                elif axisNum == 2:\n",
    "                    axis = 'X'\n",
    "                    Xmat.append(wordMat)\n",
    "                elif axisNum == 3:\n",
    "                    axis = 'Y'\n",
    "                    Ymat.append(wordMat)\n",
    "                elif axisNum == 4:\n",
    "                    axis = 'Z'\n",
    "                    Zmat.append(wordMat)\n",
    "                f.close()\n",
    "                \n",
    "        finalMat = np.append(Wmat, Xmat, axis = 1)\n",
    "        finalMat = np.append(finalMat, Ymat, axis = 1)\n",
    "        finalMat = np.append(finalMat, Zmat, axis = 1)\n",
    "        \n",
    "        #print(finalMat)\n",
    "        \n",
    "        return finalMat\n",
    "            #print(Xmat)\n",
    "    \n",
    "    elif vectModel == \"tfidf\":\n",
    "        for axisNum in range(1, 5):\n",
    "            if axisNum == 1:\n",
    "                axis = 'W'\n",
    "            elif axisNum == 2:\n",
    "                axis = 'X'\n",
    "            elif axisNum == 3:\n",
    "                axis = 'Y'\n",
    "            elif axisNum == 4:\n",
    "                axis = 'Z'\n",
    "                \n",
    "            for file in sorted(glob.glob(directory + axis + \"/tfidf_vectors_*.txt\"), key=numericalSort):\n",
    "                #if not file.endswith(\".txt\") or not file.startswith(\"tfidf_vectors_\"):\n",
    "                    #continue\n",
    "                #read tf file\n",
    "                f = open(file, \"r\")\n",
    "                tf_vectors = f.readlines()\n",
    "        \n",
    "                gestWords = []\n",
    "                tfVals = []\n",
    "        \n",
    "                #split the line into the word and tf value\n",
    "                for line in tf_vectors:\n",
    "                    noDash = line.split(\"-\")\n",
    "                    tf_val = noDash[1]\n",
    "                    tf_val = tf_val.replace(\"\\n\", \"\")\n",
    "                    gestWords.append(noDash[0])\n",
    "                    tfVals.append(tf_val)\n",
    "           \n",
    "        \n",
    "                index = 0\n",
    "                startI = \"1\"\n",
    "                for y in range(1, w):\n",
    "                    startI = startI + \"1\"\n",
    "                startI = int(startI)\n",
    "        \n",
    "                #create dictionary with every word for every sensor and every directory\n",
    "                numWords = (startI * (2*r) - startI) * 20\n",
    "                wordMat = []\n",
    "    \n",
    "                for i in range(0, numWords + 20):\n",
    "                    wordMat.append(0)\n",
    "                \n",
    "                # put tf values into matrix where column = word\n",
    "                for x in gestWords:\n",
    "                    word = x.split(\", \")\n",
    "                    sensorNum = word[1].replace(\"'\", \"\")\n",
    "                    wordID = word[2].replace(\"'\", \"\")\n",
    "                    wordID = wordID.replace(\")\", \"\")\n",
    "                \n",
    "                    wordID = int(wordID)\n",
    "                    sensorNum = int(sensorNum)\n",
    "                \n",
    "                    #axisSplit = len(wordMat) / 4\n",
    "                    sensorSplit = len(wordMat) / 20\n",
    "                    \n",
    "                    wordIndex = int(wordID + ((sensorNum - 1) * sensorSplit)) #+ ((axisNum - 1) * axisSplit)))\n",
    "                    wordIndex = wordIndex - startI\n",
    "            \n",
    "                    wordMat[wordIndex] = float(tfVals[index])\n",
    "                    index = index + 1\n",
    "                \n",
    "\n",
    "                if axisNum == 1:\n",
    "                    axis = 'W'\n",
    "                    Wmat.append(wordMat)\n",
    "                elif axisNum == 2:\n",
    "                    axis = 'X'\n",
    "                    Xmat.append(wordMat)\n",
    "                elif axisNum == 3:\n",
    "                    axis = 'Y'\n",
    "                    Ymat.append(wordMat)\n",
    "                elif axisNum == 4:\n",
    "                    axis = 'Z'\n",
    "                    Zmat.append(wordMat)\n",
    "                f.close()\n",
    "           \n",
    "        finalMat = np.append(Wmat, Xmat, axis = 1)\n",
    "        finalMat = np.append(finalMat, Ymat, axis = 1)\n",
    "        finalMat = np.append(finalMat, Zmat, axis = 1)\n",
    "        \n",
    "        #print(finalMat)\n",
    "        \n",
    "        return finalMat\n",
    "            #print(Xmat)\n",
    "    \n",
    "def createdictofComponents(topk, k):\n",
    "    #creates a matrix that contains tuples of the word and score\n",
    "    outputMat = []\n",
    "    outputMat = topk\n",
    "    startI = \"1\"\n",
    "    for y in range(1, w):\n",
    "        startI = startI + \"1\"\n",
    "    startI = int(startI)\n",
    "    numWords = (startI * (2*r) - startI)\n",
    "    \n",
    "    for j in range(0, len(topk.columns)):\n",
    "        axisSplit = len(topk) / 4\n",
    "        sensorSplit = axisSplit / 20\n",
    "        \n",
    "        ax = int(j/axisSplit)\n",
    "        sens = int((j - (ax * axisSplit))/sensorSplit)\n",
    "        word = int((j - ((sens * sensorSplit) + (ax * axisSplit))) - startI)\n",
    "        \n",
    "        for row in range(0, k):\n",
    "            if ax == 0:\n",
    "                axis = 'W'\n",
    "            elif ax == 1:\n",
    "                axis = 'X'\n",
    "            elif ax == 2:\n",
    "                axis = 'Y'\n",
    "            elif ax == 3:\n",
    "                axis = 'Z'\n",
    "            label = axis + str(sens) + str(word)\n",
    "            outputMat[j][row] = (topk[j][row], label)\n",
    "            \n",
    "        \n",
    "                \n",
    "            \"\"\"for sensor in range(0, 20):\n",
    "            \n",
    "                for word in range(0, numWords + 1):\n",
    "                    label = axis + str(sensor - 1) + str(word + startI)\n",
    "                    \n",
    "                    #axisSplit = len(topk) / 4\n",
    "                    sensorSplit = axisSplit / 20\n",
    "                    \n",
    "                    wordIndex = int((word + startI) + (sensor * sensorSplit) + (ax * axisSplit))\n",
    "                    wordIndex = wordIndex - startI\n",
    "                    outputMat[wordIndex][row] = (topk[wordIndex][row], label)\"\"\"\n",
    "                \n",
    "                \n",
    "    #for i in range(0, k):\n",
    "     #   word = \"w\" + str(i)\n",
    "      #  for j in range(0, len(topk)):\n",
    "       #     outputMat[i][j] = (topk[i][j], word)\n",
    "            \n",
    "    print(outputMat)\n",
    "    #sorts the words acording to their scores\n",
    "    outputMat = np.transpose(outputMat)\n",
    "    outputMat = outputMat.apply(lambda x: x.sort_values(ascending = False).values)\n",
    "    outputMat = np.transpose(outputMat)\n",
    "    finalMat = outputMat\n",
    "    #for i in range(0, k):\n",
    "     #   for j in range(0, len(outputMat)):\n",
    "      #      finalMat[i][j] = (outputMat[i][j][1], outputMat[i][j][0])\n",
    "            \n",
    "    file = open(\"./userOutput.txt\", \"w\")\n",
    "    file.write(str(finalMat))\n",
    "    file.close()\n",
    "    return finalMat\n",
    "    \n",
    "    \n",
    "def task1(vectModel, useOp, k):\n",
    "    \n",
    "    if useOp == \"PCA\":\n",
    "        #PCA\n",
    "        wordMat = makeMat(vectModel)\n",
    "        \n",
    "        topk = PCAsetup(wordMat, k)\n",
    "        print(\"\\nPCA:\\n\")\n",
    "        #print(topk)\n",
    "        \n",
    "        dictofComponents = createdictofComponents(topk, k)\n",
    "        print(dictofComponents)\n",
    "        \n",
    "        \n",
    "    elif useOp == \"SVD\":\n",
    "        wordMat = makeMat(vectModel)\n",
    "        topk = SVDsetup(wordMat, k)\n",
    "        print(\"\\nSVD:\\n\")\n",
    "        #print(topk)\n",
    "        \n",
    "        dictofComponents = createdictofComponents(topk, k)\n",
    "        print(dictofComponents)\n",
    "        \n",
    "        \n",
    "    elif useOp == \"NMF\":\n",
    "        wordMat = makeMat(vectModel)\n",
    "        topk = NMFsetup(wordMat, k)\n",
    "        print(\"\\nNMF:\\n\")\n",
    "        #print(topk)\n",
    "        \n",
    "        dictofComponents = createdictofComponents(topk, k)\n",
    "        print(dictofComponents)\n",
    "\n",
    "        \n",
    "    elif useOp == \"LDA\":\n",
    "        wordMat = makeMat(vectModel)\n",
    "        topk = LDAsetup(wordMat, k)\n",
    "        print(\"\\nLDA:\\n\")\n",
    "        #print(topk)\n",
    "        \n",
    "        dictofComponents = createdictofComponents(topk, k)\n",
    "        print(dictofComponents)\n",
    "        \n",
    "\n",
    "\n",
    "vectModel = input(\"Enter the vector model (ex: tf): \")\n",
    "k = input(\"Enter k (ex: 4): \")\n",
    "useOp = input(\"Enter the analysis you would like to use (ex: PCA): \")\n",
    "k = int(k)\n",
    "task1(vectModel, useOp, k)\n",
    "\n",
    "\n",
    "#sample output: \n",
    "#Enter the vector model: tf\n",
    "#Enter k: 10\n",
    "#Enter the analysis you would like to use: PCA\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter gesture file(e.g Data/1.csv) :\n",
      "* WARNING : Please make .pkl in TASK 1 before you use principal component\n",
      "Data/59.csv\n",
      "Enter vector model (tf, tfidf):\n",
      "tfidf\n",
      "Enter user options (1 ~ 7)\n",
      "* HINT : 1 = Dot similarity, 2 = PCA, 3 = SVD, 4 = NMF, 5 = LDA, 6 = Edit Distance, 7 = DTW\n",
      "7\n",
      "Most similar (gesture, score) \n",
      "(59, 0.0)\n",
      "(3, 517.226686729692)\n",
      "(0, 546.2733304634784)\n",
      "(32, 548.2429271479214)\n",
      "(42, 588.7590218944844)\n",
      "(14, 595.3573641879669)\n",
      "(2, 599.1992888542443)\n",
      "(45, 606.6645926870148)\n",
      "(6, 630.4273810272091)\n",
      "(36, 636.7623912495804)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity                          \n",
    "from math import log\n",
    "import re\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy import spatial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log2\n",
    "\n",
    "\n",
    "def dot_similarity(gesture1, gesture2):                  \n",
    "    x = np.array(gesture1)\n",
    "    y = np.array(gesture2)\n",
    "    \"\"\"if len(x) > len(y):\n",
    "        y = np.pad(y, (0, len(x) - len(y)))\n",
    "    else:\n",
    "        x = np.pad(x, (0, len(y) - len(x)))\"\"\"\n",
    "    return np.dot(x, y)\n",
    "\n",
    "def pears_similarity(vec1, vec2):\n",
    "    pearson_coef, p_value = stats.pearsonr(vec1, vec2)\n",
    "    return pearson_coef\n",
    "    \n",
    "def cos_similarity(vec1, vec2):                            \n",
    "    x = np.array(vec1)\n",
    "    y = np.array(vec2)                                                    \n",
    "    similarity = 1- spatial.distance.cosine(x, y)                             \n",
    "                                                                            \n",
    "    return similarity                             \n",
    "\n",
    "def KL_div_similarity(p, q):\n",
    "    return sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n",
    "\n",
    "def edit_distance(sensor1, sensor2, m, n):\n",
    "    # Create a table to store results of subproblems \n",
    "    dp = [[0 for x in range(n + 1)] for x in range(m + 1)] \n",
    "  \n",
    "    # Fill d[][] in bottom up manner \n",
    "    for i in range(m + 1): \n",
    "        for j in range(n + 1): \n",
    "  \n",
    "            # If first string is empty, only option is to \n",
    "            # insert all characters of second string \n",
    "            if i == 0: \n",
    "                dp[i][j] = j    # Min. operations = j \n",
    "  \n",
    "            # If second string is empty, only option is to \n",
    "            # remove all characters of second string \n",
    "            elif j == 0: \n",
    "                dp[i][j] = i    # Min. operations = i \n",
    "  \n",
    "            # If last characters are same, ignore last char \n",
    "            # and recur for remaining string \n",
    "            elif sensor1[i-1] == sensor2[j-1]: \n",
    "                dp[i][j] = dp[i-1][j-1] \n",
    "  \n",
    "            # If last character are different, consider all \n",
    "            # possibilities and find minimum \n",
    "            else: \n",
    "                dp[i][j] = 1 + min(dp[i][j-1],        # Insert \n",
    "                                   dp[i-1][j],        # Remove \n",
    "                                   dp[i-1][j-1])    # Replace \n",
    "  \n",
    "    return dp[m][n] \n",
    "    \n",
    "def dynamic_time_warping(sensor1, sensor2, m, n):\n",
    "    dp = [[0 for x in range(n+1)] for x in range(m+1)] \n",
    "    \n",
    "    for i in range(1, m+1): \n",
    "        for j in range(1, n+1): \n",
    "            dp[i][j] = abs(sensor1[i-1] - sensor2[j-1])\n",
    "             \n",
    "            if i == 1 and j == 1:\n",
    "                continue\n",
    "            elif i == 1 and j != 1:\n",
    "                dp[i][j] += dp[i][j-1] \n",
    "            elif i != 1 and j == 1:\n",
    "                dp[i][j] += dp[i-1][j]\n",
    "            else:\n",
    "                dp[i][j] += min(dp[i][j-1],        # Insert \n",
    "                                dp[i-1][j],        # Remove \n",
    "                                dp[i-1][j-1])    # Replace \n",
    "            \n",
    "    return dp[m][n] \n",
    "\n",
    "def tfidf_loader(directory):\n",
    "    gestures = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.startswith(\"tfidf_\") or not filename.endswith(\".txt\") :\n",
    "            continue\n",
    "            \n",
    "        path = directory+\"/\"+filename\n",
    "        with open(path, \"r\") as w:\n",
    "            gesture = w.readlines()\n",
    "\n",
    "        matrix=[]\n",
    "\n",
    "        for sensor in gesture :\n",
    "            tfidf = float(sensor.split(\"-\")[1].strip())\n",
    "            matrix.append(tfidf)\n",
    "                \n",
    "        gestures.append(matrix)\n",
    "        \n",
    "    return gestures\n",
    "    \n",
    "    \n",
    "def tf_loader(directory):\n",
    "    gestures = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.startswith(\"tf_\") or not filename.endswith(\".txt\") :\n",
    "            continue\n",
    "            \n",
    "        path = directory+\"/\"+filename\n",
    "        with open(path, \"r\") as w:\n",
    "            gesture = w.readlines()\n",
    "\n",
    "        matrix=[]\n",
    "\n",
    "        for sensor in gesture :\n",
    "            tf = float(sensor.split(\"-\")[1].strip())   \n",
    "            matrix.append(tf)\n",
    "        gestures.append(matrix)\n",
    "        \n",
    "    return gestures\n",
    "    \n",
    "def symbol_loader(directory):\n",
    "    gestures = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".wrd\") :\n",
    "            continue\n",
    "        path = directory+\"/\"+filename\n",
    "        with open(path, \"r\") as w:\n",
    "            gesture = w.readlines()\n",
    "\n",
    "        matrix=[]\n",
    "\n",
    "        for sensor in gesture :\n",
    "            sym_quant_window = [re.findall(r'\\d+',word)[0] for word in sensor.split(\" - \")[1].split(\",\")] \n",
    "            matrix.append(sym_quant_window)\n",
    "                \n",
    "        gestures.append(matrix)\n",
    "        \n",
    "    return gestures\n",
    "\n",
    "def amplitude_loader(directory):\n",
    "    gestures = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".wrd\") :\n",
    "            continue\n",
    "        path = directory+\"/\"+filename\n",
    "        with open(path, \"r\") as w:\n",
    "            gesture = w.readlines()\n",
    "\n",
    "        matrix=[]\n",
    "\n",
    "        for sensor in gesture :\n",
    "            avg_quant_ampitude = [float(word) for word in sensor.split(\" - \")[0].split(\"[\")[1].replace(\"],\",\"\").split(\",\")]\n",
    "            matrix.append(avg_quant_ampitude)\n",
    "                \n",
    "        gestures.append(matrix)\n",
    "        \n",
    "    return gestures\n",
    "\n",
    "###### GET Input from users######\n",
    "print(\"Enter gesture file(e.g Data/1.csv) :\")\n",
    "print(\"* WARNING : Please make .pkl in TASK 1 before you use principal component\")\n",
    "gesture_path = input()\n",
    "\n",
    "directory = gesture_path.split(\"/\")[0]\n",
    "#axis = gesture_path.split(\"/\")[1]\n",
    "filename = gesture_path.split(\"/\")[1]\n",
    "key_idx = int(re.findall(r'\\d+', filename)[0])\n",
    "\n",
    "print(\"Enter vector model (tf, tfidf):\")\n",
    "vector_model = input()\n",
    "\n",
    "print(\"Enter user options (1 ~ 7)\")\n",
    "print(\"* HINT : 1 = Dot similarity, 2 = PCA, 3 = SVD, 4 = NMF, 5 = LDA, 6 = Edit Distance, 7 = DTW\")\n",
    "user_option = int(input())  \n",
    "\n",
    "#Retrieve only top 10 gestures with high similarity\n",
    "top_K = 10\n",
    "\n",
    "#Calculate similarity(cost) based on User options\n",
    "if user_option == 1 :\n",
    "    directory = directory + \"/\"\n",
    "    gestures = makeMat(vector_model)\n",
    "    directory = directory.replace(\"/\", \"\")\n",
    "    \n",
    "    cost=[]\n",
    "    key_gesture = gestures[key_idx]\n",
    "    for gesture in gestures :\n",
    "        similarity = dot_similarity(key_gesture, gesture)\n",
    "        cost.append(similarity)\n",
    "    cost_top_K = sorted(cost, reverse=True)[0:top_K]                                        \n",
    "    \n",
    "elif user_option == 2 :\n",
    "    PC_path = [\"PCA\", vector_model]\n",
    "    PC_path = \"_\".join(PC_path)\n",
    "    pca = pd.read_pickle(PC_path + \".pkl\")\n",
    "    num = len(pca[0])\n",
    "\n",
    "    pca = pca.T\n",
    "    key_vec = pca[key_idx]\n",
    "    cost=[]\n",
    "    for idx in range(num) :\n",
    "        similarity = pears_similarity(key_vec, pca[idx])\n",
    "        cost.append(similarity)\n",
    "    cost_top_K = sorted(cost, reverse=True)[0:top_K]                                        \n",
    "\n",
    "elif user_option == 3 : \n",
    "    PC_path = [\"SVD\", vector_model]\n",
    "    PC_path = \"_\".join(PC_path)\n",
    "    svd = pd.read_pickle(PC_path + \".pkl\")                                      \n",
    "    num = len(svd[0])                                                           \n",
    "                                                                                \n",
    "    svd = svd.T                                                                 \n",
    "    key_vec = svd[key_idx]      \n",
    "    \n",
    "    cost=[]                                                                     \n",
    "    for idx in range(num) :                                                     \n",
    "        similarity = cos_similarity(key_vec, svd[idx])                            \n",
    "        cost.append(similarity)                 \n",
    "    cost_top_K = sorted(cost, reverse=True)[0:top_K]                                        \n",
    "    \n",
    "elif user_option == 4 :                                                         \n",
    "    PC_path = [\"NMF\", vector_model]\n",
    "    PC_path = \"_\".join(PC_path)\n",
    "    nmf = pd.read_pickle(PC_path + \".pkl\")\n",
    "    num = len(nmf[0])                                                           \n",
    "                                                                                \n",
    "    nmf = nmf.T                                                                 \n",
    "    key_vec = nmf[key_idx]    \n",
    "    \n",
    "    cost=[]                                                                     \n",
    "    for idx in range(num) :                                                     \n",
    "        similarity = cos_similarity(key_vec, nmf[idx])                            \n",
    "        cost.append(similarity)           \n",
    "    cost_top_K = sorted(cost, reverse=True)[0:top_K]                                        \n",
    "                  \n",
    "elif user_option == 5 :                                                         \n",
    "    PC_path = [\"LDA\", vector_model]\n",
    "    PC_path = \"_\".join(PC_path)\n",
    "    lda = pd.read_pickle(PC_path + \".pkl\")\n",
    "    num = len(lda[0])                                                           \n",
    "                                                                                \n",
    "    lda = lda.T                                                                 \n",
    "    key_vec = lda[key_idx]    \n",
    "    \n",
    "    cost=[]                                                                     \n",
    "    for idx in range(num) :                                                     \n",
    "        similarity = KL_div_similarity(key_vec, lda[idx])                            \n",
    "        cost.append(similarity)                                  \n",
    "    cost_top_K = sorted(cost, reverse=True)[0:top_K]                                        \n",
    "\n",
    "elif user_option == 6 :\n",
    "    pathW = directory + \"/W\"\n",
    "    pathX = directory + \"/X\"\n",
    "    pathY = directory + \"/Y\"\n",
    "    pathZ = directory + \"/Z\"\n",
    "    gesturesW = symbol_loader(pathW)\n",
    "    gesturesX = symbol_loader(pathX)\n",
    "    gesturesY = symbol_loader(pathY)\n",
    "    gesturesZ = symbol_loader(pathZ)\n",
    "    \n",
    "    gestures = np.append(gesturesW, gesturesX, axis = 1)\n",
    "    gestures = np.append(gestures, gesturesY, axis = 1)\n",
    "    gestures = np.append(gestures, gesturesZ, axis = 1)\n",
    "        \n",
    "    key_gesture = gestures[key_idx]\n",
    "    \n",
    "    cost=[0]*len(gestures)\n",
    "    for i, gesture in enumerate(gestures) :\n",
    "        for j, sensor in enumerate(gesture) :\n",
    "            n = len(sensor)\n",
    "            m = len(key_gesture[j])\n",
    "            cost[i] += edit_distance(key_gesture[j], sensor, m, n)\n",
    "    cost_top_K = sorted(cost)[0:top_K]                                        \n",
    "\n",
    "elif user_option == 7 :\n",
    "    pathW = directory + \"/W\"\n",
    "    pathX = directory + \"/X\"\n",
    "    pathY = directory + \"/Y\"\n",
    "    pathZ = directory + \"/Z\"\n",
    "    gesturesW = amplitude_loader(pathW)\n",
    "    gesturesX = amplitude_loader(pathX)\n",
    "    gesturesY = amplitude_loader(pathY)\n",
    "    gesturesZ = amplitude_loader(pathZ)\n",
    "    \n",
    "    gestures = np.append(gesturesW, gesturesX, axis = 1)\n",
    "    gestures = np.append(gestures, gesturesY, axis = 1)\n",
    "    gestures = np.append(gestures, gesturesZ, axis = 1)\n",
    "    \n",
    "    key_gesture = gestures[key_idx]\n",
    "    \n",
    "    cost=[0]*len(gestures)\n",
    "    for i, gesture in enumerate(gestures) :                                     \n",
    "        for j, sensor in enumerate(gesture) :                                   \n",
    "            n = len(sensor)                                                     \n",
    "            m = len(key_gesture[j])                                             \n",
    "            cost[i] += dynamic_time_warping(key_gesture[j], sensor, m, n)    \n",
    "    cost_top_K = sorted(cost)[0:top_K]                                        \n",
    "else:\n",
    "    print(\"ERROR : No such user option in this program\")\n",
    "    \n",
    "                                                           \n",
    "print(\"Most similar (gesture, score) \")              \n",
    "for k in cost_top_K :                                                       \n",
    "    print((cost.index(k), k))       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter directory (e.g. Data/) :\n",
      "Data/\n",
      "Enter value p :\n",
      "2\n",
      "Enter user options (1 ~ 7)\n",
      "* HINT : 1 = Dot similarity, 2 = PCA, 3 = SVD, 4 = NMF, 5 = LDA, 6 = Edit Distance, 7 = DTW\n",
      "7\n",
      "Enter vector model (tf, tfidf):\n",
      "tfidf\n",
      "Number of gesture :  60\n",
      "  (0, 1)\t645.5961929219249\n",
      "  (0, 2)\t359.8403641295804\n",
      "  (0, 3)\t543.429668243926\n",
      "  (0, 4)\t966.7755832345446\n",
      "  (0, 5)\t497.5924771129154\n",
      "  (0, 6)\t439.30065925015754\n",
      "  (0, 7)\t745.411470209462\n",
      "  (0, 8)\t571.9781967746479\n",
      "  (0, 9)\t904.3991199087293\n",
      "  (0, 10)\t467.8503101181589\n",
      "  (0, 11)\t474.80572628258795\n",
      "  (0, 12)\t532.2697940849744\n",
      "  (0, 13)\t756.0596523840481\n",
      "  (0, 14)\t400.65403645152\n",
      "  (0, 15)\t499.71817994456813\n",
      "  (0, 16)\t417.04063289155545\n",
      "  (0, 17)\t435.5735241966867\n",
      "  (0, 18)\t442.93062634620077\n",
      "  (0, 19)\t470.83767546008056\n",
      "  (0, 20)\t490.862333477008\n",
      "  (0, 21)\t528.6210835266593\n",
      "  (0, 22)\t547.6178609120802\n",
      "  (0, 23)\t485.2150234476063\n",
      "  (0, 24)\t507.10800152537547\n",
      "  (0, 25)\t687.603554166801\n",
      "  :\t:\n",
      "  (59, 34)\t784.4059098547118\n",
      "  (59, 35)\t1219.993378772826\n",
      "  (59, 36)\t636.7623912495804\n",
      "  (59, 37)\t1191.5636093679727\n",
      "  (59, 38)\t656.0084044187993\n",
      "  (59, 39)\t727.5443743533627\n",
      "  (59, 40)\t757.4289455085697\n",
      "  (59, 41)\t821.2318229683314\n",
      "  (59, 42)\t588.7590218944844\n",
      "  (59, 43)\t777.6913172177472\n",
      "  (59, 44)\t777.5603039694053\n",
      "  (59, 45)\t606.6645926870148\n",
      "  (59, 46)\t653.3181800296182\n",
      "  (59, 47)\t878.2404429733319\n",
      "  (59, 48)\t650.3199499630684\n",
      "  (59, 49)\t761.343967810399\n",
      "  (59, 50)\t661.2571867687749\n",
      "  (59, 51)\t803.8823350277112\n",
      "  (59, 52)\t796.0249592811748\n",
      "  (59, 53)\t757.9445927247035\n",
      "  (59, 54)\t794.1625351864047\n",
      "  (59, 55)\t976.6323160285602\n",
      "  (59, 56)\t819.2281273969094\n",
      "  (59, 57)\t685.3224877504643\n",
      "  (59, 58)\t654.6319060185971\n",
      "------TOP P SVD after  DTW -----\n",
      "* ordered in gesture, score\n",
      "            0            1            2            3            4   \\\n",
      "0  4391.453822  5691.632274  4342.108846  4895.653483  6878.909518   \n",
      "1  -178.788204   541.811235  -278.865602  -369.159523  1159.360103   \n",
      "\n",
      "            5            6            7            8            9   \\\n",
      "0  4739.398306  4110.685299  6967.812879  4700.360154  7063.964870   \n",
      "1  -326.552080  -667.328898  1033.683176  -635.491637  1067.916762   \n",
      "\n",
      "      ...                50           51           52           53  \\\n",
      "0     ...       4655.942328  4627.241152  4384.904330  4528.582570   \n",
      "1     ...       -804.663476  -603.871098  -952.861942  -947.365392   \n",
      "\n",
      "            54           55           56           57           58  \\\n",
      "0  4496.680707  5360.730812  4919.473139  6943.759630  4509.657222   \n",
      "1  -609.210920  -845.981681  -598.809320  1041.139629   -56.111127   \n",
      "\n",
      "            59  \n",
      "0  5890.687784  \n",
      "1   570.892543  \n",
      "\n",
      "[2 rows x 60 columns]\n",
      "------TOP P NMF after  DTW -----\n",
      "* ordered in gesture, score\n",
      "          0          1          2          3          4          5   \\\n",
      "0  19.360819  20.123637  19.797989  22.670558  21.080023  21.751294   \n",
      "1  10.263689  21.814650   9.026860   9.580427  31.917574   9.609835   \n",
      "\n",
      "          6          7          8          9     ...             50  \\\n",
      "0  21.331678  22.254683  23.572733  22.432914    ...      24.475066   \n",
      "1   4.111193  30.784640   6.103759  31.430211    ...       4.118835   \n",
      "\n",
      "          51         52         53         54         55         56  \\\n",
      "0  23.066594  24.303594  24.863444  22.558829  27.664923  24.245049   \n",
      "1   6.247659   1.732239   2.194097   5.826307   5.624564   7.118955   \n",
      "\n",
      "          57         58         59  \n",
      "0  22.107538  19.061458  20.761228  \n",
      "1  30.798836  11.945426  22.690995  \n",
      "\n",
      "[2 rows x 60 columns]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity                          \n",
    "from math import log\n",
    "import re\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy import spatial\n",
    "import pandas as pd\n",
    "import numpy as np                  \n",
    "from math import log2                                                           \n",
    "                                                                                \n",
    "def KL_div_similarity(p, q):                                                    \n",
    "    return sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n",
    "\n",
    "def dot_similarity(gesture1, gesture2):                  \n",
    "    x = np.array(gesture1)\n",
    "    y = np.array(gesture2)\n",
    "    \"\"\"if len(x) > len(y):\n",
    "        y = np.pad(y, (0, len(x) - len(y)))\n",
    "    else:\n",
    "        x = np.pad(x, (0, len(y) - len(x)))\"\"\"\n",
    "    return np.dot(x, y)\n",
    "\n",
    "def pears_similarity(vec1, vec2):\n",
    "    pearson_coef, p_value = stats.pearsonr(vec1, vec2)\n",
    "    return pearson_coef\n",
    "    \n",
    "def cos_similarity(vec1, vec2):                            \n",
    "    x = np.array(vec1)\n",
    "    y = np.array(vec2)                                                    \n",
    "    similarity = 1- spatial.distance.cosine(x, y)                             \n",
    "                                                                            \n",
    "    return similarity                             \n",
    "\n",
    "def edit_distance(sensor1, sensor2, m, n):\n",
    "    # Create a table to store results of subproblems \n",
    "    dp = [[0 for x in range(n + 1)] for x in range(m + 1)] \n",
    "  \n",
    "    # Fill d[][] in bottom up manner \n",
    "    for i in range(m + 1): \n",
    "        for j in range(n + 1): \n",
    "  \n",
    "            # If first string is empty, only option is to \n",
    "            # insert all characters of second string \n",
    "            if i == 0: \n",
    "                dp[i][j] = j    # Min. operations = j \n",
    "  \n",
    "            # If second string is empty, only option is to \n",
    "            # remove all characters of second string \n",
    "            elif j == 0: \n",
    "                dp[i][j] = i    # Min. operations = i \n",
    "  \n",
    "            # If last characters are same, ignore last char \n",
    "            # and recur for remaining string \n",
    "            elif sensor1[i-1] == sensor2[j-1]: \n",
    "                dp[i][j] = dp[i-1][j-1] \n",
    "  \n",
    "            # If last character are different, consider all \n",
    "            # possibilities and find minimum \n",
    "            else: \n",
    "                dp[i][j] = 1 + min(dp[i][j-1],        # Insert \n",
    "                                   dp[i-1][j],        # Remove \n",
    "                                   dp[i-1][j-1])    # Replace \n",
    "  \n",
    "    return dp[m][n] \n",
    "    \n",
    "def dynamic_time_warping(sensor1, sensor2, m, n):\n",
    "    dp = [[0 for x in range(n+1)] for x in range(m+1)] \n",
    "    \n",
    "    for i in range(1, m+1): \n",
    "        for j in range(1, n+1): \n",
    "            dp[i][j] = abs(sensor1[i-1] - sensor2[j-1])\n",
    "             \n",
    "            if i == 1 and j == 1:\n",
    "                continue\n",
    "            elif i == 1 and j != 1:\n",
    "                dp[i][j] += dp[i][j-1] \n",
    "            elif i != 1 and j == 1:\n",
    "                dp[i][j] += dp[i-1][j]\n",
    "            else:\n",
    "                dp[i][j] += min(dp[i][j-1],        # Insert \n",
    "                                dp[i-1][j],        # Remove \n",
    "                                dp[i-1][j-1])    # Replace \n",
    "            \n",
    "    return dp[m][n] \n",
    "\n",
    "def tfidf_loader(directory):\n",
    "    gestures = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.startswith(\"tfidf_\") or not filename.endswith(\".txt\") :\n",
    "            continue\n",
    "            \n",
    "        path = directory+\"/\"+filename\n",
    "        with open(path, \"r\") as w:\n",
    "            gesture = w.readlines()\n",
    "\n",
    "        matrix=[]\n",
    "\n",
    "        for sensor in gesture :\n",
    "            tfidf = float(sensor.split(\"-\")[1].strip())\n",
    "            matrix.append(tfidf)\n",
    "                \n",
    "        gestures.append(matrix)\n",
    "        \n",
    "    return gestures\n",
    "    \n",
    "    \n",
    "def tf_loader(directory):\n",
    "    gestures = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.startswith(\"tf_\") or not filename.endswith(\".txt\") :\n",
    "            continue\n",
    "            \n",
    "        path = directory+\"/\"+filename\n",
    "        with open(path, \"r\") as w:\n",
    "            gesture = w.readlines()\n",
    "\n",
    "        matrix=[]\n",
    "\n",
    "        for sensor in gesture :\n",
    "            tf = float(sensor.split(\"-\")[1].strip())   \n",
    "            matrix.append(tf)\n",
    "        gestures.append(matrix)\n",
    "        \n",
    "    return gestures\n",
    "    \n",
    "def symbol_loader(directory):\n",
    "    gestures = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".wrd\") :\n",
    "            continue\n",
    "        path = directory+\"/\"+filename\n",
    "        with open(path, \"r\") as w:\n",
    "            gesture = w.readlines()\n",
    "\n",
    "        matrix=[]\n",
    "\n",
    "        for sensor in gesture :\n",
    "            sym_quant_window = [re.findall(r'\\d+',word)[0] for word in sensor.split(\" - \")[1].split(\",\")] \n",
    "            matrix.append(sym_quant_window)\n",
    "                \n",
    "        gestures.append(matrix)\n",
    "        \n",
    "    return gestures\n",
    "\n",
    "def amplitude_loader(directory):\n",
    "    gestures = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".wrd\") :\n",
    "            continue\n",
    "        path = directory+\"/\"+filename\n",
    "        with open(path, \"r\") as w:\n",
    "            gesture = w.readlines()\n",
    "\n",
    "        matrix=[]\n",
    "\n",
    "        for sensor in gesture :\n",
    "            avg_quant_ampitude = [float(word) for word in sensor.split(\" - \")[0].split(\"[\")[1].replace(\"],\",\"\").split(\",\")]\n",
    "            matrix.append(avg_quant_ampitude)\n",
    "                \n",
    "        gestures.append(matrix)\n",
    "        \n",
    "    return gestures\n",
    "\n",
    "print(\"Enter directory (e.g. Data/) :\")\n",
    "directory = input()\n",
    "#axis = directory.split(\"/\")[1]\n",
    "\n",
    "print(\"Enter value p :\")\n",
    "p = int(input())\n",
    "print(\"Enter user options (1 ~ 7)\")\n",
    "print(\"* HINT : 1 = Dot similarity, 2 = PCA, 3 = SVD, 4 = NMF, 5 = LDA, 6 = Edit Distance, 7 = DTW\")\n",
    "user_option = int(input())  \n",
    "\n",
    "print(\"Enter vector model (tf, tfidf):\")\n",
    "vector_model = input()\n",
    "\n",
    "num_gestures = 0\n",
    "for filename in os.listdir(directory + \"X\"):                                          \n",
    "    if not filename.endswith(\".csv\") :                                          \n",
    "        continue\n",
    "    num_gestures+=1\n",
    "print(\"Number of gesture : \", num_gestures)\n",
    "gest_gest_sim = [0.0]*num_gestures\n",
    "for filename in os.listdir(directory + \"X\"):                                          \n",
    "    if not filename.endswith(\".csv\") :                                          \n",
    "        continue                    \n",
    "    #get indext of file                                            \n",
    "    key_idx = int(filename.split(\".\")[0])\n",
    "\n",
    "    if user_option == 1:\n",
    "        outname=\"DP\"\n",
    "        \n",
    "        gestures = makeMat(vector_model)\n",
    "        \n",
    "        #gestures = tf_loader(directory)\n",
    "        key_gesture = gestures[key_idx-1]\n",
    "        cost=[]\n",
    "        for gesture in gestures :\n",
    "            similarity = dot_similarity(key_gesture, gesture)\n",
    "            cost.append(similarity)\n",
    "        gest_gest_sim[key_idx-1] = cost\n",
    "    \n",
    "    elif user_option == 2:                                                        \n",
    "        outname=\"PCA\"\n",
    "        PC_path = [\"PCA\", vector_model]\n",
    "        PC_path = \"_\".join(PC_path)\n",
    "\n",
    "        pca = pd.read_pickle(PC_path + \".pkl\")\n",
    "        num = len(pca[0])\n",
    "        pca = pca.T\n",
    "        key_vec = pca[key_idx-1]\n",
    "        cost=[]\n",
    "        for idx in range(num) :\n",
    "            similarity = pears_similarity(key_vec, pca[idx])\n",
    "            cost.append(similarity)\n",
    "        gest_gest_sim[key_idx-1] = cost                                         \n",
    "    elif user_option == 3:\n",
    "        outname=\"SVD\"\n",
    "        PC_path = [\"SVD\", vector_model]\n",
    "        PC_path = \"_\".join(PC_path)\n",
    "        svd = pd.read_pickle(PC_path + \".pkl\")                                      \n",
    "        num = len(svd[0])                                                           \n",
    "                                                                                    \n",
    "        svd = svd.T                                                                 \n",
    "        key_vec = svd[key_idx-1]      \n",
    "        \n",
    "        cost=[]                                                                     \n",
    "        for idx in range(num) :                                                     \n",
    "            similarity = cos_similarity(key_vec, svd[idx])                            \n",
    "            cost.append(similarity)                 \n",
    "        gest_gest_sim[key_idx-1] = cost                                         \n",
    "    elif user_option == 4:\n",
    "        outname=\"NMF\"\n",
    "        PC_path = [\"NMF\", vector_model]\n",
    "        PC_path = \"_\".join(PC_path)\n",
    "        nmf = pd.read_pickle(PC_path + \".pkl\")\n",
    "        num = len(nmf[0])                                                           \n",
    "                                                                                    \n",
    "        nmf = nmf.T                                                                 \n",
    "        key_vec = nmf[key_idx-1]    \n",
    "        \n",
    "        cost=[]                                                                     \n",
    "        for idx in range(num) :                                                     \n",
    "            similarity = cos_similarity(key_vec, nmf[idx])                           \n",
    "            cost.append(similarity)\n",
    "        gest_gest_sim[key_idx-1] = cost                                         \n",
    "\n",
    "    elif user_option == 5: \n",
    "        outname=\"LDA\"\n",
    "        PC_path = [\"LDA\", vector_model]\n",
    "        PC_path = \"_\".join(PC_path)\n",
    "        lda = pd.read_pickle(PC_path + \".pkl\")\n",
    "        num = len(lda[0])                                                           \n",
    "                                                                                    \n",
    "        lda = lda.T                                                                 \n",
    "        key_vec = lda[key_idx-1]    \n",
    "        \n",
    "        cost=[]                                                                     \n",
    "        for idx in range(num) :                                                     \n",
    "            similarity = KL_div_similarity(key_vec, lda[idx])                            \n",
    "            cost.append(similarity)        \n",
    "        gest_gest_sim[key_idx-1] = cost                                         \n",
    "    elif user_option == 6 :\n",
    "        outname=\"ED\"\n",
    "        pathW = directory + \"/W\"\n",
    "        pathX = directory + \"/X\"\n",
    "        pathY = directory + \"/Y\"\n",
    "        pathZ = directory + \"/Z\"\n",
    "        gesturesW = symbol_loader(pathW)\n",
    "        gesturesX = symbol_loader(pathX)\n",
    "        gesturesY = symbol_loader(pathY)\n",
    "        gesturesZ = symbol_loader(pathZ)\n",
    "    \n",
    "        gestures = np.append(gesturesW, gesturesX, axis = 1)\n",
    "        gestures = np.append(gestures, gesturesY, axis = 1)\n",
    "        gestures = np.append(gestures, gesturesZ, axis = 1)\n",
    "        \n",
    "        #gestures = symbol_loader(path)\n",
    "        key_gesture = gestures[key_idx-1]\n",
    "        \n",
    "        cost=[0]*len(gestures)\n",
    "        for i, gesture in enumerate(gestures) :\n",
    "            for j, sensor in enumerate(gesture) :\n",
    "                n = len(sensor)\n",
    "                m = len(key_gesture[j])\n",
    "                cost[i] += edit_distance(key_gesture[j], sensor, m, n)\n",
    "        gest_gest_sim[key_idx-1] = cost                                         \n",
    "\n",
    "    elif user_option == 7 :\n",
    "        outname=\"DTW\"\n",
    "        \n",
    "        pathW = directory + \"/W\"\n",
    "        pathX = directory + \"/X\"\n",
    "        pathY = directory + \"/Y\"\n",
    "        pathZ = directory + \"/Z\"\n",
    "        gesturesW = amplitude_loader(pathW)\n",
    "        gesturesX = amplitude_loader(pathX)\n",
    "        gesturesY = amplitude_loader(pathY)\n",
    "        gesturesZ = amplitude_loader(pathZ)\n",
    "    \n",
    "        gestures = np.append(gesturesW, gesturesX, axis = 1)\n",
    "        gestures = np.append(gestures, gesturesY, axis = 1)\n",
    "        gestures = np.append(gestures, gesturesZ, axis = 1)\n",
    "        \n",
    "        #gestures = amplitude_loader(path)\n",
    "        key_gesture = gestures[key_idx-1]\n",
    "        \n",
    "        cost=[0]*len(gestures)\n",
    "        for i, gesture in enumerate(gestures) :                                     \n",
    "            for j, sensor in enumerate(gesture) :                                   \n",
    "                n = len(sensor)                                                     \n",
    "                m = len(key_gesture[j])                                             \n",
    "                cost[i] += dynamic_time_warping(key_gesture[j], sensor, m, n)    \n",
    "        gest_gest_sim[key_idx-1] = cost                                         \n",
    "     \n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD                                  \n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import random as sparse_random\n",
    "from sklearn.random_projection import sparse_random_matrix\n",
    "from sklearn.decomposition import NMF\n",
    "matrix = coo_matrix(gest_gest_sim,shape=(num_gestures,num_gestures)) \n",
    "print(matrix)\n",
    "np.savetxt(\"gest_sim.csv\", gest_gest_sim, delimiter=',')\n",
    "svd = TruncatedSVD(n_components=p)\n",
    "pc = svd.fit_transform(matrix)\n",
    "df = pd.DataFrame(data = pc) \n",
    "df = df.T\n",
    "print(\"------TOP P SVD after \", outname, \"-----\")\n",
    "print(\"* ordered in gesture, score\")\n",
    "print(df)\n",
    "df.to_pickle(\"./\"+\"SVD_\"+outname+\".pkl\")\n",
    "np.savetxt('component_SVD.csv', svd.components_, delimiter=',')\n",
    "\n",
    "\n",
    "nmf = NMF(n_components=p)\n",
    "pc = nmf.fit_transform(matrix)\n",
    "df = pd.DataFrame(data = pc) \n",
    "df = df.T\n",
    "print(\"------TOP P NMF after \", outname, \"-----\")\n",
    "print(\"* ordered in gesture, score\")\n",
    "print(df)\n",
    "df.to_pickle(\"./\"+\"NMF_\"+outname+\".pkl\")\n",
    "np.savetxt('component_NMF.csv', nmf.components_, delimiter=',')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
