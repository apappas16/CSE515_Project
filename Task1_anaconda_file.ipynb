{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the data directory path: Data/\n",
      "Enter the window length: 2\n",
      "Enter the shift length: 2\n",
      "Enter the resolution: 2\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "import os\n",
    "from scipy.integrate import quad\n",
    "\n",
    "\n",
    "\n",
    "def calcAvgSensorValue(sensorValues):\n",
    "    avg = sum(sensorValues) / len(sensorValues)\n",
    "    avg = round(avg, 9)\n",
    "    return avg\n",
    "\n",
    "\n",
    "def calcStdDev(sensorValues, meanVal):\n",
    "    sd = []\n",
    "    for num in sensorValues:\n",
    "        result = num - meanVal\n",
    "        result = result * result\n",
    "        sd.append(result)\n",
    "    return calcAvgSensorValue(sd)\n",
    "\n",
    "\n",
    "def normalize(sensorValues):\n",
    "    normalized_sensor = []\n",
    "    for val in sensorValues:\n",
    "        val = float(val)\n",
    "        normalized = 2 * ((val - min(sensorValues)) / (max(sensorValues) - min(sensorValues))) - 1\n",
    "        normalized_sensor.append(normalized)\n",
    "    return normalized_sensor\n",
    "\n",
    "\n",
    "def integral(i):\n",
    "    return getGaussianVal(i, 0, 0.25)\n",
    "\n",
    "\n",
    "def getGaussianVal(i, avg, sd):\n",
    "    i = float(i - avg) / sd\n",
    "    gauss = math.exp(-i * i / 2.0) / math.sqrt(2.0 * math.pi) / sd\n",
    "    return gauss\n",
    "\n",
    "\n",
    "def determineBands():\n",
    "    numBands = r * 2\n",
    "    bandList = []\n",
    "    bandStart = -1\n",
    "    for i in range(1, numBands):\n",
    "        integral1, e = quad(integral, (i - r - 1) / r, (i - r) / r)\n",
    "        integral2, e = quad(integral, -1, 1)\n",
    "        length_i = 2 * (integral1 / integral2)\n",
    "        band = bandStart + length_i\n",
    "        bandList.append(band)\n",
    "        bandStart = band\n",
    "    bandList.append(1.0)\n",
    "    return bandList\n",
    "\n",
    "\n",
    "def quantize(values, bandList):\n",
    "    quantized = \"\"\n",
    "    for i in range(len(values)):\n",
    "        bound = -1\n",
    "        for band in bandList:\n",
    "            if band >= values[i] > bound:\n",
    "                quantized += str(bandList.index(band) + 1)\n",
    "                break\n",
    "            else:\n",
    "                bound = band\n",
    "    return quantized\n",
    "\n",
    "\n",
    "def getWords():\n",
    "    wordList = []\n",
    "    i = 0\n",
    "    while (i + w - 1) < len(quantizedSensor):\n",
    "        word = quantizedSensor[i:i + w]\n",
    "        wordList.append(word)\n",
    "        i += s\n",
    "    return wordList\n",
    "\n",
    "\n",
    "def addToUniqueDict(word_tuple):\n",
    "    inList = False\n",
    "    for word in unique_dict:\n",
    "        if word == word_tuple:\n",
    "            inList = True\n",
    "            break\n",
    "    if not inList:\n",
    "        unique_dict.append(word_tuple)\n",
    "\n",
    "\n",
    "def calcAvgQuanAmp():\n",
    "    avgQuanAmpList = []\n",
    "    normWord = []\n",
    "    i = 0\n",
    "    while (i + w - 1) < len(normSensorVals):\n",
    "        word = normSensorVals[i:i + w]\n",
    "        normWord.append(word)\n",
    "        i += s\n",
    "\n",
    "    for word in normWord:\n",
    "        avgAmp = sum(word) / len(word)\n",
    "        avgQuanAmpList.append(avgAmp)\n",
    "\n",
    "    return avgQuanAmpList\n",
    "\n",
    "\n",
    "def getWordsFromFile(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        words = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            row = line.split(\" - [\")  # list of all words as they occur in the file\n",
    "            if len(row) > 1:\n",
    "                words.append(row[1])\n",
    "        f.close()\n",
    "    return formatWordsFromFile(words)\n",
    "\n",
    "\n",
    "def formatWordsFromFile(tempList):\n",
    "    allWords = []\n",
    "    for sensorWords in tempList:\n",
    "        sensorWords = sensorWords.replace(\"]\", \"\")\n",
    "        sensorWords = sensorWords.replace(\"'\", \"\")\n",
    "        wrd = sensorWords.split(\", \")\n",
    "        allWords.append(wrd)\n",
    "    return allWords\n",
    "\n",
    "\n",
    "def getUniqueWordsInGesture(allWordsInGesture):\n",
    "    uniqueWords = []\n",
    "    for i in range(len(allWordsInGesture)):  # i is each sensor in the gesture\n",
    "        for word in allWordsInGesture[i]:\n",
    "            uniqueWord = (direct, i+1, word)\n",
    "            if uniqueWord not in uniqueWords:\n",
    "                uniqueWords.append(uniqueWord)\n",
    "    return uniqueWords\n",
    "\n",
    "\n",
    "def calcTfValue(wordTuple):\n",
    "    totalWords = 0\n",
    "    sensorIndex = wordTuple[1]-1\n",
    "    num_occurs = 0\n",
    "    component = wordTuple[0]\n",
    "\n",
    "    for file in os.listdir(directory + component):\n",
    "        if file.endswith(\".wrd\"):\n",
    "            file = directory + component + \"/\" + file\n",
    "            gestFileWords = getWordsFromFile(file)\n",
    "            num_occurs += gestFileWords[sensorIndex].count(wordTuple[2])\n",
    "            totalWords += len(gestFileWords[sensorIndex])\n",
    "    value = num_occurs / totalWords\n",
    "    return value\n",
    "\n",
    "\n",
    "def calcIdfValue(wordTuple):\n",
    "    numObjs = 60\n",
    "    numObjsWithWord = 1\n",
    "    sensorIndex = wordTuple[1]-1\n",
    "    component = wordTuple[0]\n",
    "\n",
    "    for file in os.listdir(directory + component):\n",
    "        if file != gestureFile and file.endswith(\".wrd\"):\n",
    "            file = directory + component + \"/\" + file\n",
    "            gestFileWords = getWordsFromFile(file)\n",
    "            if word_tuple[2] in gestFileWords[sensorIndex]:\n",
    "                numObjsWithWord += 1\n",
    "                break\n",
    "\n",
    "    value = math.log(numObjs / numObjsWithWord)\n",
    "    return value\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # GLOBAL VARIABLES:\n",
    "    unique_dict = []  # stores list of all unique words found\n",
    "    gesture_dict = []  # stores list of all words found across all files\n",
    "\n",
    "    # TASK 0\n",
    "    # TASK 0A\n",
    "    directory = input(\"Enter the data directory path: \")\n",
    "    w = input(\"Enter the window length: \")\n",
    "    s = input(\"Enter the shift length: \")\n",
    "    r = input(\"Enter the resolution: \")\n",
    "\n",
    "    w = int(w)\n",
    "    s = int(s)\n",
    "    r = int(r)\n",
    "\n",
    "    # for each data file create a .wrd file containing the following:\n",
    "    for direct in os.listdir(directory):\n",
    "        # for each csv file in X,Y,W,Z:\n",
    "        for filename in os.listdir(directory + direct):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                bands = determineBands()\n",
    "\n",
    "                # generate .wrd file\n",
    "                with open(str(directory) + str(direct) + \"/\" + str(filename) + \".wrd\", \"w\") as wrdFile:\n",
    "\n",
    "                    sensor_id = 1\n",
    "                    csvFile = open(str(directory) + str(direct) + \"/\" + filename, \"r\")\n",
    "                    reader = csv.reader(csvFile, delimiter=',')\n",
    "                    # for each sensor sj in file\n",
    "                    for sensor in reader:\n",
    "                        # output component ID, c in output file\n",
    "                        wrdFile.write(str(direct) + \", \")\n",
    "\n",
    "                        # write sensorID to wrd file\n",
    "                        wrdFile.write(str(sensor_id) + \", \")\n",
    "\n",
    "                        # compute and output average amplitude, avgij of the values\n",
    "                        sensorVals = list(sensor)\n",
    "                        sensorVals = [float(i) for i in sensorVals]\n",
    "                        sensorAvg = calcAvgSensorValue(sensorVals)\n",
    "                        wrdFile.write(str(sensorAvg) + \", \")\n",
    "\n",
    "                        # compute and output standard deviations stdij of the values\n",
    "                        stdDev = calcStdDev(sensorVals, sensorAvg)\n",
    "                        wrdFile.write(str(stdDev) + \", \")\n",
    "\n",
    "                        # normalize entries between -1 and 1\n",
    "                        normSensorVals = normalize(sensorVals)\n",
    "\n",
    "                        # quantizes entries into 2r levels as in phase 1\n",
    "                        quantizedSensor = quantize(normSensorVals, bands)\n",
    "\n",
    "                        # moves a w-length window on time series (by shifting it s units at a time), and at position h\n",
    "                        sensorWords = getWords()\n",
    "\n",
    "                        # computes and outputs in file average quantized amplitude avgQijh for window h of sensor sj\n",
    "                        avgQuanAmp = calcAvgQuanAmp()\n",
    "                        wrdFile.write(str(avgQuanAmp) + \", \" + \" - \")\n",
    "\n",
    "                        # outputs symbolic quantized window descriptor winQijh for the window h of sensor sj\n",
    "                        wrdFile.write(str(sensorWords) + \"\\n\")\n",
    "\n",
    "                        # add dictionary of each window to gestureDict list\n",
    "                        for window in sensorWords:\n",
    "                            wordDict = (direct, sensor_id, window)\n",
    "                            gesture_dict.append(wordDict)\n",
    "                            addToUniqueDict(wordDict)\n",
    "\n",
    "                        sensor_id += 1\n",
    "        # The dictionary of the words consists of <componentName, sensorID, winQ>\n",
    "\n",
    "    # TASK 0B\n",
    "    for direct in os.listdir(directory):\n",
    "        # for each gesture file in W,X,Y,Z:\n",
    "        # create vector .txt files with tf and tf-idf values\n",
    "        for filename in os.listdir(directory + direct):\n",
    "            if filename.endswith(\".wrd\"):\n",
    "                tfFile = open(directory + direct + \"/tf_vectors_\" + filename[:-8] + \".txt\", \"w\")\n",
    "                tfidfFile = open(directory + direct + \"/tfidf_vectors_\" + filename[:-8] + \".txt\", \"w\")\n",
    "                gestureFile = directory + direct + \"/\" + filename\n",
    "\n",
    "                allWordsInGesture = getWordsFromFile(gestureFile)\n",
    "                uniqueWordsInGesture = getUniqueWordsInGesture(allWordsInGesture)\n",
    "\n",
    "                for word_tuple in uniqueWordsInGesture:\n",
    "                    tfValue = calcTfValue(word_tuple)\n",
    "                    idfValue = calcIdfValue(word_tuple)\n",
    "                    tf_idf_value = tfValue * idfValue\n",
    "                    tfFile.write(str(word_tuple) + \" - \" + str(tfValue) + \"\\n\")\n",
    "                    tfidfFile.write(str(word_tuple) + \" - \" + str(tf_idf_value) + \"\\n\")\n",
    "    # End of TASK1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the folder that you want analyzed: X\n",
      "Enter the vector model: tf\n",
      "Enter k: 10\n",
      "Enter the analysis you would like to use: LDA\n",
      "\n",
      "LDA for X gesture files:\n",
      "\n",
      "           0         1         2         3         4         5         6  \\\n",
      "0   0.053208  0.053208  0.053208  0.053208  0.521126  0.053208  0.053208   \n",
      "1   0.053243  0.053243  0.053243  0.053243  0.520811  0.053243  0.053243   \n",
      "2   0.053703  0.053703  0.053703  0.053703  0.516677  0.053703  0.053703   \n",
      "3   0.055065  0.055065  0.055065  0.055065  0.504418  0.055065  0.055065   \n",
      "4   0.052673  0.052673  0.052673  0.052673  0.525941  0.052673  0.052673   \n",
      "5   0.054033  0.054033  0.054033  0.054033  0.513704  0.054033  0.054033   \n",
      "6   0.057577  0.057577  0.057577  0.057577  0.481809  0.057577  0.057577   \n",
      "7   0.052569  0.052569  0.052569  0.052569  0.526881  0.052569  0.052569   \n",
      "8   0.056131  0.056131  0.056131  0.056131  0.494820  0.056131  0.056131   \n",
      "9   0.053604  0.053604  0.053604  0.053604  0.517561  0.053604  0.053604   \n",
      "10  0.054391  0.054391  0.054391  0.054391  0.510485  0.054391  0.054391   \n",
      "11  0.054872  0.054872  0.054872  0.054872  0.506154  0.054872  0.054872   \n",
      "12  0.055074  0.055074  0.055074  0.055074  0.504337  0.055074  0.055074   \n",
      "13  0.053783  0.053783  0.053783  0.053783  0.515956  0.053783  0.053783   \n",
      "14  0.054803  0.054803  0.054803  0.054803  0.506770  0.054803  0.054803   \n",
      "15  0.054511  0.054511  0.054511  0.054511  0.509405  0.054511  0.054511   \n",
      "16  0.054758  0.054758  0.054758  0.054758  0.507176  0.054758  0.054758   \n",
      "17  0.053549  0.053549  0.053549  0.053549  0.518060  0.053549  0.053549   \n",
      "18  0.054726  0.054726  0.054726  0.054726  0.507467  0.054726  0.054726   \n",
      "19  0.054690  0.054690  0.054690  0.054690  0.507791  0.054690  0.054690   \n",
      "20  0.055003  0.055003  0.055003  0.055003  0.504970  0.055003  0.055003   \n",
      "21  0.054996  0.054996  0.054996  0.054996  0.505039  0.054996  0.054996   \n",
      "22  0.053051  0.053051  0.053051  0.053051  0.522544  0.053051  0.053051   \n",
      "23  0.054189  0.054189  0.054189  0.054189  0.512301  0.054189  0.054189   \n",
      "24  0.054295  0.054295  0.054295  0.054295  0.511346  0.054295  0.054295   \n",
      "25  0.083776  0.083776  0.083776  0.083776  0.246015  0.083776  0.083776   \n",
      "26  0.054314  0.054314  0.054314  0.054314  0.511171  0.054314  0.054314   \n",
      "27  0.052711  0.052711  0.052711  0.052711  0.525597  0.052711  0.052711   \n",
      "28  0.052398  0.052398  0.052398  0.052398  0.528414  0.052398  0.052398   \n",
      "29  0.052770  0.052770  0.052770  0.052770  0.525066  0.052770  0.052770   \n",
      "30  0.052518  0.052518  0.052518  0.052518  0.527342  0.052518  0.052518   \n",
      "31  0.053184  0.053184  0.053184  0.053184  0.521342  0.053184  0.053184   \n",
      "32  0.053631  0.053631  0.053631  0.053631  0.517322  0.053631  0.053631   \n",
      "33  0.054059  0.054059  0.054059  0.054059  0.513468  0.054059  0.054059   \n",
      "34  0.053143  0.053143  0.053143  0.053143  0.521714  0.053143  0.053143   \n",
      "35  0.052929  0.052929  0.052929  0.052929  0.523642  0.052929  0.052929   \n",
      "36  0.053618  0.053618  0.053618  0.053618  0.517440  0.053618  0.053618   \n",
      "37  0.052772  0.052772  0.052772  0.052772  0.525051  0.052772  0.052772   \n",
      "38  0.052938  0.052938  0.052938  0.052938  0.523557  0.052938  0.052938   \n",
      "39  0.055679  0.055679  0.055679  0.055679  0.498889  0.055679  0.055679   \n",
      "40  0.058784  0.058784  0.058784  0.058784  0.470943  0.058784  0.058784   \n",
      "41  0.052548  0.052548  0.052548  0.052548  0.527070  0.052548  0.052548   \n",
      "42  0.055666  0.055666  0.055666  0.055666  0.499005  0.055666  0.055666   \n",
      "43  0.053142  0.053142  0.053142  0.053142  0.521720  0.053142  0.053142   \n",
      "44  0.053822  0.053822  0.053822  0.053822  0.515603  0.053822  0.053822   \n",
      "45  0.055585  0.055585  0.055585  0.055585  0.499732  0.055585  0.055585   \n",
      "46  0.053435  0.053435  0.053435  0.053435  0.519084  0.053435  0.053435   \n",
      "47  0.052626  0.052626  0.052626  0.052626  0.526368  0.052626  0.052626   \n",
      "48  0.053730  0.053730  0.053730  0.053730  0.516433  0.053730  0.053730   \n",
      "49  0.054526  0.054526  0.054526  0.054526  0.509270  0.054526  0.054526   \n",
      "50  0.054439  0.054439  0.054439  0.054439  0.510046  0.054439  0.054439   \n",
      "51  0.054409  0.054409  0.054409  0.054409  0.510316  0.054409  0.054409   \n",
      "52  0.054926  0.054926  0.054926  0.054926  0.505666  0.054926  0.054926   \n",
      "53  0.055394  0.055394  0.055394  0.055394  0.501453  0.055394  0.055394   \n",
      "54  0.054410  0.054410  0.054410  0.054410  0.510311  0.054410  0.054410   \n",
      "55  0.054853  0.054853  0.054853  0.054853  0.506319  0.054853  0.054853   \n",
      "56  0.054650  0.054650  0.054650  0.054650  0.508146  0.054650  0.054650   \n",
      "57  0.052866  0.052866  0.052866  0.052866  0.524208  0.052866  0.052866   \n",
      "58  0.053162  0.053162  0.053162  0.053162  0.521543  0.053162  0.053162   \n",
      "59  0.052857  0.052857  0.052857  0.052857  0.524287  0.052857  0.052857   \n",
      "\n",
      "           7         8         9  \n",
      "0   0.053208  0.053208  0.053208  \n",
      "1   0.053243  0.053243  0.053243  \n",
      "2   0.053703  0.053703  0.053703  \n",
      "3   0.055065  0.055065  0.055065  \n",
      "4   0.052673  0.052673  0.052673  \n",
      "5   0.054033  0.054033  0.054033  \n",
      "6   0.057577  0.057577  0.057577  \n",
      "7   0.052569  0.052569  0.052569  \n",
      "8   0.056131  0.056131  0.056131  \n",
      "9   0.053604  0.053604  0.053604  \n",
      "10  0.054391  0.054391  0.054391  \n",
      "11  0.054872  0.054872  0.054872  \n",
      "12  0.055074  0.055074  0.055074  \n",
      "13  0.053783  0.053783  0.053783  \n",
      "14  0.054803  0.054803  0.054803  \n",
      "15  0.054511  0.054511  0.054511  \n",
      "16  0.054758  0.054758  0.054758  \n",
      "17  0.053549  0.053549  0.053549  \n",
      "18  0.054726  0.054726  0.054726  \n",
      "19  0.054690  0.054690  0.054690  \n",
      "20  0.055003  0.055003  0.055003  \n",
      "21  0.054996  0.054996  0.054996  \n",
      "22  0.053051  0.053051  0.053051  \n",
      "23  0.054189  0.054189  0.054189  \n",
      "24  0.054295  0.054295  0.054295  \n",
      "25  0.083776  0.083776  0.083776  \n",
      "26  0.054314  0.054314  0.054314  \n",
      "27  0.052711  0.052711  0.052711  \n",
      "28  0.052398  0.052398  0.052398  \n",
      "29  0.052770  0.052770  0.052770  \n",
      "30  0.052518  0.052518  0.052518  \n",
      "31  0.053184  0.053184  0.053184  \n",
      "32  0.053631  0.053631  0.053631  \n",
      "33  0.054059  0.054059  0.054059  \n",
      "34  0.053143  0.053143  0.053143  \n",
      "35  0.052929  0.052929  0.052929  \n",
      "36  0.053618  0.053618  0.053618  \n",
      "37  0.052772  0.052772  0.052772  \n",
      "38  0.052938  0.052938  0.052938  \n",
      "39  0.055679  0.055679  0.055679  \n",
      "40  0.058784  0.058784  0.058784  \n",
      "41  0.052548  0.052548  0.052548  \n",
      "42  0.055666  0.055666  0.055666  \n",
      "43  0.053142  0.053142  0.053142  \n",
      "44  0.053822  0.053822  0.053822  \n",
      "45  0.055585  0.055585  0.055585  \n",
      "46  0.053435  0.053435  0.053435  \n",
      "47  0.052626  0.052626  0.052626  \n",
      "48  0.053730  0.053730  0.053730  \n",
      "49  0.054526  0.054526  0.054526  \n",
      "50  0.054439  0.054439  0.054439  \n",
      "51  0.054409  0.054409  0.054409  \n",
      "52  0.054926  0.054926  0.054926  \n",
      "53  0.055394  0.055394  0.055394  \n",
      "54  0.054410  0.054410  0.054410  \n",
      "55  0.054853  0.054853  0.054853  \n",
      "56  0.054650  0.054650  0.054650  \n",
      "57  0.052866  0.052866  0.052866  \n",
      "58  0.053162  0.053162  0.053162  \n",
      "59  0.052857  0.052857  0.052857  \n"
     ]
    }
   ],
   "source": [
    "#Task 1\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "#unnecessary lines if you are using anaconda or another \n",
    "print(\"Please enter the following inputs as the same values you used for task 0: \")\n",
    "directory = input(\"Enter the data directory path: \")\n",
    "w = input(\"Enter the window length: \")\n",
    "s = input(\"Enter the shift length: \")\n",
    "r = input(\"Enter the resolution: \")\n",
    "\n",
    "w = int(w)\n",
    "s = int(s)\n",
    "r = int(r)\n",
    "#end of non-anaconda lines\n",
    "\n",
    "#returns the top-k topics \n",
    "def PCAsetup(wordMat, k):\n",
    "    #calculate PCA\n",
    "    pca = PCA(k)\n",
    "    pc = pca.fit_transform(wordMat)\n",
    "    topK = pd.DataFrame(data = pc)\n",
    "    \n",
    "    return topK\n",
    "\n",
    "def SVDsetup(wordMat, k):\n",
    "    #calculate SVD\n",
    "    svd =  TruncatedSVD(k)\n",
    "    sv = svd.fit_transform(wordMat)\n",
    "    topK = pd.DataFrame(data = sv)\n",
    "    return topK\n",
    "\n",
    "def NMFsetup(wordMat, k):\n",
    "    #calculate NMF\n",
    "    nmf = NMF(k)\n",
    "    nm = nmf.fit_transform(wordMat)\n",
    "    topK = pd.DataFrame(data = nm)\n",
    "    return topK\n",
    "\n",
    "def LDAsetup(wordMat, k):\n",
    "    #calculate LDA\n",
    "    lda = LDA(k)\n",
    "    ld = lda.fit_transform(wordMat)\n",
    "    topK = pd.DataFrame(data = ld)\n",
    "    \n",
    "    return topK\n",
    "\n",
    "\n",
    "def makeMat(vectModel, axis):\n",
    "        \n",
    "    #read files\n",
    "    if vectModel == \"tf\":\n",
    "        Xmat = []\n",
    "        for file in glob.glob(directory + axis + \"/tf_vectors_*.txt\"):\n",
    "            #read tf file\n",
    "            f = open(file, \"r\")\n",
    "            tf_vectors = f.readlines()\n",
    "        \n",
    "            gestWords = []\n",
    "            tfVals = []\n",
    "        \n",
    "            #split the line into the word and tf value\n",
    "            for line in tf_vectors:\n",
    "                noDash = line.split(\"-\")\n",
    "                tf_val = noDash[1]\n",
    "                word = noDash[0].split(\",\")\n",
    "                word[2] = word[2].replace(\")\",\"\")\n",
    "                word[2] = word[2].replace(\"'\", \"\")\n",
    "                word[2] = word[2].replace(\" \", \"\")\n",
    "                tf_val = tf_val.replace(\"\\n\", \"\")\n",
    "                gestWords.append(word[2])\n",
    "                tfVals.append(tf_val)\n",
    "           \n",
    "        \n",
    "            index = 0\n",
    "            startI = \"1\"\n",
    "            for y in range(1, w):\n",
    "                startI = startI + \"1\"\n",
    "            startI = int(startI)\n",
    "        \n",
    "            #create dictionary with every word\n",
    "            numWords = startI * (2*r) - startI\n",
    "            wordMat = []\n",
    "    \n",
    "            for i in range(0, numWords + 1):\n",
    "                wordMat.append(0)\n",
    "            \n",
    "            # put tf values into matrix where column = word\n",
    "            for x in gestWords:\n",
    "                xint = int(x)\n",
    "            \n",
    "                #if wordMat[xint - startI] == 0:\n",
    "                wordMat[xint - startI] = wordMat[xint - startI] + float(tfVals[index])\n",
    "                index = index + 1\n",
    "                \n",
    "            \n",
    "            for iterate in range(0, len(wordMat)):\n",
    "                wordMat[iterate] = wordMat[iterate] / 20\n",
    "\n",
    "            \n",
    "            Xmat.append(wordMat)\n",
    "            f.close()\n",
    "           \n",
    "        return Xmat\n",
    "    \n",
    "    elif vectModel == \"tfidf\":\n",
    "        Xmat = []\n",
    "        for file in glob.glob(directory + axis + \"/tfidf_vectors_*.txt\"):\n",
    "            #read tf file\n",
    "            f = open(file, \"r\")\n",
    "            tf_vectors = f.readlines()\n",
    "        \n",
    "            gestWords = []\n",
    "            tfVals = []\n",
    "        \n",
    "            #split the line into the word and tf value\n",
    "            for line in tf_vectors:\n",
    "                noDash = line.split(\"-\")\n",
    "                tf_val = noDash[1]\n",
    "                word = noDash[0].split(\",\")\n",
    "                word[2] = word[2].replace(\")\",\"\")\n",
    "                word[2] = word[2].replace(\"'\", \"\")\n",
    "                word[2] = word[2].replace(\" \", \"\")\n",
    "                tf_val = tf_val.replace(\"\\n\", \"\")\n",
    "                gestWords.append(word[2])\n",
    "                tfVals.append(tf_val)\n",
    "           \n",
    "        \n",
    "            index = 0\n",
    "            startI = \"1\"\n",
    "            for y in range(1, w):\n",
    "                startI = startI + \"1\"\n",
    "            startI = int(startI)\n",
    "        \n",
    "            #create dictionary with every word\n",
    "            numWords = startI * (2*r) - startI\n",
    "            wordMat = []\n",
    "    \n",
    "            for i in range(0, numWords + 1):\n",
    "                wordMat.append(0)\n",
    "            \n",
    "            # put tf values into matrix where column = word\n",
    "            for x in gestWords:\n",
    "                xint = int(x)\n",
    "            \n",
    "                #if wordMat[xint - startI] == 0:\n",
    "                wordMat[xint - startI] = wordMat[xint - startI] + float(tfVals[index])\n",
    "                index = index + 1\n",
    "                \n",
    "            \n",
    "            for iterate in range(0, len(wordMat)):\n",
    "                wordMat[iterate] = wordMat[iterate] / 20\n",
    "\n",
    "            \n",
    "            Xmat.append(wordMat)\n",
    "            f.close()\n",
    "           \n",
    "        return Xmat\n",
    "    \n",
    "def createdictofComponents(topk, k):\n",
    "    \n",
    "    \n",
    "    print(topk[0][1])\n",
    "    for i in range(0, k):\n",
    "        word = \"w\" + str(k)\n",
    "        for j in range(0, len(topk)):\n",
    "            topk[i][j] = (word, topk[j][i])\n",
    "            \n",
    "    print(topk)\n",
    "    \n",
    "def task1(gestfiles, vectModel, useOp, k):\n",
    "    \n",
    "    if useOp == \"PCA\":\n",
    "        #PCA for X axis\n",
    "        wordMat = makeMat(vectModel, gestfiles)\n",
    "        topk = PCAsetup(wordMat, k)\n",
    "        print(\"\\nPCA for \" + gestfiles + \" gesture files:\\n\")\n",
    "        print(topk)\n",
    "        \n",
    "        original_df = pd.DataFrame(topk)\n",
    "        original_df.to_pickle(\"./PCA_\" + gestfiles + \"_\" + vectModel + \".pkl\")\n",
    "        \n",
    "        #dictofComponents = createdictofComponents(topk, k)\n",
    "        \n",
    "    elif useOp == \"SVD\":\n",
    "        wordMat = makeMat(vectModel, gestfiles)\n",
    "        topk = SVDsetup(wordMat, k)\n",
    "        print(\"\\nSVD for \" + gestfiles + \" gesture files:\\n\")\n",
    "        print(topk)\n",
    "        \n",
    "        original_df = pd.DataFrame(topk)\n",
    "        original_df.to_pickle(\"./SVD_\" + gestfiles + \"_\" + vectModel + \".pkl\")\n",
    "        \n",
    "    elif useOp == \"NMF\":\n",
    "        wordMat = makeMat(vectModel, gestfiles)\n",
    "        topk = NMFsetup(wordMat, k)\n",
    "        print(\"\\nNMF for \" + gestfiles + \" gesture files:\\n\")\n",
    "        print(topk)\n",
    "        \n",
    "        original_df = pd.DataFrame(topk)\n",
    "        original_df.to_pickle(\"./NMF_\" + gestfiles + \"_\" + vectModel + \".pkl\")\n",
    "        \n",
    "    elif useOp == \"LDA\":\n",
    "        wordMat = makeMat(vectModel, gestfiles)\n",
    "        topk = LDAsetup(wordMat, k)\n",
    "        print(\"\\nLDA for \" + gestfiles + \" gesture files:\\n\")\n",
    "        print(topk)\n",
    "        \n",
    "        original_df = pd.DataFrame(topk)\n",
    "        original_df.to_pickle(\"./LDA_\" + gestfiles + \"_\" + vectModel + \".pkl\")\n",
    "\n",
    "\n",
    "gestfiles = input(\"Enter the folder that you want analyzed: \")\n",
    "vectModel = input(\"Enter the vector model: \")\n",
    "k = input(\"Enter k: \")\n",
    "useOp = input(\"Enter the analysis you would like to use: \")\n",
    "k = int(k)\n",
    "task1(gestfiles, vectModel, useOp, k)\n",
    "\n",
    "\n",
    "#sample output: \n",
    "#Enter the folder that you want analyzed: X\n",
    "#Enter the vector model: tf\n",
    "#Enter k: 10\n",
    "#Enter the analysis you would like to use: PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
