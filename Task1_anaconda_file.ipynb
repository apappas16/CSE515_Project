{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the data directory path: small_data/\n",
      "Enter the window length: 2\n",
      "Enter the shift length: 2\n",
      "Enter the resolution: 2\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "import os\n",
    "from scipy.integrate import quad\n",
    "\n",
    "\n",
    "def calcAvgSensorValue(sensorValues):\n",
    "    avg = sum(sensorValues) / len(sensorValues)\n",
    "    avg = round(avg, 9)\n",
    "    return avg\n",
    "\n",
    "\n",
    "def calcStdDev(sensorValues, meanVal):\n",
    "    sd = []\n",
    "    for num in sensorValues:\n",
    "        result = num - meanVal\n",
    "        result = result * result\n",
    "        sd.append(result)\n",
    "    return calcAvgSensorValue(sd)\n",
    "\n",
    "\n",
    "def normalize(sensorValues):\n",
    "    normalized_sensor = []\n",
    "    for val in sensorValues:\n",
    "        val = float(val)\n",
    "        normalized = 2 * ((val - min(sensorValues)) / (max(sensorValues) - min(sensorValues))) - 1\n",
    "        normalized_sensor.append(normalized)\n",
    "    return normalized_sensor\n",
    "\n",
    "\n",
    "def integral(i):\n",
    "    return getGaussianVal(i, 0, 0.25)\n",
    "\n",
    "\n",
    "def getGaussianVal(i, avg, sd):\n",
    "    i = float(i - avg) / sd\n",
    "    gauss = math.exp(-i * i / 2.0) / math.sqrt(2.0 * math.pi) / sd\n",
    "    return gauss\n",
    "\n",
    "\n",
    "def determineBands():\n",
    "    numBands = r * 2\n",
    "    bandList = []\n",
    "    bandStart = -1\n",
    "    for i in range(1, numBands):\n",
    "        integral1, e = quad(integral, (i - r - 1) / r, (i - r) / r)\n",
    "        integral2, e = quad(integral, -1, 1)\n",
    "        length_i = 2 * (integral1 / integral2)\n",
    "        band = bandStart + length_i\n",
    "        bandList.append(band)\n",
    "        bandStart = band\n",
    "    bandList.append(1.0)\n",
    "    return bandList\n",
    "\n",
    "\n",
    "def quantize(values, bandList):\n",
    "    quantized = \"\"\n",
    "    for i in range(len(values)):\n",
    "        bound = -1\n",
    "        for band in bandList:\n",
    "            if band >= values[i] > bound:\n",
    "                quantized += str(bandList.index(band) + 1)\n",
    "                break\n",
    "            else:\n",
    "                bound = band\n",
    "    return quantized\n",
    "\n",
    "\n",
    "def getWords():\n",
    "    wordList = []\n",
    "    i = 0\n",
    "    while (i + w - 1) < len(quantizedSensor):\n",
    "        word = quantizedSensor[i:i + w]\n",
    "        wordList.append(word)\n",
    "        i += s\n",
    "    return wordList\n",
    "\n",
    "\n",
    "def addToUniqueDict(word_tuple):\n",
    "    inList = False\n",
    "    for word in unique_dict:\n",
    "        if word == word_tuple:\n",
    "            inList = True\n",
    "            break\n",
    "    if not inList:\n",
    "        unique_dict.append(word_tuple)\n",
    "\n",
    "\n",
    "def calcAvgQuanAmp():\n",
    "    avgQuanAmpList = []\n",
    "    normWord = []\n",
    "    i = 0\n",
    "    while (i + w - 1) < len(normSensorVals):\n",
    "        word = normSensorVals[i:i + w]\n",
    "        normWord.append(word)\n",
    "        i += s\n",
    "\n",
    "    for word in normWord:\n",
    "        avgAmp = sum(word) / len(word)\n",
    "        avgQuanAmpList.append(avgAmp)\n",
    "\n",
    "    return avgQuanAmpList\n",
    "\n",
    "\n",
    "def getWordsFromFile(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        words = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            row = line.split(\" - [\")  # list of all words as they occur in the file\n",
    "            if len(row) > 1:\n",
    "                words.append(row[1])\n",
    "        f.close()\n",
    "    return formatWordsFromFile(words)\n",
    "\n",
    "\n",
    "def formatWordsFromFile(tempList):\n",
    "    allWords = []\n",
    "    for sensorWords in tempList:\n",
    "        sensorWords = sensorWords.replace(\"]\", \"\")\n",
    "        sensorWords = sensorWords.replace(\"'\", \"\")\n",
    "        wrd = sensorWords.split(\", \")\n",
    "        allWords.append(wrd)\n",
    "    return allWords\n",
    "\n",
    "\n",
    "def getUniqueWordsInGesture(allWordsInGesture):\n",
    "    uniqueWords = []\n",
    "    for i in range(len(allWordsInGesture)):  # i is each sensor in the gesture\n",
    "        for word in allWordsInGesture[i]:\n",
    "            uniqueWord = (direct, i+1, word)\n",
    "            if uniqueWord not in uniqueWords:\n",
    "                uniqueWords.append(uniqueWord)\n",
    "    return uniqueWords\n",
    "\n",
    "\n",
    "def calcTfValue(wordTuple):\n",
    "    totalWords = 0\n",
    "    sensorIndex = wordTuple[1]-1\n",
    "    num_occurs = 0\n",
    "    component = wordTuple[0]\n",
    "\n",
    "    for file in os.listdir(directory + component):\n",
    "        if file.endswith(\".wrd\"):\n",
    "            file = directory + component + \"/\" + file\n",
    "            gestFileWords = getWordsFromFile(file)\n",
    "            num_occurs += gestFileWords[sensorIndex].count(wordTuple[2])\n",
    "            totalWords += len(gestFileWords[sensorIndex])\n",
    "    value = num_occurs / totalWords\n",
    "    return value\n",
    "\n",
    "\n",
    "def calcIdfValue(wordTuple):\n",
    "    numObjs = 60\n",
    "    numObjsWithWord = 0\n",
    "    sensorIndex = wordTuple[1]-1\n",
    "    component = wordTuple[0]\n",
    "\n",
    "    for file in os.listdir(directory + component):\n",
    "        if file.endswith(\".wrd\"):\n",
    "            file = directory + component + \"/\" + file\n",
    "            gestFileWords = getWordsFromFile(file)\n",
    "            if word_tuple[2] in gestFileWords[sensorIndex]:\n",
    "                numObjsWithWord += 1\n",
    "\n",
    "    value = math.log(numObjs / numObjsWithWord)\n",
    "    return value\n",
    "\n",
    "\n",
    "def writeValsToFile():\n",
    "    allWordsInGesture = getWordsFromFile(fullFileName)\n",
    "    uniqueWordsInGesture = getUniqueWordsInGesture(allWordsInGesture)\n",
    "    sensorId = 1\n",
    "    #unique_dict is what I want\n",
    "    for sensor in uniqueWordsInGesture:\n",
    "        word_tuple = (sensor[0], sensor[1], sensor[2])\n",
    "        for n, i in enumerate(unique_dict):\n",
    "            if i == word_tuple:\n",
    "                tfVal = unique_tf_dict[n]\n",
    "                tfidfVal = unique_tfidf_dict[n]\n",
    "                tfFile.write(str(i) + \"-\")\n",
    "                tfidfFile.write(str(i) + \"-\")\n",
    "                tfFile.write(str(tfVal))\n",
    "                tfidfFile.write(str(tfidfVal))\n",
    "                tfFile.write(\"\\n\")\n",
    "                tfidfFile.write(\"\\n\")\n",
    "        \n",
    "        #sensorId += 1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # GLOBAL VARIABLES:\n",
    "    unique_dict = []  # stores list of all unique word tuples found in entire DB in order\n",
    "    unique_tf_dict = []\n",
    "    unique_tfidf_dict = []\n",
    "    all_gesture_dict = []  # stores list of all words found in DB\n",
    "\n",
    "    # TASK 0\n",
    "    # TASK 0A\n",
    "    directory = input(\"Enter the data directory path: \")\n",
    "    w = input(\"Enter the window length: \")\n",
    "    s = input(\"Enter the shift length: \")\n",
    "    r = input(\"Enter the resolution: \")\n",
    "\n",
    "    w = int(w)\n",
    "    s = int(s)\n",
    "    r = int(r)\n",
    "\n",
    "    # for each data file create a .wrd file containing the following:\n",
    "    for direct in os.listdir(directory):\n",
    "        # for each csv file in X,Y,W,Z:\n",
    "        for filename in os.listdir(directory + direct):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                bands = determineBands()\n",
    "\n",
    "                # generate .wrd file\n",
    "                with open(str(directory) + str(direct) + \"/\" + str(filename) + \".wrd\", \"w\") as wrdFile:\n",
    "\n",
    "                    sensor_id = 1\n",
    "                    csvFile = open(str(directory) + str(direct) + \"/\" + filename, \"r\")\n",
    "                    reader = csv.reader(csvFile, delimiter=',')\n",
    "                    # for each sensor sj in file\n",
    "                    for sensor in reader:\n",
    "                        # output component ID, c in output file\n",
    "                        wrdFile.write(str(direct) + \", \")\n",
    "\n",
    "                        # write sensorID to wrd file\n",
    "                        wrdFile.write(str(sensor_id) + \", \")\n",
    "\n",
    "                        # compute and output average amplitude, avgij of the values\n",
    "                        sensorVals = list(sensor)\n",
    "                        sensorVals = [float(i) for i in sensorVals]\n",
    "                        sensorAvg = calcAvgSensorValue(sensorVals)\n",
    "                        wrdFile.write(str(sensorAvg) + \", \")\n",
    "\n",
    "                        # compute and output standard deviations stdij of the values\n",
    "                        stdDev = calcStdDev(sensorVals, sensorAvg)\n",
    "                        wrdFile.write(str(stdDev) + \", \")\n",
    "\n",
    "                        # normalize entries between -1 and 1\n",
    "                        normSensorVals = normalize(sensorVals)\n",
    "\n",
    "                        # quantizes entries into 2r levels as in phase 1\n",
    "                        quantizedSensor = quantize(normSensorVals, bands)\n",
    "\n",
    "                        # moves a w-length window on time series (by shifting it s units at a time), and at position h\n",
    "                        sensorWords = getWords()\n",
    "\n",
    "                        # computes and outputs in file average quantized amplitude avgQijh for window h of sensor sj\n",
    "                        avgQuanAmp = calcAvgQuanAmp()\n",
    "                        wrdFile.write(str(avgQuanAmp) + \", \" + \" - \")\n",
    "\n",
    "                        # outputs symbolic quantized window descriptor winQijh for the window h of sensor sj\n",
    "                        wrdFile.write(str(sensorWords) + \"\\n\")\n",
    "\n",
    "                        # add dictionary of each window to gestureDict list\n",
    "                        for window in sensorWords:\n",
    "                            wordDict = (direct, sensor_id, window)\n",
    "                            all_gesture_dict.append(wordDict)\n",
    "                            if wordDict not in unique_dict:\n",
    "                                unique_dict.append(wordDict)\n",
    "\n",
    "                        sensor_id += 1\n",
    "        # The dictionary of the words consists of <componentName, sensorID, winQ>\n",
    "\n",
    "    # TASK 0B\n",
    "    for word_tuple in unique_dict:\n",
    "        tfValue = calcTfValue(word_tuple)\n",
    "        idfValue = calcIdfValue(word_tuple)\n",
    "        tf_idf_value = tfValue * idfValue\n",
    "        unique_tf_dict.append(tfValue)\n",
    "        unique_tfidf_dict.append(tf_idf_value)\n",
    "\n",
    "    for direct in os.listdir(directory):\n",
    "        for wrdFile in os.listdir(directory + direct):\n",
    "            if wrdFile.endswith(\".wrd\"):\n",
    "                fullFileName = directory + direct + \"/\" + wrdFile\n",
    "                tfFile = open(directory + direct + \"/tf_vectors_\" + wrdFile[:-8] + \".txt\", \"w\")\n",
    "                tfidfFile = open(directory + direct + \"/tfidf_vectors_\" + wrdFile[:-8] + \".txt\", \"w\")\n",
    "                writeValsToFile()\n",
    "                tfFile.close()\n",
    "                tfidfFile.close()\n",
    "    # End of TASK0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the following inputs as the same values you used for task 0: \n",
      "Enter the data directory path (ex: Data/: small_data/\n",
      "Enter the window length (ex: 3): 2\n",
      "Enter the shift length (ex: 3): 2\n",
      "Enter the resolution (ex: 3): 2\n",
      "Enter the vector model (ex: tf): tf\n",
      "Enter k (ex: 4): 5\n",
      "Enter the analysis you would like to use (ex: PCA): PCA\n",
      "(10, 680)\n",
      "(10, 680)\n",
      "(10, 680)\n",
      "(10, 680)\n",
      "[[0.04280156 0.01945525 0.         ... 0.         0.         0.04669261]\n",
      " [0.04280156 0.         0.         ... 0.         0.0155642  0.04669261]\n",
      " [0.         0.         0.         ... 0.         0.         0.04669261]\n",
      " ...\n",
      " [0.04280156 0.01945525 0.         ... 0.         0.         0.04669261]\n",
      " [0.04280156 0.         0.         ... 0.         0.         0.04669261]\n",
      " [0.04280156 0.01945525 0.         ... 0.         0.0155642  0.        ]]\n",
      "       0         1             2             3             4             5     \\\n",
      "0 -0.008160 -0.005575  2.107966e-20 -8.632631e-19  7.742087e-19  9.241019e-19   \n",
      "1  0.024239  0.021013 -1.153227e-19  3.545164e-19 -9.440784e-19 -5.292412e-19   \n",
      "2 -0.023074 -0.013948  1.366219e-17  3.601051e-18  6.338009e-18  2.060454e-18   \n",
      "3 -0.003972 -0.000102  1.171200e-17 -2.338427e-17  8.096518e-17  3.492356e-17   \n",
      "4  0.054459  0.003364 -3.319513e-17  8.171369e-17 -2.559863e-16 -1.115503e-16   \n",
      "\n",
      "           6             7             8     9       ...     2710  2711  2712  \\\n",
      "0 -2.534934e-20 -8.586103e-20 -2.404063e-19  -0.0    ...     -0.0  -0.0  -0.0   \n",
      "1  1.422307e-20  3.999832e-20  1.127386e-19   0.0    ...      0.0   0.0   0.0   \n",
      "2 -3.505664e-20 -7.402341e-20 -2.132774e-19  -0.0    ...     -0.0  -0.0  -0.0   \n",
      "3 -1.462531e-20  6.088683e-20  1.548792e-19   0.0    ...      0.0   0.0   0.0   \n",
      "4 -1.578479e-19 -2.676361e-19 -7.433331e-19   0.0    ...      0.0   0.0   0.0   \n",
      "\n",
      "   2713  2714  2715  2716  2717      2718      2719  \n",
      "0  -0.0  -0.0  -0.0  -0.0  -0.0 -0.004468 -0.015204  \n",
      "1   0.0   0.0   0.0   0.0   0.0  0.000646 -0.008457  \n",
      "2  -0.0  -0.0  -0.0  -0.0  -0.0 -0.019795  0.049660  \n",
      "3   0.0   0.0   0.0   0.0   0.0 -0.017139  0.000004  \n",
      "4   0.0   0.0   0.0   0.0   0.0 -0.005733 -0.065184  \n",
      "\n",
      "[5 rows x 2720 columns]\n",
      "\n",
      "PCA:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexs\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:288: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\alexs\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             0                                  1     \\\n",
      "0  (-0.008159779243325753, W0-11)    (-0.005575091547708888, W16-11)   \n",
      "1   (0.024239485499994676, W0-11)      (0.02101327211977433, W16-11)   \n",
      "2  (-0.023073664248697372, W0-11)    (-0.013948194920579233, W16-11)   \n",
      "3   (-0.00397232778867483, W0-11)  (-0.00010238594425239617, W16-11)   \n",
      "4    (0.05445858323249671, W0-11)    (0.0033635053884826832, W16-11)   \n",
      "\n",
      "                                2                                 3     \\\n",
      "0    (2.107965769379996e-20, X12-11)   (-8.632631388126141e-19, Y8-11)   \n",
      "1    (-1.15322661303602e-19, X12-11)     (3.54516408268936e-19, Y8-11)   \n",
      "2   (1.3662186612993436e-17, X12-11)    (3.601051374861382e-18, Y8-11)   \n",
      "3   (1.1712002379929263e-17, X12-11)  (-2.3384265342832222e-17, Y8-11)   \n",
      "4  (-3.3195128001306756e-17, X12-11)    (8.171368583497191e-17, Y8-11)   \n",
      "\n",
      "                              4                                 5     \\\n",
      "0   (7.742087150449491e-19, Z4-11)    (9.241018727760257e-19, Z0-11)   \n",
      "1  (-9.440783843874009e-19, Z4-11)   (-5.292412270268246e-19, Z0-11)   \n",
      "2    (6.33800895691618e-18, Z4-11)    (2.060454321014247e-18, Z0-11)   \n",
      "3   (8.096517942678263e-17, Z4-11)    (3.492356002988881e-17, Z0-11)   \n",
      "4  (-2.559863164317336e-16, Z4-11)  (-1.1155034632661944e-16, Z0-11)   \n",
      "\n",
      "                                6                                  7     \\\n",
      "0  (-2.5349342391259786e-20, Z16-11)   (-8.586102836935093e-20, Z12-11)   \n",
      "1   (1.4223070033635308e-20, Z16-11)   (3.9998316166959736e-20, Z12-11)   \n",
      "2   (-3.505664118037266e-20, Z16-11)   (-7.402341188201634e-20, Z12-11)   \n",
      "3   (-1.462530836483538e-20, Z16-11)    (6.088683291994947e-20, Z12-11)   \n",
      "4  (-1.5784794733266548e-19, Z16-11)  (-2.6763613066006295e-19, Z12-11)   \n",
      "\n",
      "                               8              9     \\\n",
      "0  (-2.4040625058935885e-19, Z8-11)  (-0.0, Z4-11)   \n",
      "1    (1.127385639720172e-19, Z8-11)   (0.0, Z4-11)   \n",
      "2  (-2.1327743377985108e-19, Z8-11)  (-0.0, Z4-11)   \n",
      "3   (1.5487922181318744e-19, Z8-11)   (0.0, Z4-11)   \n",
      "4   (-7.433331300655944e-19, Z8-11)   (0.0, Z4-11)   \n",
      "\n",
      "                ...                         2710            2711  \\\n",
      "0               ...                (-0.0, Z0-11)  (-0.0, Z16-11)   \n",
      "1               ...                 (0.0, Z0-11)   (0.0, Z16-11)   \n",
      "2               ...                (-0.0, Z0-11)  (-0.0, Z16-11)   \n",
      "3               ...                 (0.0, Z0-11)   (0.0, Z16-11)   \n",
      "4               ...                 (0.0, Z0-11)   (0.0, Z16-11)   \n",
      "\n",
      "             2712           2713           2714           2715  \\\n",
      "0  (-0.0, Z12-11)  (-0.0, Z8-11)  (-0.0, Z4-11)  (-0.0, Z0-11)   \n",
      "1   (0.0, Z12-11)   (0.0, Z8-11)   (0.0, Z4-11)   (0.0, Z0-11)   \n",
      "2  (-0.0, Z12-11)  (-0.0, Z8-11)  (-0.0, Z4-11)  (-0.0, Z0-11)   \n",
      "3   (0.0, Z12-11)   (0.0, Z8-11)   (0.0, Z4-11)   (0.0, Z0-11)   \n",
      "4   (0.0, Z12-11)   (0.0, Z8-11)   (0.0, Z4-11)   (0.0, Z0-11)   \n",
      "\n",
      "             2716            2717                            2718  \\\n",
      "0  (-0.0, Z16-11)  (-0.0, Z12-11)  (-0.004468423408197867, Z8-11)   \n",
      "1   (0.0, Z16-11)   (0.0, Z12-11)  (0.0006459537166951181, Z8-11)   \n",
      "2  (-0.0, Z16-11)  (-0.0, Z12-11)   (-0.01979516359599265, Z8-11)   \n",
      "3   (0.0, Z16-11)   (0.0, Z12-11)  (-0.017138889448367264, Z8-11)   \n",
      "4   (0.0, Z16-11)   (0.0, Z12-11)  (-0.005732979828922537, Z8-11)   \n",
      "\n",
      "                             2719  \n",
      "0  (-0.015203880613854161, Z4-11)  \n",
      "1  (-0.008457145049114944, Z4-11)  \n",
      "2     (0.0496601849811295, Z4-11)  \n",
      "3  (4.043307369221422e-06, Z4-11)  \n",
      "4   (-0.06518371536880913, Z4-11)  \n",
      "\n",
      "[5 rows x 2720 columns]\n",
      "                            0                              1     \\\n",
      "0   (0.08865631016580527, Z4-11)   (0.08865631016580527, Z4-11)   \n",
      "1  (0.21761156646555174, Z16-11)  (0.21761156646555174, Z16-11)   \n",
      "2   (0.07149529745714472, Z8-11)   (0.07149529745714472, Z8-11)   \n",
      "3  (0.24547678033977519, Z16-11)  (0.24547678033977519, Z16-11)   \n",
      "4   (0.30299558910113084, Z8-11)   (0.30299558910113084, Z8-11)   \n",
      "\n",
      "                            2                              3     \\\n",
      "0   (0.08865631016580527, Z4-11)   (0.08865631016580527, Z4-11)   \n",
      "1  (0.21761156646555174, Z16-11)  (0.21761156646555174, Z16-11)   \n",
      "2   (0.07149529745714472, Z8-11)   (0.07149529745714472, Z8-11)   \n",
      "3  (0.24547678033977519, Z16-11)  (0.24547678033977519, Z16-11)   \n",
      "4   (0.30299558910113084, Z8-11)   (0.30299558910113084, Z8-11)   \n",
      "\n",
      "                            4                              5     \\\n",
      "0   (0.0869649236815024, Z16-11)   (0.0869649236815024, Z16-11)   \n",
      "1    (0.2009202513346146, Z4-11)    (0.2009202513346146, Z4-11)   \n",
      "2  (0.07023861818654442, Z16-11)  (0.07023861818654442, Z16-11)   \n",
      "3   (0.11506724078426961, Z8-11)   (0.11506724078426961, Z8-11)   \n",
      "4  (0.09378646907113515, Z16-11)  (0.09378646907113515, Z16-11)   \n",
      "\n",
      "                            6                              7     \\\n",
      "0   (0.0869649236815024, Z16-11)   (0.0869649236815024, Z16-11)   \n",
      "1    (0.2009202513346146, Z4-11)    (0.2009202513346146, Z4-11)   \n",
      "2  (0.07023861818654442, Z16-11)  (0.07023861818654442, Z16-11)   \n",
      "3   (0.11506724078426961, Z8-11)   (0.11506724078426961, Z8-11)   \n",
      "4  (0.09378646907113515, Z16-11)  (0.09378646907113515, Z16-11)   \n",
      "\n",
      "                             8                               9     \\\n",
      "0   (0.050632298405358246, Z4-11)   (0.050632298405358246, Z4-11)   \n",
      "1     (0.1020054217807274, Z8-11)     (0.1020054217807274, Z8-11)   \n",
      "2  (0.062167225977130924, Z16-11)  (0.062167225977130924, Z16-11)   \n",
      "3   (0.09537119870605966, Z12-11)   (0.09537119870605966, Z12-11)   \n",
      "4    (0.09140865589924618, Z4-11)    (0.09140865589924618, Z4-11)   \n",
      "\n",
      "                ...                                          2710  \\\n",
      "0               ...                 (-0.13057994927641065, Z8-11)   \n",
      "1               ...                (-0.053225721156125574, Z0-11)   \n",
      "2               ...                 (-0.16259004910071917, Z4-11)   \n",
      "3               ...                 (-0.0726377760229661, Z16-11)   \n",
      "4               ...                 (-0.06518371536880913, Z4-11)   \n",
      "\n",
      "                             2711                            2712  \\\n",
      "0   (-0.13057994927641065, Z8-11)    (-0.222662830639328, Z16-11)   \n",
      "1  (-0.053225721156125574, Z0-11)  (-0.08191858102637714, Z16-11)   \n",
      "2   (-0.16259004910071917, Z4-11)   (-0.1709634552798893, Z16-11)   \n",
      "3   (-0.0726377760229661, Z16-11)  (-0.08423723536802896, Z16-11)   \n",
      "4   (-0.06518371536880913, Z4-11)  (-0.07454650546847202, Z16-11)   \n",
      "\n",
      "                             2713                            2714  \\\n",
      "0    (-0.222662830639328, Z16-11)    (-0.222662830639328, Z16-11)   \n",
      "1  (-0.08191858102637714, Z16-11)  (-0.08191858102637714, Z16-11)   \n",
      "2   (-0.1709634552798893, Z16-11)   (-0.1709634552798893, Z16-11)   \n",
      "3  (-0.08423723536802896, Z16-11)  (-0.08423723536802896, Z16-11)   \n",
      "4  (-0.07454650546847202, Z16-11)  (-0.07454650546847202, Z16-11)   \n",
      "\n",
      "                             2715                            2716  \\\n",
      "0    (-0.222662830639328, Z16-11)   (-0.38604811867479744, Z0-11)   \n",
      "1  (-0.08191858102637714, Z16-11)   (-0.32038590526436106, Z4-11)   \n",
      "2   (-0.1709634552798893, Z16-11)   (-0.28563776709016586, Z4-11)   \n",
      "3  (-0.08423723536802896, Z16-11)    (-0.3155852679928762, Z4-11)   \n",
      "4  (-0.07454650546847202, Z16-11)  (-0.17124621331945208, Z16-11)   \n",
      "\n",
      "                             2717                            2718  \\\n",
      "0   (-0.38604811867479744, Z0-11)   (-0.38604811867479744, Z0-11)   \n",
      "1   (-0.32038590526436106, Z4-11)   (-0.32038590526436106, Z4-11)   \n",
      "2   (-0.28563776709016586, Z4-11)   (-0.28563776709016586, Z4-11)   \n",
      "3    (-0.3155852679928762, Z4-11)    (-0.3155852679928762, Z4-11)   \n",
      "4  (-0.17124621331945208, Z16-11)  (-0.17124621331945208, Z16-11)   \n",
      "\n",
      "                             2719  \n",
      "0   (-0.38604811867479744, Z0-11)  \n",
      "1   (-0.32038590526436106, Z4-11)  \n",
      "2   (-0.28563776709016586, Z4-11)  \n",
      "3    (-0.3155852679928762, Z4-11)  \n",
      "4  (-0.17124621331945208, Z16-11)  \n",
      "\n",
      "[5 rows x 2720 columns]\n",
      "pickle\n",
      "          0         1         2         3         4\n",
      "0 -0.262115  0.688352  0.402047 -0.010317  0.042310\n",
      "1 -0.302401 -0.246725  0.068745 -0.436814 -0.008424\n",
      "2  0.948265 -0.098317  0.180710  0.021032 -0.308473\n",
      "3 -0.152333  0.111252 -0.193523 -0.032305 -0.148232\n",
      "4  0.051021  0.088854 -0.324764  0.073994  0.155015\n",
      "5 -0.408075 -0.491112  0.311013  0.047890  0.093805\n",
      "6  0.791815 -0.017583 -0.009565 -0.034686  0.346575\n",
      "7 -0.191528  0.053105 -0.219877 -0.026411 -0.044598\n",
      "8 -0.294271 -0.169488  0.039201  0.436959 -0.024561\n",
      "9 -0.180377  0.081662 -0.253986 -0.039342 -0.103418\n"
     ]
    }
   ],
   "source": [
    "#Task 1\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "#unnecessary lines if you are using anaconda or another \n",
    "print(\"Please enter the following inputs as the same values you used for task 0: \")\n",
    "directory = input(\"Enter the data directory path (ex: Data/: \")\n",
    "w = input(\"Enter the window length (ex: 3): \")\n",
    "s = input(\"Enter the shift length (ex: 3): \")\n",
    "r = input(\"Enter the resolution (ex: 3): \")\n",
    "\n",
    "w = int(w)\n",
    "s = int(s)\n",
    "r = int(r)\n",
    "#end of non-anaconda lines\n",
    "\n",
    "#returns the top-k topics \n",
    "def PCAsetup(wordMat, k):\n",
    "    #calculate PCA\n",
    "    pca = PCA(k)\n",
    "    pc = pca.fit_transform(wordMat)\n",
    "    UT = pca.components_\n",
    "    topK = pd.DataFrame(data = UT)\n",
    "    \n",
    "    original_df = pd.DataFrame(data = pc)\n",
    "    original_df.to_pickle(\"./PCA_\" + vectModel + \".pkl\")\n",
    "    \n",
    "    print(topK)\n",
    "    return topK\n",
    "\n",
    "def SVDsetup(wordMat, k):\n",
    "    #calculate SVD\n",
    "    svd = TruncatedSVD(k)\n",
    "    sv = svd.fit_transform(wordMat)\n",
    "    VT = svd.components_\n",
    "    topK = pd.DataFrame(data = VT)\n",
    "    \n",
    "    original_df = pd.DataFrame(data = sv)\n",
    "    original_df.to_pickle(\"./SVD_\" + vectModel + \".pkl\")\n",
    "    \n",
    "    print(topK)\n",
    "    return topK\n",
    "\n",
    "def NMFsetup(wordMat, k):\n",
    "    #calculate NMF\n",
    "    nmf = NMF(k)\n",
    "    nm = nmf.fit_transform(wordMat)\n",
    "    R = nmf.components_\n",
    "    topK = pd.DataFrame(data = R)\n",
    "    \n",
    "    original_df = pd.DataFrame(data = nm)\n",
    "    original_df.to_pickle(\"./NMF_\" + vectModel + \".pkl\")\n",
    "    \n",
    "    print(topK)\n",
    "    return topK\n",
    "\n",
    "def LDAsetup(wordMat, k):\n",
    "    #calculate LDA\n",
    "    lda = LDA(k)\n",
    "    ld = lda.fit_transform(wordMat)\n",
    "    V = lda.components_\n",
    "    topK = pd.DataFrame(data = V)\n",
    "    \n",
    "    original_df = pd.DataFrame(data = ld)\n",
    "    original_df.to_pickle(\"./LDA_\" + vectModel + \".pkl\")\n",
    "    \n",
    "    print(topK)\n",
    "    return topK\n",
    "\n",
    "\n",
    "def makeMat(vectModel):    \n",
    "    #read files\n",
    "    Wmat = []\n",
    "    Xmat = []\n",
    "    Ymat = []\n",
    "    Zmat = []\n",
    "    if vectModel == \"tf\":\n",
    "        for axisNum in range(1, 5):\n",
    "            if axisNum == 1:\n",
    "                axis = 'W'\n",
    "            elif axisNum == 2:\n",
    "                axis = 'X'\n",
    "            elif axisNum == 3:\n",
    "                axis = 'Y'\n",
    "            elif axisNum == 4:\n",
    "                axis = 'Z'\n",
    "                \n",
    "            for file in glob.glob(directory + axis + \"/tf_vectors_*.txt\"):\n",
    "                #Xmat = []\n",
    "                #read tf file\n",
    "                f = open(file, \"r\")\n",
    "                tf_vectors = f.readlines()\n",
    "        \n",
    "                gestWords = []\n",
    "                tfVals = []\n",
    "        \n",
    "                #split the line into the word and tf value\n",
    "                for line in tf_vectors:\n",
    "                    noDash = line.split(\"-\")\n",
    "                    tf_val = noDash[1]\n",
    "                    tf_val = tf_val.replace(\"\\n\", \"\")\n",
    "                    gestWords.append(noDash[0])\n",
    "                    tfVals.append(tf_val)\n",
    "           \n",
    "        \n",
    "                index = 0\n",
    "                startI = \"1\"\n",
    "                for y in range(1, w):\n",
    "                    startI = startI + \"1\"\n",
    "                startI = int(startI)\n",
    "        \n",
    "                #create dictionary with every word for every sensor and every directory\n",
    "                numWords = (startI * (2*r) - startI) * 20\n",
    "                wordMat = []\n",
    "    \n",
    "                for i in range(0, numWords + 20):\n",
    "                    wordMat.append(0)\n",
    "                \n",
    "                # put tf values into matrix where column = word\n",
    "                for x in gestWords:\n",
    "                    word = x.split(\", \")\n",
    "                    sensorNum = word[1].replace(\"'\", \"\")\n",
    "                    wordID = word[2].replace(\"'\", \"\")\n",
    "                    wordID = wordID.replace(\")\", \"\")\n",
    "                \n",
    "                    wordID = int(wordID)\n",
    "                    sensorNum = int(sensorNum)\n",
    "                \n",
    "                    #axisSplit = len(wordMat) / 4\n",
    "                    sensorSplit = len(wordMat) / 20\n",
    "                    \n",
    "                    wordIndex = int(wordID + ((sensorNum - 1) * sensorSplit)) #+ ((axisNum - 1) * axisSplit)))\n",
    "                    wordIndex = wordIndex - startI\n",
    "            \n",
    "                    wordMat[wordIndex] = float(tfVals[index])\n",
    "                    index = index + 1\n",
    "                \n",
    "\n",
    "                if axisNum == 1:\n",
    "                    axis = 'W'\n",
    "                    Wmat.append(wordMat)\n",
    "                elif axisNum == 2:\n",
    "                    axis = 'X'\n",
    "                    Xmat.append(wordMat)\n",
    "                elif axisNum == 3:\n",
    "                    axis = 'Y'\n",
    "                    Ymat.append(wordMat)\n",
    "                elif axisNum == 4:\n",
    "                    axis = 'Z'\n",
    "                    Zmat.append(wordMat)\n",
    "                f.close()\n",
    "                \n",
    "        finalMat = np.append(Wmat, Xmat, axis = 1)\n",
    "        finalMat = np.append(finalMat, Ymat, axis = 1)\n",
    "        finalMat = np.append(finalMat, Zmat, axis = 1)\n",
    "        \n",
    "        #print(finalMat)\n",
    "        \n",
    "        return finalMat\n",
    "            #print(Xmat)\n",
    "    \n",
    "    elif vectModel == \"tfidf\":\n",
    "        for axisNum in range(1, 5):\n",
    "            if axisNum == 1:\n",
    "                axis = 'W'\n",
    "            elif axisNum == 2:\n",
    "                axis = 'X'\n",
    "            elif axisNum == 3:\n",
    "                axis = 'Y'\n",
    "            elif axisNum == 4:\n",
    "                axis = 'Z'\n",
    "                \n",
    "            for file in glob.glob(directory + axis + \"/tfidf_vectors_*.txt\"):\n",
    "                #read tf file\n",
    "                f = open(file, \"r\")\n",
    "                tf_vectors = f.readlines()\n",
    "        \n",
    "                gestWords = []\n",
    "                tfVals = []\n",
    "        \n",
    "                #split the line into the word and tf value\n",
    "                for line in tf_vectors:\n",
    "                    noDash = line.split(\"-\")\n",
    "                    tf_val = noDash[1]\n",
    "                    tf_val = tf_val.replace(\"\\n\", \"\")\n",
    "                    gestWords.append(noDash[0])\n",
    "                    tfVals.append(tf_val)\n",
    "           \n",
    "        \n",
    "                index = 0\n",
    "                startI = \"1\"\n",
    "                for y in range(1, w):\n",
    "                    startI = startI + \"1\"\n",
    "                startI = int(startI)\n",
    "        \n",
    "                #create dictionary with every word for every sensor and every directory\n",
    "                numWords = (startI * (2*r) - startI) * 20\n",
    "                wordMat = []\n",
    "    \n",
    "                for i in range(0, numWords + 20):\n",
    "                    wordMat.append(0)\n",
    "                \n",
    "                # put tf values into matrix where column = word\n",
    "                for x in gestWords:\n",
    "                    word = x.split(\", \")\n",
    "                    sensorNum = word[1].replace(\"'\", \"\")\n",
    "                    wordID = word[2].replace(\"'\", \"\")\n",
    "                    wordID = wordID.replace(\")\", \"\")\n",
    "                \n",
    "                    wordID = int(wordID)\n",
    "                    sensorNum = int(sensorNum)\n",
    "                \n",
    "                    #axisSplit = len(wordMat) / 4\n",
    "                    sensorSplit = len(wordMat) / 20\n",
    "                    \n",
    "                    wordIndex = int(wordID + ((sensorNum - 1) * sensorSplit)) #+ ((axisNum - 1) * axisSplit)))\n",
    "                    wordIndex = wordIndex - startI\n",
    "            \n",
    "                    wordMat[wordIndex] = float(tfVals[index])\n",
    "                    index = index + 1\n",
    "                \n",
    "\n",
    "                if axisNum == 1:\n",
    "                    axis = 'W'\n",
    "                    Wmat.append(wordMat)\n",
    "                elif axisNum == 2:\n",
    "                    axis = 'X'\n",
    "                    Xmat.append(wordMat)\n",
    "                elif axisNum == 3:\n",
    "                    axis = 'Y'\n",
    "                    Ymat.append(wordMat)\n",
    "                elif axisNum == 4:\n",
    "                    axis = 'Z'\n",
    "                    Zmat.append(wordMat)\n",
    "                f.close()\n",
    "           \n",
    "        finalMat = np.append(Wmat, Xmat, axis = 1)\n",
    "        finalMat = np.append(finalMat, Ymat, axis = 1)\n",
    "        finalMat = np.append(finalMat, Zmat, axis = 1)\n",
    "        \n",
    "        #print(finalMat)\n",
    "        \n",
    "        return finalMat\n",
    "            #print(Xmat)\n",
    "    \n",
    "def createdictofComponents(topk, k):\n",
    "    #creates a matrix that contains tuples of the word and score\n",
    "    outputMat = []\n",
    "    outputMat = topk\n",
    "    startI = \"1\"\n",
    "    for y in range(1, w):\n",
    "        startI = startI + \"1\"\n",
    "    startI = int(startI)\n",
    "    numWords = (startI * (2*r) - startI)\n",
    "    \n",
    "    for j in range(0, len(topk.columns)):\n",
    "        axisSplit = len(topk) / 4\n",
    "        sensorSplit = axisSplit / 20\n",
    "        \n",
    "        ax = int(j/axisSplit)\n",
    "        sens = int((j - (ax * axisSplit))/sensorSplit)\n",
    "        word = int((j - ((sens * sensorSplit) + (ax * axisSplit))) - startI)\n",
    "        \n",
    "        for row in range(0, k):\n",
    "            if ax == 0:\n",
    "                axis = 'W'\n",
    "            elif ax == 1:\n",
    "                axis = 'X'\n",
    "            elif ax == 2:\n",
    "                axis = 'Y'\n",
    "            elif ax == 3:\n",
    "                axis = 'Z'\n",
    "            label = axis + str(sens) + str(word)\n",
    "            outputMat[j][row] = (topk[j][row], label)\n",
    "            \n",
    "        \n",
    "                \n",
    "            \"\"\"for sensor in range(0, 20):\n",
    "            \n",
    "                for word in range(0, numWords + 1):\n",
    "                    label = axis + str(sensor - 1) + str(word + startI)\n",
    "                    \n",
    "                    #axisSplit = len(topk) / 4\n",
    "                    sensorSplit = axisSplit / 20\n",
    "                    \n",
    "                    wordIndex = int((word + startI) + (sensor * sensorSplit) + (ax * axisSplit))\n",
    "                    wordIndex = wordIndex - startI\n",
    "                    outputMat[wordIndex][row] = (topk[wordIndex][row], label)\"\"\"\n",
    "                \n",
    "                \n",
    "    #for i in range(0, k):\n",
    "     #   word = \"w\" + str(i)\n",
    "      #  for j in range(0, len(topk)):\n",
    "       #     outputMat[i][j] = (topk[i][j], word)\n",
    "            \n",
    "    print(outputMat)\n",
    "    #sorts the words acording to their scores\n",
    "    outputMat = np.transpose(outputMat)\n",
    "    outputMat = outputMat.apply(lambda x: x.sort_values(ascending = False).values)\n",
    "    outputMat = np.transpose(outputMat)\n",
    "    finalMat = outputMat\n",
    "    #for i in range(0, k):\n",
    "     #   for j in range(0, len(outputMat)):\n",
    "      #      finalMat[i][j] = (outputMat[i][j][1], outputMat[i][j][0])\n",
    "            \n",
    "    file = open(\"./userOutput.txt\", \"w\")\n",
    "    file.write(str(finalMat))\n",
    "    file.close()\n",
    "    return finalMat\n",
    "    \n",
    "    \n",
    "def task1(vectModel, useOp, k):\n",
    "    \n",
    "    if useOp == \"PCA\":\n",
    "        #PCA\n",
    "        wordMat = makeMat(vectModel)\n",
    "        \n",
    "        topk = PCAsetup(wordMat, k)\n",
    "        print(\"\\nPCA:\\n\")\n",
    "        #print(topk)\n",
    "        \n",
    "        dictofComponents = createdictofComponents(topk, k)\n",
    "        print(dictofComponents)\n",
    "        \n",
    "        \n",
    "    elif useOp == \"SVD\":\n",
    "        wordMat = makeMat(vectModel)\n",
    "        topk = SVDsetup(wordMat, k)\n",
    "        print(\"\\nSVD:\\n\")\n",
    "        #print(topk)\n",
    "        \n",
    "        original_df = pd.DataFrame(topk)\n",
    "        original_df.to_pickle(\"./SVD_\" + vectModel + \".pkl\")\n",
    "        \n",
    "        dictofComponents = createdictofComponents(topk, k)\n",
    "        print(dictofComponents)\n",
    "        \n",
    "        \n",
    "    elif useOp == \"NMF\":\n",
    "        wordMat = makeMat(vectModel)\n",
    "        topk = NMFsetup(wordMat, k)\n",
    "        print(\"\\nNMF:\\n\")\n",
    "        #print(topk)\n",
    "        \n",
    "        original_df = pd.DataFrame(topk)\n",
    "        original_df.to_pickle(\"./NMF_\" + vectModel + \".pkl\")\n",
    "        \n",
    "        dictofComponents = createdictofComponents(topk, k)\n",
    "        print(dictofComponents)\n",
    "\n",
    "        \n",
    "    elif useOp == \"LDA\":\n",
    "        wordMat = makeMat(vectModel)\n",
    "        topk = LDAsetup(wordMat, k)\n",
    "        print(\"\\nLDA:\\n\")\n",
    "        #print(topk)\n",
    "        \n",
    "        original_df = pd.DataFrame(topk)\n",
    "        original_df.to_pickle(\"./LDA_\" + vectModel + \".pkl\")\n",
    "        \n",
    "        dictofComponents = createdictofComponents(topk, k)\n",
    "        print(dictofComponents)\n",
    "        \n",
    "\n",
    "\n",
    "vectModel = input(\"Enter the vector model (ex: tf): \")\n",
    "k = input(\"Enter k (ex: 4): \")\n",
    "useOp = input(\"Enter the analysis you would like to use (ex: PCA): \")\n",
    "k = int(k)\n",
    "task1(vectModel, useOp, k)\n",
    "\n",
    "\n",
    "#sample output: \n",
    "#Enter the vector model: tf\n",
    "#Enter k: 10\n",
    "#Enter the analysis you would like to use: PCA\n",
    "\n",
    "p = pickle.load( open( \"PCA_tf.pkl\", \"rb\" ) )\n",
    "print(\"pickle\")\n",
    "print(p)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter gesture file(e.g Data/1.csv) :\n",
      "* WARNING : Please make .pkl in TASK 1 before you use principal component\n",
      "small_data/5.csv\n",
      "Enter vector model (tf, tfidf):\n",
      "tf\n",
      "Enter user options (1 ~ 7)\n",
      "* HINT : 1 = Dot similarity, 2 = PCA, 3 = SVD, 4 = NMF, 5 = LDA, 6 = Edit Distance, 7 = DTW\n",
      "7\n",
      "Most similar (gesture, score) \n",
      "(5, 0.0)\n",
      "(8, 763.9849793821021)\n",
      "(1, 963.4535877666793)\n",
      "(7, 1073.0613679040994)\n",
      "(0, 1137.6742837095742)\n",
      "(2, 1141.0164576592588)\n",
      "(9, 1159.8417325134235)\n",
      "(4, 1274.8779483124354)\n",
      "(3, 1337.2028566713602)\n",
      "(6, 1367.6411098666713)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity                          \n",
    "from math import log\n",
    "import re\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy import spatial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log2\n",
    "\n",
    "\n",
    "def dot_similarity(gesture1, gesture2):                  \n",
    "    x = np.array(gesture1)\n",
    "    y = np.array(gesture2)\n",
    "    \"\"\"if len(x) > len(y):\n",
    "        y = np.pad(y, (0, len(x) - len(y)))\n",
    "    else:\n",
    "        x = np.pad(x, (0, len(y) - len(x)))\"\"\"\n",
    "    return np.dot(x, y)\n",
    "\n",
    "def pears_similarity(vec1, vec2):\n",
    "    pearson_coef, p_value = stats.pearsonr(vec1, vec2)\n",
    "    return pearson_coef\n",
    "    \n",
    "def cos_similarity(vec1, vec2):                            \n",
    "    x = np.array(vec1)\n",
    "    y = np.array(vec2)                                                    \n",
    "    similarity = 1- spatial.distance.cosine(x, y)                             \n",
    "                                                                            \n",
    "    return similarity                             \n",
    "\n",
    "def KL_div_similarity(p, q):\n",
    "    return sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n",
    "\n",
    "def edit_distance(sensor1, sensor2, m, n):\n",
    "    # Create a table to store results of subproblems \n",
    "    dp = [[0 for x in range(n + 1)] for x in range(m + 1)] \n",
    "  \n",
    "    # Fill d[][] in bottom up manner \n",
    "    for i in range(m + 1): \n",
    "        for j in range(n + 1): \n",
    "  \n",
    "            # If first string is empty, only option is to \n",
    "            # insert all characters of second string \n",
    "            if i == 0: \n",
    "                dp[i][j] = j    # Min. operations = j \n",
    "  \n",
    "            # If second string is empty, only option is to \n",
    "            # remove all characters of second string \n",
    "            elif j == 0: \n",
    "                dp[i][j] = i    # Min. operations = i \n",
    "  \n",
    "            # If last characters are same, ignore last char \n",
    "            # and recur for remaining string \n",
    "            elif sensor1[i-1] == sensor2[j-1]: \n",
    "                dp[i][j] = dp[i-1][j-1] \n",
    "  \n",
    "            # If last character are different, consider all \n",
    "            # possibilities and find minimum \n",
    "            else: \n",
    "                dp[i][j] = 1 + min(dp[i][j-1],        # Insert \n",
    "                                   dp[i-1][j],        # Remove \n",
    "                                   dp[i-1][j-1])    # Replace \n",
    "  \n",
    "    return dp[m][n] \n",
    "    \n",
    "def dynamic_time_warping(sensor1, sensor2, m, n):\n",
    "    dp = [[0 for x in range(n+1)] for x in range(m+1)] \n",
    "    \n",
    "    for i in range(1, m+1): \n",
    "        for j in range(1, n+1): \n",
    "            dp[i][j] = abs(sensor1[i-1] - sensor2[j-1])\n",
    "             \n",
    "            if i == 1 and j == 1:\n",
    "                continue\n",
    "            elif i == 1 and j != 1:\n",
    "                dp[i][j] += dp[i][j-1] \n",
    "            elif i != 1 and j == 1:\n",
    "                dp[i][j] += dp[i-1][j]\n",
    "            else:\n",
    "                dp[i][j] += min(dp[i][j-1],        # Insert \n",
    "                                dp[i-1][j],        # Remove \n",
    "                                dp[i-1][j-1])    # Replace \n",
    "            \n",
    "    return dp[m][n] \n",
    "\n",
    "def tfidf_loader(directory):\n",
    "    gestures = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.startswith(\"tfidf_\") or not filename.endswith(\".txt\") :\n",
    "            continue\n",
    "            \n",
    "        path = directory+\"/\"+filename\n",
    "        with open(path, \"r\") as w:\n",
    "            gesture = w.readlines()\n",
    "\n",
    "        matrix=[]\n",
    "\n",
    "        for sensor in gesture :\n",
    "            tfidf = float(sensor.split(\"-\")[1].strip())\n",
    "            matrix.append(tfidf)\n",
    "                \n",
    "        gestures.append(matrix)\n",
    "        \n",
    "    return gestures\n",
    "    \n",
    "    \n",
    "def tf_loader(directory):\n",
    "    gestures = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.startswith(\"tf_\") or not filename.endswith(\".txt\") :\n",
    "            continue\n",
    "            \n",
    "        path = directory+\"/\"+filename\n",
    "        with open(path, \"r\") as w:\n",
    "            gesture = w.readlines()\n",
    "\n",
    "        matrix=[]\n",
    "\n",
    "        for sensor in gesture :\n",
    "            tf = float(sensor.split(\"-\")[1].strip())   \n",
    "            matrix.append(tf)\n",
    "        gestures.append(matrix)\n",
    "        \n",
    "    return gestures\n",
    "    \n",
    "def symbol_loader(directory):\n",
    "    gestures = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".wrd\") :\n",
    "            continue\n",
    "        path = directory+\"/\"+filename\n",
    "        with open(path, \"r\") as w:\n",
    "            gesture = w.readlines()\n",
    "\n",
    "        matrix=[]\n",
    "\n",
    "        for sensor in gesture :\n",
    "            sym_quant_window = [re.findall(r'\\d+',word)[0] for word in sensor.split(\" - \")[1].split(\",\")] \n",
    "            matrix.append(sym_quant_window)\n",
    "                \n",
    "        gestures.append(matrix)\n",
    "        \n",
    "    return gestures\n",
    "\n",
    "def amplitude_loader(directory):\n",
    "    gestures = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".wrd\") :\n",
    "            continue\n",
    "        path = directory+\"/\"+filename\n",
    "        with open(path, \"r\") as w:\n",
    "            gesture = w.readlines()\n",
    "\n",
    "        matrix=[]\n",
    "\n",
    "        for sensor in gesture :\n",
    "            avg_quant_ampitude = [float(word) for word in sensor.split(\" - \")[0].split(\"[\")[1].replace(\"],\",\"\").split(\",\")]\n",
    "            matrix.append(avg_quant_ampitude)\n",
    "                \n",
    "        gestures.append(matrix)\n",
    "        \n",
    "    return gestures\n",
    "\n",
    "###### GET Input from users######\n",
    "print(\"Enter gesture file(e.g Data/1.csv) :\")\n",
    "print(\"* WARNING : Please make .pkl in TASK 1 before you use principal component\")\n",
    "gesture_path = input()\n",
    "\n",
    "directory = gesture_path.split(\"/\")[0]\n",
    "#axis = gesture_path.split(\"/\")[1]\n",
    "filename = gesture_path.split(\"/\")[1]\n",
    "key_idx = int(re.findall(r'\\d+', filename)[0])\n",
    "\n",
    "print(\"Enter vector model (tf, tfidf):\")\n",
    "vector_model = input()\n",
    "\n",
    "print(\"Enter user options (1 ~ 7)\")\n",
    "print(\"* HINT : 1 = Dot similarity, 2 = PCA, 3 = SVD, 4 = NMF, 5 = LDA, 6 = Edit Distance, 7 = DTW\")\n",
    "user_option = int(input())  \n",
    "\n",
    "#Retrieve only top 10 gestures with high similarity\n",
    "top_K = 10\n",
    "\n",
    "#Calculate similarity(cost) based on User options\n",
    "if user_option == 1 :\n",
    "    directory = directory + \"/\"\n",
    "    gestures = makeMat(vector_model)\n",
    "    directory = directory.replace(\"/\", \"\")\n",
    "    \n",
    "    cost=[]\n",
    "    key_gesture = gestures[key_idx]\n",
    "    for gesture in gestures :\n",
    "        similarity = dot_similarity(key_gesture, gesture)\n",
    "        cost.append(similarity)\n",
    "    cost_top_K = sorted(cost, reverse=True)#[1:top_K+1]                                        \n",
    "    \n",
    "elif user_option == 2 :\n",
    "    PC_path = [\"PCA\", vector_model]\n",
    "    PC_path = \"_\".join(PC_path)\n",
    "    pca = pd.read_pickle(PC_path + \".pkl\")\n",
    "    num = len(pca[0])\n",
    "\n",
    "    pca = pca.T\n",
    "    key_vec = pca[key_idx]\n",
    "    cost=[]\n",
    "    for idx in range(num) :\n",
    "        similarity = pears_similarity(key_vec, pca[idx])\n",
    "        cost.append(similarity)\n",
    "    cost_top_K = sorted(cost, reverse=True)#[1:top_K+1]                                        \n",
    "\n",
    "elif user_option == 3 :                   \n",
    "    PC_path = [\"SVD\", vector_model]\n",
    "    PC_path = \"_\".join(PC_path)\n",
    "    svd = pd.read_pickle(PC_path + \".pkl\")                                      \n",
    "    num = len(svd[0])                                                           \n",
    "                                                                                \n",
    "    svd = svd.T                                                                 \n",
    "    key_vec = svd[key_idx]      \n",
    "    \n",
    "    cost=[]                                                                     \n",
    "    for idx in range(num) :                                                     \n",
    "        similarity = cos_similarity(key_vec, svd[idx])                            \n",
    "        cost.append(similarity)                 \n",
    "    cost_top_K = sorted(cost, reverse=True)#[1:top_K+1]                                        \n",
    "    \n",
    "elif user_option == 4 :                                                         \n",
    "    PC_path = [\"NMF\", vector_model]\n",
    "    PC_path = \"_\".join(PC_path)\n",
    "    nmf = pd.read_pickle(PC_path + \".pkl\")\n",
    "    num = len(nmf[0])                                                           \n",
    "                                                                                \n",
    "    nmf = nmf.T                                                                 \n",
    "    key_vec = nmf[key_idx]    \n",
    "    \n",
    "    cost=[]                                                                     \n",
    "    for idx in range(num) :                                                     \n",
    "        similarity = cos_similarity(key_vec, nmf[idx])                            \n",
    "        cost.append(similarity)           \n",
    "    cost_top_K = sorted(cost, reverse=True)#[1:top_K+1]                                        \n",
    "                  \n",
    "elif user_option == 5 :                                                         \n",
    "    PC_path = [\"LDA\", vector_model]\n",
    "    PC_path = \"_\".join(PC_path)\n",
    "    lda = pd.read_pickle(PC_path + \".pkl\")\n",
    "    num = len(lda[0])                                                           \n",
    "                                                                                \n",
    "    lda = lda.T                                                                 \n",
    "    key_vec = lda[key_idx]    \n",
    "    \n",
    "    cost=[]                                                                     \n",
    "    for idx in range(num) :                                                     \n",
    "        similarity = KL_div_similarity(key_vec, lda[idx])                            \n",
    "        cost.append(similarity)                                  \n",
    "    cost_top_K = sorted(cost, reverse=True)#[1:top_K+1]                                        \n",
    "\n",
    "elif user_option == 6 :\n",
    "    pathW = directory + \"/W\"\n",
    "    pathX = directory + \"/X\"\n",
    "    pathY = directory + \"/Y\"\n",
    "    pathZ = directory + \"/Z\"\n",
    "    gesturesW = symbol_loader(pathW)\n",
    "    gesturesX = symbol_loader(pathX)\n",
    "    gesturesY = symbol_loader(pathY)\n",
    "    gesturesZ = symbol_loader(pathZ)\n",
    "    \n",
    "    gestures = np.append(gesturesW, gesturesX, axis = 1)\n",
    "    gestures = np.append(gestures, gesturesY, axis = 1)\n",
    "    gestures = np.append(gestures, gesturesZ, axis = 1)\n",
    "        \n",
    "    key_gesture = gestures[key_idx]\n",
    "    \n",
    "    cost=[0]*len(gestures)\n",
    "    for i, gesture in enumerate(gestures) :\n",
    "        for j, sensor in enumerate(gesture) :\n",
    "            n = len(sensor)\n",
    "            m = len(key_gesture[j])\n",
    "            cost[i] += edit_distance(key_gesture[j], sensor, m, n)\n",
    "    cost_top_K = sorted(cost)#[1:top_K+1]                                        \n",
    "\n",
    "elif user_option == 7 :\n",
    "    pathW = directory + \"/W\"\n",
    "    pathX = directory + \"/X\"\n",
    "    pathY = directory + \"/Y\"\n",
    "    pathZ = directory + \"/Z\"\n",
    "    gesturesW = amplitude_loader(pathW)\n",
    "    gesturesX = amplitude_loader(pathX)\n",
    "    gesturesY = amplitude_loader(pathY)\n",
    "    gesturesZ = amplitude_loader(pathZ)\n",
    "    \n",
    "    gestures = np.append(gesturesW, gesturesX, axis = 1)\n",
    "    gestures = np.append(gestures, gesturesY, axis = 1)\n",
    "    gestures = np.append(gestures, gesturesZ, axis = 1)\n",
    "    \n",
    "    key_gesture = gestures[key_idx]\n",
    "    \n",
    "    cost=[0]*len(gestures)\n",
    "    for i, gesture in enumerate(gestures) :                                     \n",
    "        for j, sensor in enumerate(gesture) :                                   \n",
    "            n = len(sensor)                                                     \n",
    "            m = len(key_gesture[j])                                             \n",
    "            cost[i] += dynamic_time_warping(key_gesture[j], sensor, m, n)    \n",
    "    cost_top_K = sorted(cost)#[1:top_K+1]                                        \n",
    "else:\n",
    "    print(\"ERROR : No such user option in this program\")\n",
    "    \n",
    "                                                           \n",
    "print(\"Most similar (gesture, score) \")              \n",
    "for k in cost_top_K :                                                       \n",
    "    print((cost.index(k), k))       \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
